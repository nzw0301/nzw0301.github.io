<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>
    <description>not δ</description>
    <link>/</link>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Sun, 14 Feb 2016 16:45:58 +0900</pubDate>
    <lastBuildDate>Sun, 14 Feb 2016 16:45:58 +0900</lastBuildDate>
    <generator>Jekyll v3.0.3</generator>
    
      <item>
        <title>Sequence to Sequence Learning with Neural Networks</title>
        <description>&lt;h4 id=&quot;section&quot;&gt;はじめに&lt;/h4&gt;

&lt;p&gt;対話文を生成する
&lt;a href=&quot;http://arxiv.org/pdf/1506.05869.pdf&quot;&gt;A Neural Conversational Model&lt;/a&gt;の実装がしたくなったんですが，そこで使われるSequence to Sequenceを知らなかったのでその紹介．&lt;/p&gt;

&lt;p&gt;Sequence to Sequenceは，NIPS2014の&lt;a href=&quot;http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf&quot;&gt;論文&lt;/a&gt;で提案されているアーキテクチャ．&lt;/p&gt;

&lt;p&gt;論文ではSequence to Sequenceという入出力が可変長な系列データを扱うためのネットワークの提案を英仏の翻訳のタスクで評価．
なので前回紹介したencoder–decoderとやりたいことは同じ．
例によって翻訳がわかってないので，モデルの紹介だけ．&lt;/p&gt;

&lt;p&gt;encoder–decoderと違うこととしては&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;LSTMを2つくっつけて使う (LSTMを使うことで長期依存を取ろうとしている)
    &lt;ul&gt;
      &lt;li&gt;2つというのは入力系列用のLSTMと出力系列用のLSTM&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;4層のLSTM
    &lt;ul&gt;
      &lt;li&gt;(nzw: 単層ならわかるが，4層の場合に入力系列の入れ方と中間のベクトルをどうやって作るのかわからない)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;入力系列の順序を逆順にする
    &lt;ul&gt;
      &lt;li&gt;(nzw: interactiveにつかえないような)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;section-1&quot;&gt;本題&lt;/h4&gt;

&lt;p&gt;モデルはencoder–decoderよりもさらにシンプル（にみえる）．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/seq2seq.svg&quot; alt=&quot;model&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ただし，&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;ABC&lt;/code&gt; : 入力系列 (ex: 英語の1文)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;EOS&amp;gt;&lt;/code&gt; : 文末記号&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;WXYZW&lt;/code&gt; : 出力系列 (ex: フランス語の1文)&lt;/li&gt;
  &lt;li&gt;出力層 : &lt;code class=&quot;highlighter-rouge&quot;&gt;softmax&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;とする．&lt;/p&gt;

&lt;p&gt;あと，翻訳先ともとでembeddingsは2つ存在する．
(nzw: 会話の場合はembeddinsが1つでいいかもしれない)&lt;/p&gt;

&lt;p&gt;encoder部分とdecoder部分はそれぞれ4層のLSTMからできてるのでもう少し複雑になる．
4層のLSTMに系列いれるのがよくわかってません．&lt;/p&gt;

&lt;h4 id=&quot;section-2&quot;&gt;その他&lt;/h4&gt;
&lt;p&gt;入力系列を逆順に入れると翻訳の性能が上がるらしい．
つまり，1.ではなくて2.をいれる．&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;月 が きれい です ね&lt;/li&gt;
  &lt;li&gt;ね です きれい が 月&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;section-3&quot;&gt;所感&lt;/h4&gt;
&lt;p&gt;モデルをもう少し詳しく書いてほしい．．．&lt;/p&gt;

&lt;p&gt;encoder–decoder (というか翻訳) の時点で思ってたのは，この枠組が使えると翻訳以外でも会話，言い換え，QAあたりもできるので，汎用性が高い印象を持った． (ただし十分なデータ量がない気がしている)&lt;/p&gt;

&lt;p&gt;4層のLSTMの入力のあたりは関連研究を参考にして調べてみようかと思います．&lt;/p&gt;
</description>
        <pubDate>Sun, 14 Feb 2016 16:45:00 +0900</pubDate>
        <link>/2016/02/seq2seq</link>
        <guid isPermaLink="true">/2016/02/seq2seq</guid>
        
        
      </item>
    
      <item>
        <title>トピックモデルによる統計的潜在意味解析のp56の最初の式</title>
        <description>&lt;h4 id=&quot;section&quot;&gt;はじめに&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;http://amzn.to/1Tc0ost&quot;&gt;トピックモデルによる統計的潜在意味解析&lt;/a&gt; を頭から読んでいて3.2.4の周辺化ギブスサンプリングの式が気になったのでその部分の式展開だけをちょっと詳しめに書きます．&lt;/p&gt;

&lt;h4 id=&quot;section-1&quot;&gt;本題&lt;/h4&gt;
&lt;p&gt;p55の最後の式からp56の最初の式変形をやります．
2016年2月10日にアクセスした正誤表には記述がないのですが，
p56の最後の項は同時確率ではなく，条件付き確率であると思われます．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
p(z_{d,i} = k, w_{d,i}=v, \mathbf{w}^{\backslash d,i}, \mathbf{z}^{\backslash d,i} | {\boldsymbol \alpha}, {\boldsymbol \beta})
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;がp55の最後の式です．
これを順番に分解していきます．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
p(z_{d,i} = k, w_{d,i}=v, \mathbf{w}^{\backslash d,i}, \mathbf{z}^{\backslash d,i} | {\boldsymbol \alpha}, {\boldsymbol \beta}) &amp;amp;=&amp;amp;
p( w_{d,i} = v |z_{d,i} = k, \mathbf{w}^{\backslash d,i}, \mathbf{z}^{\backslash d,i},  {\boldsymbol \alpha}, {\boldsymbol \beta})  \\
&amp;amp; &amp;amp; \times
p(z_{d,i} = k, \mathbf{w}^{\backslash d,i}, \mathbf{z}^{\backslash d,i} | {\boldsymbol \alpha}, {\boldsymbol \beta}) \tag{1} \\
&amp;amp;=&amp;amp;
p( w_{d,i} = v |z_{d,i} = k, \mathbf{w}^{\backslash d,i}, \mathbf{z}^{\backslash d,i},  {\boldsymbol \alpha}, {\boldsymbol \beta}) \\
&amp;amp; &amp;amp; \times
p(z_{d,i} = k| \mathbf{w}^{\backslash d,i}, \mathbf{z}^{\backslash d,i} , {\boldsymbol \alpha}, {\boldsymbol \beta}) \\
&amp;amp; &amp;amp;  \times
p(\mathbf{w}^{\backslash d,i}, \mathbf{z}^{\backslash d,i} | {\boldsymbol \alpha}, {\boldsymbol \beta}) \tag{2}
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;式1の左辺から右辺の変形は以下の関係式を使います．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
p(w_{d,i}=v | z_{d,i} = k, \mathbf{w}^{\backslash d,i}, \mathbf{z}^{\backslash d,i} ,  {\boldsymbol \alpha}, {\boldsymbol \beta}) = \frac{
  p(z_{d,i} = k, w_{d,i}=v, \mathbf{w}^{\backslash d,i}, \mathbf{z}^{\backslash d,i} | {\boldsymbol \alpha}, {\boldsymbol \beta})}
{p(z_{d,i} = k, \mathbf{w}^{\backslash d,i}, \mathbf{z}^{\backslash d,i} | {\boldsymbol \alpha}, {\boldsymbol \beta})}
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;式2では式1右辺の第2項目を以下の等式を用いて2つの項（式2の第2項，第3項）に分解しています．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
p(z_{d,i} = k | \mathbf{w}^{\backslash d,i}, \mathbf{z}^{\backslash d,i} , {\boldsymbol \alpha}, {\boldsymbol \beta}) = \frac{
p(z_{d,i} = k, \mathbf{w}^{\backslash d,i}, \mathbf{z}^{\backslash d,i} | {\boldsymbol \alpha}, {\boldsymbol \beta})}
{p(\mathbf{w}^{\backslash d,i}, \mathbf{z}^{\backslash d,i} | {\boldsymbol \alpha}, {\boldsymbol \beta})}
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;以上から本のように同時確率\(p(\mathbf{w}^{\backslash d,i}, \mathbf{z}^{\backslash d,i} , {\boldsymbol \alpha}, {\boldsymbol \beta})\)ではなく条件付き確率になるような気がしています．
ただ，条件付き確率であろうとなかろうとこの項は\(z_{d,i}\)を含んでいないので消去されることから期待値計算には関係してきません．&lt;/p&gt;
</description>
        <pubDate>Wed, 10 Feb 2016 18:36:00 +0900</pubDate>
        <link>/2016/02/collapseGibbsEquation</link>
        <guid isPermaLink="true">/2016/02/collapseGibbsEquation</guid>
        
        
      </item>
    
      <item>
        <title>Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation</title>
        <description>&lt;h4 id=&quot;section&quot;&gt;はじめに&lt;/h4&gt;
&lt;p&gt;このページは研究室の輪講のために作成されたページです．&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://amzn.to/1T9JfPS&quot;&gt;青深層学習の7章&lt;/a&gt; でRNNを扱いました．
本ではCTCを使って，入出力で系列長の異なるデータへの対応をしました（読んでる限りは入力系列より短くはできるが長くはできないような）．&lt;/p&gt;

&lt;p&gt;この &lt;a href=&quot;http://arxiv.org/pdf/1406.1078v3.pdf&quot;&gt;EMNLP2014&lt;/a&gt; の論文では，encoder–decoderの提案，LSTM likeなユニットの提案，それらを使った英仏のフレーズ翻訳のタスクでの評価実験しています．（word embeddingsも少し出てきます）&lt;/p&gt;

&lt;p&gt;nzwは機械翻訳について全くの素人であるので，モデル自体の話しかしません．
また詳しい話は菊池さんの &lt;a href=&quot;http://www.slideshare.net/yutakikuchi927/learning-phrase-representations-using-rnn-encoderdecoder-for-statistical-machine-translation&quot;&gt;slideshare&lt;/a&gt; が非常にわかりやすいのでこちらをお勧めします．&lt;/p&gt;

&lt;h4 id=&quot;section-1&quot;&gt;本題&lt;/h4&gt;

&lt;p&gt;まずタイトルにあるencoder–decoderを別々に説明すると，&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;encoder：可変長の入力系列を固定長ベクトルに変換するRNN&lt;/li&gt;
  &lt;li&gt;decoder：固定長ベクトルを可変長の系列データに変換するRNN&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;です．&lt;/p&gt;

&lt;p&gt;構造自体はシンプルです．
まずencoderでは，入力系列（例えば，英語のフレーズ）を時刻ごとに受け取り，入力系列の最後まで入力したら固定長ベクトルの1つに変換します．
次にdecoderでは，encoderで求めた固定長ベクトルとdecoder部分のRNNを使って時刻ごとに単語を1つずつ出力していき，文末を表す記号を出力するまで繰り返します．&lt;/p&gt;

&lt;p&gt;論文の図1では下がencoder，上がdecoderになります．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/model.png&quot; alt=&quot;nn&quot; /&gt;&lt;/p&gt;

&lt;p&gt;このとき，&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(\mathbf{x}\) ：1単語に対応するone–hotベクトル&lt;/li&gt;
  &lt;li&gt;\(\mathbf{c}\) ：固定長ベクトル&lt;/li&gt;
  &lt;li&gt;\(\mathbf{y}\) ：1単語に対応するone–hotベクトル&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;です．&lt;/p&gt;

&lt;p&gt;encoder–decoderの用途は，&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;入力系列から出力系列の生成&lt;/li&gt;
  &lt;li&gt;入出力の系列ペアのスコアリング&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;と言及されています．&lt;/p&gt;

&lt;h5 id=&quot;encoder&quot;&gt;encoderの計算&lt;/h5&gt;
&lt;p&gt;\(\mathbf{h}_{\langle t \rangle}\) は時刻\(t\) におけるencoderの白いユニット部分の値です．&lt;/p&gt;

&lt;p&gt;計算式はRNNなので以下のようになります．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
\mathbf{h}_{\langle t \rangle} = f( \mathbf{h}_{\langle t-1 \rangle} , x_t)
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;関数 \(f\) はロジスティックシグモイドやLSTMなどをおけます．
この論文ではLSTM likeなユニットを導入しているので，それが\(f\)に対応します．&lt;/p&gt;

&lt;p&gt;encoderのRNNは，青い本のRNNとほぼ同じ形です．&lt;/p&gt;

&lt;h5 id=&quot;decoder&quot;&gt;decoderの計算&lt;/h5&gt;
&lt;p&gt;\begin{eqnarray}
\mathbf{h}_{\langle t \rangle} = f( \mathbf{h}_{\langle t-1 \rangle} , y_{t-1}, \mathbf{c})
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;\(\mathbf{h}_{\langle t \rangle}\) は時刻\(t\) におけるdecoderの白いユニット部分の値です．
encoderと異なり，\(\mathbf{c}\)が隠れ層に常に関係してきます．&lt;/p&gt;

&lt;p&gt;出力層の計算は&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
p(y_{t}|y_{t-1},y_{t-2},…,y_{1},\mathbf{c}) = g(\mathbf{h}_{\langle t \rangle}, y_{t-1}, \mathbf{c})
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;となります．
確率値にしたいので，\(g\)には，例えばsoftmax関数が使われます．&lt;/p&gt;

&lt;h4 id=&quot;lstm-likernn&quot;&gt;LSTM likeなRNNの隠れ層のユニット&lt;/h4&gt;

&lt;p&gt;LSTMよりもシンプルで似た働き（記憶と忘却）を持たせるために導入しています．&lt;/p&gt;

&lt;p&gt;これが1ユニットです（LSTMの1メモリユニットに相当）．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/encoder-decoder-unit.png&quot; alt=&quot;nn&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(r\)：reset gate，確率値で0に近いほど前の層の情報\(h\)を無視，頻繁に活発化すれば短期記憶&lt;/li&gt;
  &lt;li&gt;\(z\)：update gate，確率値で前の層の情報をどれだけ今の層に伝えるか，ずっと活性化していれば長期記憶&lt;/li&gt;
  &lt;li&gt;\(\mathbf{x}\)：入力ベクトル&lt;/li&gt;
  &lt;li&gt;\(\mathbf{h}\)：このユニットが出力する値&lt;/li&gt;
  &lt;li&gt;\(\mathbf{\tilde{h}}\)：hの計算に使う値&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;appendix&quot;&gt;appendixの詳しい式の紹介&lt;/h4&gt;

&lt;h6 id=&quot;encoder-1&quot;&gt;encoderの計算再び&lt;/h6&gt;
&lt;p&gt;入力系列\(\mathbf{x}\)から
固定長ベクトル\(\mathbf{c}\)を求めるまでの計算式です．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
h^{\langle t \rangle}_{j} &amp;amp;=&amp;amp; z_{j} h_{j}^{\langle t-1 \rangle} + (1-z_{j}) \tilde{h}^{\langle t \rangle}_{j} \\
\tilde{h}^{\langle t \rangle}_{j} &amp;amp;=&amp;amp; tanh( [ \mathbf{W} e(\mathbf{x}_t) ]_j + [\mathbf{U}(\mathbf{r} \odot \mathbf{h}_{\langle t-1 \rangle})]_j ) \\
z_j &amp;amp;=&amp;amp; \sigma( [\mathbf{W}_z e(\mathbf{x}_t)  ]_j + [\mathbf{U}_z \mathbf{h}_{\langle t-1 \rangle})]_j) \\
r_j &amp;amp;=&amp;amp; \sigma( [\mathbf{W}_r e(\mathbf{x}_t)  ]_j + [\mathbf{U}_r \mathbf{h}_{\langle t-1 \rangle})]_j), \\
\mathbf{c} &amp;amp;=&amp;amp; tanh(\mathbf{V} \mathbf{h}^{\langle N \rangle})
\end{eqnarray}&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(z_j\)：隠れ層の\(j\)番目のユニットのupdate geteの確率値&lt;/li&gt;
  &lt;li&gt;\(r_j\)：reset geteの確率値&lt;/li&gt;
  &lt;li&gt;\(h^{\langle t \rangle}_{j}\)：時刻\(t\)の隠れ層の\(j\)番目のユニットの値&lt;/li&gt;
  &lt;li&gt;\(e\)：word embeddingsの行列(このモデルで学習できる)&lt;/li&gt;
  &lt;li&gt;\(U\)：前の層に対する重み&lt;/li&gt;
  &lt;li&gt;\(V\)：入力系列を最後まで読みこんで計算した\(h\)にかかる重み&lt;/li&gt;
  &lt;li&gt;\(W\)：入力系列の分散表現にかかる重み&lt;/li&gt;
  &lt;li&gt;\(\sigma\)：ロジスティクスシグモイド関数&lt;/li&gt;
&lt;/ul&gt;

&lt;h6 id=&quot;decoder-1&quot;&gt;decoderの計算再び&lt;/h6&gt;

&lt;p&gt;初期値は \(\mathbf{h}’^{\langle 0 \rangle} =  tanh(\mathbf{V}’ \mathbf{c})\)．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
h’^{\langle t \rangle}_{j} &amp;amp;=&amp;amp; z’_{j} h_{j}’^{\langle t-1 \rangle} + (1-z’_{j}) \tilde{h}’^{\langle t \rangle}_{j} \\
\tilde{h}’^{\langle t \rangle}_{j} &amp;amp;=&amp;amp; tanh( [ \mathbf{W}’ e(\mathbf{y}_{t-1}) ]_j + \mathbf{r}’[\mathbf{U}’\mathbf{h}’_{\langle t-1 \rangle}) + \mathbf{Cc}] ) \\
z’_j &amp;amp;=&amp;amp; \sigma( [\mathbf{W}’_z e(\mathbf{y}_{t-1})  ]_j + [\mathbf{U}’_z \mathbf{h}’_{\langle t-1 \rangle})]_j + [\mathbf{C}_z \mathbf{c}]_j) \\
r’_j &amp;amp;=&amp;amp; \sigma( [\mathbf{W}’_r e(\mathbf{y}_{t-1})  ]_j + [\mathbf{U}’_r \mathbf{h}’_{\langle t-1 \rangle})]_j + [\mathbf{C}_r \mathbf{c}]_j), \\
p(y_{t,j} = 1 | \mathbf{y}_{t-1}, …, \mathbf{y}_1 \mathbf{X}) &amp;amp;=&amp;amp; \frac{exp(\mathbf{g}_j \mathbf{s}_{\langle t \rangle})}{\sum_{j’=1}^{K} exp(\mathbf{g}_{j’} \mathbf{s}_{\langle t \rangle})} \\
s_{\langle t \rangle} &amp;amp;=&amp;amp; max ( s’^{\langle t \rangle}_{2i-1}, s’^{\langle t \rangle}_{2i} ) \\
s’^{\langle t \rangle} &amp;amp;=&amp;amp; \mathbf{O}_h \mathbf{h}’^{\langle t \rangle} + \mathbf{O}_y \mathbf{y}_{t-1} + \mathbf{O}_c \mathbf{c}
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;‘がついているものはencoderと同様です．&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(s\)：maxout関数で計算&lt;/li&gt;
  &lt;li&gt;\(O\)：出力層への入力にかかる重み&lt;/li&gt;
  &lt;li&gt;\(C\)：固定長ベクトル\(\mathbf{c}\)の重み&lt;/li&gt;
  &lt;li&gt;\(g\)：重み行列\(G\)の要素&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;nzw&quot;&gt;その他，nzw的に興味があった箇所&lt;/h4&gt;

&lt;p&gt;word embeddingsがこれでも学習できるらしく，上の式で出てきた\(e\)を可視化した図が出てきている．
NGやsubsampligがないのでw2vの両モデルほど良い結果は出ないと思うけど，お得感がある．&lt;/p&gt;

&lt;p&gt;あと\(\mathbf{c}
\)を可視化するとフレーズが近くにまとまる．
（なので，ベクトル\(\mathbf{c}
\)を求めることは，入力系列のフレーズを埋め込んでるかPCAみたいなことをしていることに相当している？）&lt;/p&gt;
</description>
        <pubDate>Tue, 09 Feb 2016 13:15:00 +0900</pubDate>
        <link>/2016/02/EncoderDecoder</link>
        <guid isPermaLink="true">/2016/02/EncoderDecoder</guid>
        
        
      </item>
    
      <item>
        <title>SENSEMBED：Learning Sense Embeddings for Word and Relational Similarity</title>
        <description>&lt;h4 id=&quot;section&quot;&gt;概要&lt;/h4&gt;
&lt;p&gt;ACL2015の&lt;a href=&quot;http://wwwusers.di.uniroma1.it/~navigli/pubs/ACL_2015_Iacobaccietal.pdf&quot;&gt;論文&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;nzwは，手法の部分が気になったので，その部分をかいつまんで説明します．（といっても数行）&lt;/p&gt;

&lt;p&gt;word2vecやGloVeといったツールで獲得できる分散表現は，多義語でも1つのベクトルとして表現する．
例えば &lt;em&gt;bank&lt;/em&gt; は，土手の意味と銀行の意味をもっているがこれを1つのベクトルで表現している．
この論文は，語義ごとにベクトルを学習できれば，より分散表現として優れるのではという研究．&lt;/p&gt;

&lt;p&gt;モデル自体は単純で，まずWSDのSOTAな手法を使ってコーパスの単語ごとに語義を付与する．
次にCBoWで分散表現を学習する．
このときに同じ単語でも別の語義は付与されていれば別の単語とみなす．
この部分だけ気になっていたのでここらへんで読むのをやめた．&lt;/p&gt;

&lt;p&gt;（コーパスだけを使って語義を取れるものだと勘違いしていた）&lt;/p&gt;

&lt;p&gt;おそらくメインは残りで，類似度計算をCosine similarityではなく，語義を考慮したような式をいくつか提案している．
それをもとに既存手法と順位相関係数で評価．&lt;/p&gt;

&lt;h4 id=&quot;section-1&quot;&gt;気になったこと&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;analogyの評価指標にaccuracyがでてこないので，Mikolov論文やglove論文などの評価と違う&lt;/li&gt;
  &lt;li&gt;分散表現の良さは，ベクトルの演算と単語の演算（？）の対応がうまくできていて，扱いやすく計算が速いこと，という認識を持っていたのでだけど，この手法だと計算が重そう．
    &lt;ul&gt;
      &lt;li&gt;（出してないということは普通にベクトルとして計算してもあんまり性能が上がらなかった？）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Wed, 27 Jan 2016 18:15:04 +0900</pubDate>
        <link>/2016/01/senseembed</link>
        <guid isPermaLink="true">/2016/01/senseembed</guid>
        
        
      </item>
    
      <item>
        <title>Topic Modeling for Word Sense Induction</title>
        <description>&lt;h4 id=&quot;section&quot;&gt;概要&lt;/h4&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Word Sense Induction&lt;/code&gt;（WSI）のタスクに&lt;code class=&quot;highlighter-rouge&quot;&gt;LDA&lt;/code&gt;を使ってみましたという&lt;a href=&quot;http://publications.wim.uni-mannheim.de/informatik/lski/Knopp13TopicModelingforWSI.pdf&quot;&gt;2013年のGSCLの会議論文&lt;/a&gt;．&lt;/p&gt;

&lt;p&gt;WSIでは，出現した単語ごとの文脈などを単位としてベクトルを作り，それを&lt;code class=&quot;highlighter-rouge&quot;&gt;k-means&lt;/code&gt;でクラスタリングすることで，出現した位置における語義を分けられるかというやり方がある．
トピックが語義に対応しているという仮定して，そのベクトルを作る際に&lt;code class=&quot;highlighter-rouge&quot;&gt;LDA&lt;/code&gt;で得られるトピックごとの単語の確率分布\(\phi\)を使ってやろうというもの．
なので&lt;code class=&quot;highlighter-rouge&quot;&gt;LDA&lt;/code&gt;を拡張したわけではないのでわりと直感的である．（&lt;code class=&quot;highlighter-rouge&quot;&gt;gensim&lt;/code&gt;+&lt;code class=&quot;highlighter-rouge&quot;&gt;scikit-learn&lt;/code&gt;+&lt;code class=&quot;highlighter-rouge&quot;&gt;NLTK&lt;/code&gt;さえあればできそう）&lt;/p&gt;

&lt;h4 id=&quot;section-1&quot;&gt;気になったこと&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;トピック数が&lt;code class=&quot;highlighter-rouge&quot;&gt;3~10&lt;/code&gt;と比較的とかなり小さい(この分野ではそうなんだろうか)&lt;/li&gt;
  &lt;li&gt;評価には&lt;code class=&quot;highlighter-rouge&quot;&gt;WordNet&lt;/code&gt;と&lt;code class=&quot;highlighter-rouge&quot;&gt;SeｍEval2010&lt;/code&gt;を使ってる（名詞と動詞だけで評価，評価指標は&lt;code class=&quot;highlighter-rouge&quot;&gt;F-score&lt;/code&gt;と&lt;code class=&quot;highlighter-rouge&quot;&gt;V-measure&lt;/code&gt;）&lt;/li&gt;
  &lt;li&gt;クラスタリングの手法は複数試したほうが良さそう（WSIのsurveyでいくつ言及あり）&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 20 Dec 2015 13:18:00 +0900</pubDate>
        <link>/2015/12/TopicModelingforWordSenseInduction</link>
        <guid isPermaLink="true">/2015/12/TopicModelingforWordSenseInduction</guid>
        
        
      </item>
    
      <item>
        <title>Gadflyの紹介</title>
        <description>&lt;h3 id=&quot;section&quot;&gt;はじめに&lt;/h3&gt;

&lt;p&gt;卒論の時期になりました．
私もクリスマス締め切りの卒論を書いています．&lt;/p&gt;

&lt;p&gt;さて，卒論の実験結果や統計処理した結果をグラフに出す必要がある方はけっこういるのではないでしょうか．&lt;/p&gt;

&lt;p&gt;というわけで&lt;a href=&quot;http://gadflyjl.org/&quot;&gt;Gadfly&lt;/a&gt;を紹介します．
夏休みに書いた論文の図はGadflyで描きましたし，輪講で使うグラフもGadflyで描きました．
ゼミ生の中で好評でした✌️&lt;/p&gt;

&lt;p&gt;実行環境はMacを前提としていますので適宜お手元の環境に読み替えてください．&lt;/p&gt;

&lt;h3 id=&quot;gadfly&quot;&gt;Gadflyって何？&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;http://julialang.org/&quot;&gt;julia&lt;/a&gt;という技術計算向けの高速でイケてる言語の描画パッケージの1つです．&lt;/p&gt;

&lt;p&gt;手軽にかっこいい画像が作れます．&lt;/p&gt;

&lt;p&gt;保存形式は以下の6つに対応しています．&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;SVG&lt;/li&gt;
  &lt;li&gt;SVGJS&lt;/li&gt;
  &lt;li&gt;PNG&lt;/li&gt;
  &lt;li&gt;PDF&lt;/li&gt;
  &lt;li&gt;PS&lt;/li&gt;
  &lt;li&gt;PGF&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;nzwはPDFで出力してTeXやパワーポイントに貼り付けてることが多いです．&lt;/p&gt;

&lt;p&gt;ちなみにGadfly以外にもいろいろとあるようです．-&amp;gt; &lt;a href=&quot;https://en.wikibooks.org/wiki/Introducing_Julia/Plotting&quot;&gt;julia plotting&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;導入&lt;/h3&gt;

&lt;p&gt;juliaの導入は&lt;code class=&quot;highlighter-rouge&quot;&gt;homebrew&lt;/code&gt;なりを使ってやるとして，juliaを起動したら以下のコマンドでGadflyをインストールします．&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-julia&quot; data-lang=&quot;julia&quot;&gt;julia&amp;gt; Pkg.add(&quot;Gadfly&quot;)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;インストールが成功したら，以下のコマンドを実行すればplotができるようになります．&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-julia&quot; data-lang=&quot;julia&quot;&gt;julia&amp;gt; using Gadfly&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;iris&lt;/code&gt;(アヤメという多年草)のデータを例にあげることがあるので&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-julia&quot; data-lang=&quot;julia&quot;&gt;Pkg.add(&quot;RDatasets&quot;)
using RDatasets&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;をあらかじめやっておきます．&lt;/p&gt;

&lt;p&gt;全て紹介すると大変なので，ここでは自分がよく使うものだけ紹介します．&lt;/p&gt;

&lt;h3 id=&quot;plot&quot;&gt;自分がよくつかうplot&lt;/h3&gt;
&lt;p&gt;まずは簡単な散布図から．&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-julia&quot; data-lang=&quot;julia&quot;&gt;plot(x=rand(13),y=rand(13))&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/images/gadfly1.svg&quot; alt=&quot;ex1&quot; /&gt;
はい，綺麗ですね．うっとりします．&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;rand(13)&lt;/code&gt;は13個の乱数がはいった&lt;code class=&quot;highlighter-rouge&quot;&gt;Array&lt;/code&gt;を生成しています．
&lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt;と&lt;code class=&quot;highlighter-rouge&quot;&gt;y&lt;/code&gt;でindexが同じものを1つのデータ点としてplotしています．&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;plotする図によってパラメータが違っていて散布図(point)であれば&lt;code class=&quot;highlighter-rouge&quot;&gt;color&lt;/code&gt;が指定できます．&lt;/p&gt;

&lt;p&gt;この例では，アヤメの品種にごとに色わけしています．
irisのデータは&lt;code class=&quot;highlighter-rouge&quot;&gt;DataFrame&lt;/code&gt;(excelの表みたいなやつ)に格納されているので，x軸，y軸を指定する必要があります．&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-julia&quot; data-lang=&quot;julia&quot;&gt;plot(dataset(&quot;datasets&quot;, &quot;iris&quot;), x=&quot;PetalLength&quot;, y=&quot;PetalWidth&quot;, color=&quot;Species&quot;, Geom.point)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/images/gadfly2.svg&quot; alt=&quot;ex2&quot; /&gt;
イィ…&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;言語処理をやっているのでword2vec(分散表現)の例を示すことがあります．
ざっくり分散表現を説明すると，密なベクトル空間の1点に単語をマッピングすることです．&lt;/p&gt;

&lt;p&gt;例えば，以下の4行からなるCSVファイル&lt;code class=&quot;highlighter-rouge&quot;&gt;w2v.csv&lt;/code&gt;がjuliaのREPLを起動したディレクトリにあったとして描画します．&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-julia&quot; data-lang=&quot;julia&quot;&gt;x,y,word
1.0,2.0,man
2.0,3.0,woman
2.0,2.0,king
3.0,3.0,queen&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-julia&quot; data-lang=&quot;julia&quot;&gt;df = readtable(&quot;w2v.csv&quot;)
plot(df, x=&quot;x&quot;, y=&quot;y&quot;, label=&quot;word&quot;, Geom.point, Geom.label, Scale.x_continuous(minvalue=0.0, maxvalue=4.0),Scale.y_continuous(minvalue=0.0, maxvalue=4.0))&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;df&lt;/code&gt;はCSVファイルを読み込んだDataFrameです．&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;x=&quot;x&quot;&lt;/code&gt;の右辺で&lt;code class=&quot;highlighter-rouge&quot;&gt;df&lt;/code&gt;の列名を指定します．&lt;code class=&quot;highlighter-rouge&quot;&gt;label=&quot;word&quot;&lt;/code&gt;についても同様です．&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Scale.x_continuous(minvalue=0.0, maxvalue=4.0)&lt;/code&gt;は，描画するx軸の最小値最大値の範囲を指定します．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/w2v.svg&quot; alt=&quot;ex1&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;関数の描画も可能です．
例えばlogistic関数とtanhについて \(-5 \leq x \leq 5\) を定義域とした場合で描画します．&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-julia&quot; data-lang=&quot;julia&quot;&gt;f(x) = 1/(1+exp(-x))
plot([f,tanh], -5, 5, color=repeat([&quot;logistic&quot;, &quot;tanh&quot;]))&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/images/func.svg&quot; alt=&quot;ex1&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;subplot_grid&lt;/code&gt;を使うと複数のグラフを並べることができます．
この例ではアルゴリズムごとに棒グラフをわけて描画してます．&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-julia&quot; data-lang=&quot;julia&quot;&gt;data = DataFrame()
data[:Algorithm] = [&quot;methodA&quot;, &quot;methodA&quot;, &quot;methodB&quot;, &quot;methodB&quot;, &quot;proposal&quot;, &quot;proposal&quot;]
data[:Dataset] = [&quot;data1&quot;, &quot;data2&quot;, &quot;data1&quot;, &quot;data2&quot;, &quot;data1&quot;, &quot;data2&quot;]
data[:Precision] = [0.5, 0.4, 0.6, 0.62, 0.8, 0.9]

plot(data, xgroup=&quot;Algorithm&quot;, x=&quot;Dataset&quot;, y=&quot;Precision&quot;, color=&quot;Dataset&quot;,
  Scale.y_continuous(minvalue=0),
  Guide.ylabel(&quot;正解率&quot;),
  Guide.title(&quot;テストデータに対する正解率の比較&quot;),
  Geom.subplot_grid(Geom.bar()))&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/images/bar.svg&quot; alt=&quot;ex&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;おわりに&lt;/h3&gt;
&lt;p&gt;紹介しきれませんでしたが，フォントサイズや表示する目盛などを調整できます．&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://gadflyjl.org/&quot;&gt;Docs&lt;/a&gt;にまとまっているのでそちらをご参照ください．&lt;/p&gt;

&lt;p&gt;以上 &lt;a href=&quot;http://www.adventar.org/calendars/1005&quot;&gt;klis advent calendar 2015&lt;/a&gt; &amp;amp; &lt;a href=&quot;http://www.adventar.org/calendars/1315&quot;&gt;卒研 advent calendar 2015&lt;/a&gt;の8日目の記事でした．&lt;/p&gt;
</description>
        <pubDate>Tue, 08 Dec 2015 12:18:00 +0900</pubDate>
        <link>/2015/12/gadfly</link>
        <guid isPermaLink="true">/2015/12/gadfly</guid>
        
        
      </item>
    
      <item>
        <title>青トピックモデル 混合ユニグラムモデルの周辺化ギブスサンプリングのサンプル式</title>
        <description>&lt;p&gt;ユニグラムモデルにおいて，トピックは文書集合に対して1つだけ存在した．
一方，混合ユニグラムモデルは，トピック \(\phi\) は&lt;code class=&quot;highlighter-rouge&quot;&gt;K&lt;/code&gt;個存在する．(これをまとめたものを\(\Phi\)とかく)
そのため各文書は，&lt;code class=&quot;highlighter-rouge&quot;&gt;K&lt;/code&gt;個のうちの1トピックをもつ．
1つの\(\phi\)は，単語のカテゴリカル分布になる．
加えて\(\theta\)はトピック分布を表す．（各トピックがどれだけ出現しやすいかを表現）
これは混合ユニグラムモデルの中で1つだけ存在する．&lt;/p&gt;

&lt;p&gt;残りはユニグラムモデルやLDAと一緒で&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;α&lt;/code&gt;及び&lt;code class=&quot;highlighter-rouge&quot;&gt;β&lt;/code&gt;：ディリクレ分布のパラメータ&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;D&lt;/code&gt;：文書集合の総数&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;N&lt;/code&gt;：文書\(d\)の単語数&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;混合ユニグラムモデルをグラフィカルモデルで描くと以下のようになる．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/mix.svg&quot; alt=&quot;nn&quot; /&gt;&lt;/p&gt;

&lt;p&gt;新聞記事で例えると，記事はいくつかのトピック（スポーツとか政治とか）のどれか一つをもっていると仮定している．
そして各トピックごとに出現しやすい単語の確率分布は異なる．
（スポーツのトピックであれば”年棒, 野球, サッカー”といった単語が高確率で出現し，”選挙, 違法献金, 年金”は低確率で出現）&lt;/p&gt;

&lt;h3 id=&quot;section&quot;&gt;ベイズ推定&lt;/h3&gt;

&lt;p&gt;ベイズ推定を使う場合，文書の生成確率を定義してから，事後確率分布を求める．
混合ユニグラムモデルの文書の生成確率は以下のようにかける．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
p(\mathbf{W}|\Phi,\theta)
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;事後確率は，\(p(\bf{z},\Phi,\theta|\bf{W})\) となる．
ここで&lt;a href=&quot;http://amzn.to/1SEuqld&quot;&gt;青トピックモデル本&lt;/a&gt;では，崩壊型ギブスサンプリングを導入するが頭がわるくてついていけなかったので少し丁寧にやってみる．&lt;/p&gt;

&lt;p&gt;事後確率\(p(\bf{z},\Phi,\theta|\mathbf{W})\) をギブスサンプリングする場合は，\(\mathbf{z},\Phi,\theta\)のうち2つを固定して1つをサンプルする．
崩壊型ギブスサンプリングでは，\(\Phi,\theta\)を積分消去して\(\mathbf{z}\)を直接求める．
この辺りについては&lt;a href=&quot;http://amzn.to/1Nbzlq3&quot;&gt;白いトピックモデル本&lt;/a&gt;が大変参考になる．&lt;/p&gt;

&lt;p&gt;つまりサンプリング式は \(p(z_d=k|\mathbf{z}_{-d}, \mathbf{W}, \alpha, \beta)\)で，これ計算したい．&lt;/p&gt;

&lt;p&gt;ベイズの定理を使い，比例式を求める(青トピック本p51の式がこれに相当する)．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
p(z_d=k | \mathbf{z}_{-d}, \mathbf{W}, \alpha, \beta) &amp;amp;=&amp;amp;
\frac{p(z_{i} = k, \bf{z}_{-d}, \bf{W} | \alpha, \beta)}{p(\bf{z}_{-d}, \bf{W} | \alpha, \beta)}  \tag{1} \\&lt;br /&gt;
&amp;amp; \propto &amp;amp; p(z_d=k, \mathbf{z}_{-d}, \mathbf{W}| \alpha, \beta)  \tag{2} \\&lt;br /&gt;
&amp;amp;= &amp;amp; p(\mathbf{w}_{d} | \mathbf{W}_{-d}, z_d=k, \mathbf{z}_{-d}, \alpha, \beta) \\\ &amp;amp; &amp;amp; \times p(\mathbf{W}_{-d}, z_d=k, \mathbf{z}_{-d} | \alpha, \beta)  \tag{3} \\&lt;br /&gt;
&amp;amp;= &amp;amp; p(\mathbf{w}_d| \mathbf{W}_{-d}, z_d=k, \mathbf{z}_{-d}, \alpha, \beta) \times p(z_d=k| \mathbf{z}_{-d}, \mathbf{W}_{-d}, \alpha, \beta) \\\ &amp;amp; &amp;amp; \times
p(\mathbf{W}_{-d}, \mathbf{z}_{-d} | \alpha, \beta)  \tag{4} \\&lt;br /&gt;
&amp;amp; \propto &amp;amp; p(\mathbf{w}_d| \mathbf{W}_{-d}, z_d=k, \mathbf{z}_{-d}, \alpha, \beta) \\\ &amp;amp; &amp;amp; \times p(z_d=k| \mathbf{z}_{-d}, \mathbf{W}_{-d}, \alpha, \beta) \tag{5} \\&lt;br /&gt;
&amp;amp;= &amp;amp; p(\mathbf{w}_d| \mathbf{W}_{-d}, z_d=k, \mathbf{z}_{-d}, \beta) \times p(z_d=k| \mathbf{z}_{-d}, \alpha) \tag{6}
\end{eqnarray}&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;式(1)は，ベイズの定理で展開&lt;/li&gt;
  &lt;li&gt;式(2)は，分母に\(z_i=k\)がないので比例式で置き換え&lt;/li&gt;
  &lt;li&gt;式(3)は，ベイズの定理で1つの文書とそれ以外に展開&lt;/li&gt;
  &lt;li&gt;式(4)は，式(3)の2項目をベイズの定理で展開&lt;/li&gt;
  &lt;li&gt;式(5)は，式(4)の3項目に\(z_i=k\)がないので比例式で置き換え&lt;/li&gt;
  &lt;li&gt;式(6)は，条件付き独立を用いる&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;間違っていたらごめんなさい．&lt;/p&gt;

</description>
        <pubDate>Sun, 06 Dec 2015 18:00:00 +0900</pubDate>
        <link>/2015/12/mixgibbs</link>
        <guid isPermaLink="true">/2015/12/mixgibbs</guid>
        
        
      </item>
    
      <item>
        <title>青深層学習 4章 まとめ</title>
        <description>&lt;p&gt;4章誤差逆伝播法のまとめです．&lt;/p&gt;

&lt;p&gt;3章で扱った勾配降下法は，誤差関数の勾配を求めて重みを更新し，ネットワークの性能を高めるために使いました．&lt;/p&gt;

&lt;p&gt;誤差逆伝播法は，誤差関数の重みによる微分を効率良く計算するアルゴリズムです．&lt;/p&gt;

&lt;h4 id=&quot;section&quot;&gt;4.1 勾配計算の難しさ&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/images/nn1.svg&quot; alt=&quot;nn&quot; /&gt;&lt;/p&gt;

&lt;p&gt;順伝播によって得られた出力を使うため，重みが出力層から遠いほど（入力層に近いほど）chain ruleで展開する項の数が増えていきます．&lt;/p&gt;

&lt;p&gt;この場合では&lt;code class=&quot;highlighter-rouge&quot;&gt;hidden layer&lt;/code&gt;と&lt;code class=&quot;highlighter-rouge&quot;&gt;output layer&lt;/code&gt;の間の重みによる微分は比較的楽にできますが，&lt;code class=&quot;highlighter-rouge&quot;&gt;input layer&lt;/code&gt;と&lt;code class=&quot;highlighter-rouge&quot;&gt;hidden layer&lt;/code&gt;の間の重みによる微分は&lt;code class=&quot;highlighter-rouge&quot;&gt;hidden layer&lt;/code&gt;の活性化関数を介しているので複雑になると言えます．&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;4.2&lt;/code&gt;以降で導出する誤差逆伝播法を用いることで効率的に計算を行えるようになります．&lt;/p&gt;

&lt;h4 id=&quot;section-1&quot;&gt;4.2 2層ネットワークでの計算&lt;/h4&gt;
&lt;p&gt;隠れ層が1層のニューラルネットワークにおいて，重みは&lt;code class=&quot;highlighter-rouge&quot;&gt;input layer&lt;/code&gt;と&lt;code class=&quot;highlighter-rouge&quot;&gt;hidden layer&lt;/code&gt;の間と&lt;code class=&quot;highlighter-rouge&quot;&gt;hidden layer&lt;/code&gt;と&lt;code class=&quot;highlighter-rouge&quot;&gt;output layer&lt;/code&gt;の間の2つに分けることができます．&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://nzw0301.github.io/2015/11/blueDeepLearningChapter42/&quot;&gt;詳しい式の導出&lt;/a&gt;はこちらを参照．&lt;/p&gt;

&lt;p&gt;冒頭の&lt;code class=&quot;highlighter-rouge&quot;&gt;4.1&lt;/code&gt;で書いたように出力層から遠いほど式が複雑になっていることが確認できます．&lt;/p&gt;

&lt;h4 id=&quot;section-2&quot;&gt;4.3 多層ネットワークへの一般化&lt;/h4&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;4.2&lt;/code&gt;は2層でしたが，層の数を一般化したときの微分を求めてます．
なので，ここで誤差逆伝播法の本筋の話になります．&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://nzw0301.github.io/2015/11/blueDeepLearningChapter43/&quot;&gt;詳しい式の導出&lt;/a&gt;はこちらをみていただくとして，
肝心なのは， 各層ごとに\(\delta\)を\(L\)層（出力層）から逆順に求めることで，各重みによる微分を効率的に計算できることです．
\(\delta\)が計算できれば，重みの微分は別々に求めることができます．&lt;/p&gt;

&lt;h4 id=&quot;section-3&quot;&gt;4.4 勾配降下法の完全アルゴリズム&lt;/h4&gt;

&lt;p&gt;前半部分では，出力層の\(\delta\)の導出を行います．
&lt;a href=&quot;http://nzw0301.github.io/2015/11/blueDeepLearningChapter44/&quot;&gt;テキストより詳しい式変形をした&lt;/a&gt;ので，これからわかる通り，出力層の\(\delta\)は出力と正解の差です．&lt;/p&gt;

&lt;p&gt;後半部分では，行列表記による&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;順伝播法&lt;/li&gt;
  &lt;li&gt;誤差逆伝播法&lt;/li&gt;
  &lt;li&gt;重み\(w\)の更新&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;の式です．
詳細については，コーディングしやすいように行列の次元を意識して&lt;a href=&quot;http://nzw0301.github.io/2015/11/blueDeepLearningChapter442/&quot;&gt;かきました&lt;/a&gt;．&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;4.4.3 勾配の差分近似計算&lt;/code&gt;  では，多層になった場合に誤差関数の勾配計算の計算が正しいかを確かめるために，近似計算を扱います．
偏微分の定義から
十分小さい\(\epsilon\)を用いた式1で検証できます．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
\frac{\partial E}{\partial w_{ji}^{(l)}} = \frac{E(…,w_{ji}^{(l)}+\epsilon,…)-E(…,w_{ji}^{(l)},…)}{\epsilon} \tag{1}
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;微分なので\(\epsilon\)は十分小さな値を選択する必要があります．
注意点として計算機の特性上，打ち切り誤差や丸めの誤差などで誤差が大きくなるため以下のように
&lt;code class=&quot;highlighter-rouge&quot;&gt;計算機イプシロン&lt;/code&gt;(計算機の浮動小数点数で表現できる1より大きい最小の数と1との差のこと)\(\epsilon_c\)を使い，以下のように近似式の\(\epsilon\)を決定します．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
\epsilon = \sqrt{\epsilon_c} |w_{ji}|
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;python&lt;/code&gt;の&lt;code class=&quot;highlighter-rouge&quot;&gt;numpy&lt;/code&gt;でいえば&lt;code class=&quot;highlighter-rouge&quot;&gt;np.finfo(float).eps&lt;/code&gt;が計算機イプシロンです．
(手元では&lt;code class=&quot;highlighter-rouge&quot;&gt;2.2204460492503131e-16&lt;/code&gt;でした)&lt;/p&gt;

&lt;p&gt;? 実際につかうんでしょうか？&lt;/p&gt;

&lt;h4 id=&quot;section-4&quot;&gt;4.5 勾配消失問題&lt;/h4&gt;

&lt;p&gt;順伝播は活性化関数を経由するので活性化関数が非線形ならば，この層の出力も非線形であり，発散しません．（ロジスティック関数が活性化関数ならば0から1の間に収まる）&lt;/p&gt;

&lt;p&gt;対して逆伝播法は定義(4.13)のように線形な計算なので，層が深くなるほど発散しやすい（あるいは一度0になるとずっと0）になってしまうため学習がうまくいかない&lt;code class=&quot;highlighter-rouge&quot;&gt;勾配消失問題&lt;/code&gt;に直面します．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
\delta_j^{(l)} &amp;amp;=&amp;amp; \sum_k \delta_k^{(l+1)} (w_{kj}^{( l+1)} f’(u_j^{(l)})) \tag{4.12}\\
\end{eqnarray}&lt;/p&gt;

&lt;h4 id=&quot;section-5&quot;&gt;補足&lt;/h4&gt;

&lt;p&gt;十分多い隠れ層が1層あれば任意の入出力の関係を実現できるが，タスクよっては層を増やしたほうが必要なノード数は少なく済む．
ただし，層を増やしすぎると勾配消失が起こる．[深層学習, 人工知能学会]&lt;/p&gt;

&lt;h4 id=&quot;section-6&quot;&gt;参考&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;深層学習. 人工知能学会, 神嶌 敏弘, 麻生 英樹, 安田 宗樹, 前田 新一, 岡野原 大輔, 岡谷 貴之, 久保 陽太郎, Bollegala Danushka. 近代科学社, 2015.&lt;/li&gt;
  &lt;li&gt;深層学習 = Deep learning. 岡谷 貴之. 講談社, MLP機械学習プロフェッショナルシリーズ, 2015.&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Fri, 13 Nov 2015 21:00:00 +0900</pubDate>
        <link>/2015/11/backpropagation</link>
        <guid isPermaLink="true">/2015/11/backpropagation</guid>
        
        
      </item>
    
      <item>
        <title>青深層学習 4章 実装</title>
        <description>&lt;h4 id=&quot;section&quot;&gt;はじめに&lt;/h4&gt;
&lt;p&gt;前回まで数式の展開をしていたので，それをもとにして実装を行いました．&lt;/p&gt;

&lt;p&gt;Pythonを使いました．&lt;/p&gt;

&lt;h4 id=&quot;section-1&quot;&gt;データとタスク&lt;/h4&gt;

&lt;p&gt;データは例によって夢野久作の作品(&lt;a href=&quot;http://www.aozora.gr.jp/cards/000096/files/2122_21847.html&quot;&gt;オンチ&lt;/a&gt;)を使います．&lt;/p&gt;

&lt;p&gt;1文が登場人物の発言なのか，地の文なのかの2値分類を行います．
活性化関数はすべてロジスティック関数，損失関数は対数尤度です．
（なので出力層のユニット数は1つ）&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;tag sentence&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;1 退け 退け ッ&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;0 疎ら に なっ た 群衆 の 背後 から 、 今 出 た ばかり の 旭 が キラキラ と 映し 込ん で 来 た 。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;このように地の文であれば0，発言であれば1をつけます．&lt;/p&gt;

&lt;h4 id=&quot;section-2&quot;&gt;コード&lt;/h4&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;hidden_layer&lt;/code&gt; で指定する層を変えても問題ないように作ってあるので層の増減とユニット数の増減は好きに試すことができます．&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/nzw0301/363b803268c2ece127f2.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;テストデータは作ってないので，パラメータによる誤差関数の変動のグラフをみて投稿を締めようかと思います．&lt;/p&gt;

&lt;p&gt;上記のコード
1つ目がミニバッチを使った確率的勾配法
2つ目が全データで学習する勾配法&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/nn_err_rate.svg&quot; alt=&quot;nn&quot; /&gt;&lt;/p&gt;

&lt;p&gt;学習回数&lt;code class=&quot;highlighter-rouge&quot;&gt;400&lt;/code&gt;，学習率&lt;code class=&quot;highlighter-rouge&quot;&gt;0.2&lt;/code&gt;で固定し，隠れ層をいじってみます．
&lt;img src=&quot;/images/nn_hidden.svg&quot; alt=&quot;nn&quot; /&gt;&lt;/p&gt;

&lt;p&gt;学習率&lt;code class=&quot;highlighter-rouge&quot;&gt;0.2&lt;/code&gt;と隠れ層を固定し，確率的勾配法を使います．
ミニバッチによる変化を見ています．
&lt;img src=&quot;/images/nn_minibatch.svg&quot; alt=&quot;nn&quot; /&gt;&lt;/p&gt;

&lt;p&gt;以上です．&lt;/p&gt;
</description>
        <pubDate>Mon, 09 Nov 2015 21:00:00 +0900</pubDate>
        <link>/2015/11/blueDeepLearningChapter44code</link>
        <guid isPermaLink="true">/2015/11/blueDeepLearningChapter44code</guid>
        
        
      </item>
    
      <item>
        <title>青深層学習 4.4.2 行列表記</title>
        <description>&lt;h4 id=&quot;section&quot;&gt;はじめに&lt;/h4&gt;
&lt;p&gt;前回の続きです．&lt;/p&gt;

&lt;p&gt;順伝播，誤差逆伝播，重みとバイアスの更新を行列の表記で行います．&lt;/p&gt;

&lt;h4 id=&quot;section-1&quot;&gt;表記&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;\(N\) ：ミニバッチのデータ数&lt;/li&gt;
  &lt;li&gt;\(\boldsymbol{x}_n\)：1件のデータ(例えば1つ文書，行は単語の頻度とか)&lt;/li&gt;
  &lt;li&gt;\(\boldsymbol{X} = \left[ \begin{array}{c} \boldsymbol{x}_1 … \boldsymbol{x}_N \end{array} \right]\)：ミニバッチの行列&lt;/li&gt;
  &lt;li&gt;\(\boldsymbol{Y} = \left[ \begin{array}{c} \boldsymbol{y}_1 … \boldsymbol{y}_N \end{array} \right]\)：各データに対する最終的な出力を列にもつ行列&lt;/li&gt;
  &lt;li&gt;\(\boldsymbol{W^{(l)}} = w_{ji}^{(l)} \)：\(l\)層のおける重みの行列 要素はユニット\(i\)からユニット\(j\)のリンクの重み&lt;/li&gt;
  &lt;li&gt;\(\boldsymbol{b}^{(l)}\)：\(l\)層のバイアスのベクトル&lt;/li&gt;
  &lt;li&gt;\(\boldsymbol{u}^{(l)}_n\)：データ\(\boldsymbol{x}_n\)のときの，\(l\)層の入力ベクトル 1行目はユニットの1つ目の入力に対応&lt;/li&gt;
  &lt;li&gt;\(\boldsymbol{U}^{(l)} = \left[ \begin{array}{c} \boldsymbol{u}_1^{(l)} … \boldsymbol{u}_N^{(l)} \end{array} \right]\)：\(l\)層の入力の行列，列がデータ1件，行がユニットに対応&lt;/li&gt;
  &lt;li&gt;\(\boldsymbol{Z}^{(l)} = \left[ \begin{array}{c} \boldsymbol{z}_1^{(l)} … \boldsymbol{z}_N^{(l)} \end{array} \right]\)：\(l\)層の出力の行列，列がデータ1件，行がユニットに対応，\(\boldsymbol{U}^{(l)}\)の各要素に活性化を適用しただけ&lt;/li&gt;
  &lt;li&gt;\(\boldsymbol{\Delta^{(l)}}\)：列がミニバッチのデータ，行がユニットのデルタ\(\delta^{(l)}_j\)の行列&lt;/li&gt;
  &lt;li&gt;k：出力層のユニット数&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;section-2&quot;&gt;順伝播&lt;/h4&gt;

&lt;p&gt;入力は恒等写像なので， \(\boldsymbol{Z}^{(1)}=\boldsymbol{X}\) ．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
\boldsymbol{U}^{(l)} &amp;amp;=&amp;amp;\boldsymbol{W^{(l)}} \boldsymbol{Z^{(l-1)}} + \boldsymbol{b}^{(l)} \boldsymbol{1}_N^{T} \tag{1} \\
\boldsymbol{Z}^{(l)} &amp;amp;=&amp;amp; f^{(l)}(\boldsymbol{U}^{(l)}) \tag{2}
\end{eqnarray}&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(j\)：\((l)\)層のユニット数（バイアス除く）&lt;/li&gt;
  &lt;li&gt;\(i\)：\((l-1)\)層のユニット数（バイアス除く）&lt;/li&gt;
  &lt;li&gt;\(\boldsymbol{W}^{(l)}\)：\(j \times i\)&lt;/li&gt;
  &lt;li&gt;\(\boldsymbol{U}^{(l)}\)：\(j \times N\)&lt;/li&gt;
  &lt;li&gt;\(\boldsymbol{Z}^{(l)}\)：\(j \times N\)&lt;/li&gt;
  &lt;li&gt;\(\boldsymbol{b}^{(l)}\)：\(j \times 1\)&lt;/li&gt;
  &lt;li&gt;\(\boldsymbol{1}_N\)：\(N \times 1\)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;section-3&quot;&gt;逆伝播&lt;/h4&gt;

&lt;p&gt;出力層のデルタ\(\boldsymbol{\Delta^{(L)}} = \boldsymbol{D} - \boldsymbol{Y}\)
を\((L-1)\)~\(2\)まで逆伝播させる．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
\boldsymbol{\Delta}^{(l)} = f’^{(l)}(\boldsymbol{U}^{(l)}) \odot (\boldsymbol{W}^{(l+1)T} \boldsymbol{\Delta}^{(l+1)}) \tag{3}
\end{eqnarray}&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(\boldsymbol{\Delta^{(L)}} , \boldsymbol{D} , \boldsymbol{Y}\)：\(k \times N\)&lt;/li&gt;
  &lt;li&gt;\(\boldsymbol{\Delta^{(l)}} \)：\(j \times N\)&lt;/li&gt;
  &lt;li&gt;\(\odot\)：&lt;a href=&quot;https://ja.wikipedia.org/wiki/%E3%82%A2%E3%83%80%E3%83%9E%E3%83%BC%E3%83%AB%E7%A9%8D&quot; title=&quot;アダマール積&quot;&gt;アダマール積&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;section-4&quot;&gt;重み更新&lt;/h4&gt;
&lt;p&gt;逆伝播で計算したデルタを使って微分し，重み\(\boldsymbol{W}\)を更新．
誤差逆伝播法で漸化式を求めたので，重みの更新は並列して計算ができる．&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(\partial \boldsymbol{W}^{(l)} \)：\((l)\)層の重み\(w_{ji}^{(l)}\)で誤差関数\(E=\sum_{n=1}^N E_n(\boldsymbol{W})\)を微分した値を\((j,i)\)成分にもつ行列&lt;/li&gt;
  &lt;li&gt;\(\partial \boldsymbol{b}^{(l)} \)：\((l)\)層の重み\(b_{j}^{(l)}\)で誤差関数\(E=\sum_{n=1}^N E_n(\boldsymbol{W})\)を微分した値を\((j)\)成分にもつ列ベクトル&lt;/li&gt;
  &lt;li&gt;\(\epsilon\)：学習率&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;\begin{eqnarray}
\partial \boldsymbol{W}^{(l)} &amp;amp;=&amp;amp; \frac{1}{N} \boldsymbol{\Delta^{(l)}} \boldsymbol{Z^{(l-1)T}} \tag{4}\\
\partial \boldsymbol{b}^{(l)} &amp;amp;=&amp;amp; \frac{1}{N} \boldsymbol{\Delta^{(l)}} \boldsymbol{1}_N \tag{5}
\end{eqnarray}&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(\partial \boldsymbol{W}^{(l)}\)：\(j \times i\)&lt;/li&gt;
  &lt;li&gt;\(\partial \boldsymbol{b}^{(l)}\)：\(j \times 1\)&lt;/li&gt;
  &lt;li&gt;\(\boldsymbol{Z^{(l-1)}} \)：\(i \times N\)&lt;/li&gt;
  &lt;li&gt;\(\boldsymbol{\Delta^{(l)}} \)：\(j \times N\)&lt;/li&gt;
  &lt;li&gt;\(j\)：\((l)\)層のユニット数（バイアス除く）&lt;/li&gt;
  &lt;li&gt;\(i\)：\((l-1)\)層のユニット数（バイアス除く）&lt;/li&gt;
  &lt;li&gt;\(N\)：ミニバッチのデータ数&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;更新式は，&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
\boldsymbol{W}^{(l)} &amp;amp;\leftarrow&amp;amp;
\boldsymbol{W}^{(l)}
- \epsilon \partial \boldsymbol{W}^{(l)} \tag{6} \\ 
\boldsymbol{b}^{(l)} &amp;amp;\leftarrow&amp;amp;
\boldsymbol{b}^{(l)}
- \epsilon \partial \boldsymbol{b}^{(l)} \tag{7} \\ 
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;以上です．&lt;/p&gt;
</description>
        <pubDate>Mon, 09 Nov 2015 01:22:00 +0900</pubDate>
        <link>/2015/11/blueDeepLearningChapter442</link>
        <guid isPermaLink="true">/2015/11/blueDeepLearningChapter442</guid>
        
        
      </item>
    
  </channel>
</rss>
