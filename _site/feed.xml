<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>
    <description>not δ</description>
    <link>/</link>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Wed, 17 Feb 2016 21:01:46 +0900</pubDate>
    <lastBuildDate>Wed, 17 Feb 2016 21:01:46 +0900</lastBuildDate>
    <generator>Jekyll v3.0.3</generator>
    
      <item>
        <title>joint topic modelのサンプリング式導出</title>
        <description>&lt;h4 id=&quot;section&quot;&gt;はじめに&lt;/h4&gt;
&lt;p&gt;間違っていたら&lt;a href=&quot;https://twitter.com/nzw0301&quot;&gt;nzw&lt;/a&gt;までお願いします．&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://amzn.to/1Nbzlq3&quot;&gt;トピックモデルによる統計的潜在意味解析&lt;/a&gt;の3章の周辺化ギブスサンプリングの式を参考にjoint topic modelのサンプリング式を導出します．&lt;/p&gt;

&lt;p&gt;まずグラフィカルモデルを&lt;a href=&quot;https://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf&quot;&gt;Blei+, 2003&lt;/a&gt;のものと合わせて示します．&lt;/p&gt;

&lt;p&gt;topic model
&lt;img src=&quot;/images/topic_model.svg&quot; alt=&quot;gmodel&quot; /&gt;&lt;/p&gt;

&lt;p&gt;joint topic model
&lt;img src=&quot;/images/joint_topic_model.svg&quot; alt=&quot;gmodel&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf&quot;&gt;Blei+, 2003&lt;/a&gt;とほぼ同じ形をしていますが， 観測できる変数が1つ増え，それに合わせて潜在変数も増えています．&lt;/p&gt;

&lt;p&gt;通常のLDAでは1つの文書集合が与えられますが，このモデルでは，さらにそれぞれの文書ごとに補助情報が与えられます．
補助情報ごとに &lt;script type=&quot;math/tex&quot;&gt;N_{*}&lt;/script&gt;のプレートと&lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;と&lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt;が増えます．&lt;/p&gt;

&lt;p&gt;このようなデータの例としては，&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;レシピの手順をひとまとめにしたもとを1文書とし，そのレシピについているつくれぽ&lt;/li&gt;
  &lt;li&gt;日本語のwikipediaの1記事と対応する英語のwikipedia1記事&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;などが挙げられます．
（WSIにこのモデルを使った論文では，context–windowとPOSを使っていました）&lt;/p&gt;

&lt;p&gt;示したグラフィカルモデルでは&lt;script type=&quot;math/tex&quot;&gt;N_{*}&lt;/script&gt;のプレートは2つですが，いくつでも増やせます．（wikipediaの例を使うと，&lt;code class=&quot;highlighter-rouge&quot;&gt;[英語, 日本語, ロシア語]&lt;/code&gt; など）&lt;/p&gt;

&lt;h4 id=&quot;section-1&quot;&gt;本題&lt;/h4&gt;

&lt;p&gt;『トピックモデルによる〜』のp55を踏襲します．&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;z_{d,i}^{1}&lt;/script&gt; は1番目の文書集合の&lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt;番目の文書の&lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;番目の単語のトピックです．
このとき&lt;script type=&quot;math/tex&quot;&gt;w_{d,i}^{1}&lt;/script&gt; は1番目の文書集合の&lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt;番目の文書の&lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;番目の単語です．&lt;/p&gt;

&lt;p&gt;同様に，&lt;script type=&quot;math/tex&quot;&gt;z_{d,i}^{2}&lt;/script&gt;は2番目の文書集合の&lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt;番目の文書の&lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;番目の単語のトピックで，&lt;script type=&quot;math/tex&quot;&gt;w_{d,i}^{2}&lt;/script&gt;は2番目の文書集合の&lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt;番目の文書の&lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;番目の単語です．
文書集合の要素に対応関係が取れていれば，対応する文書長は一致しなくても構いません．&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;{\boldsymbol \beta}^1&lt;/script&gt; は，1番目の文書集合の語彙数次元あるディリクレ分布のパラメータベクトル，&lt;script type=&quot;math/tex&quot;&gt;{\boldsymbol \beta}^2&lt;/script&gt; は，2番目の文書集合の語彙数次元あるディリクレ分布のパラメータベクトルです．
&lt;script type=&quot;math/tex&quot;&gt;{\boldsymbol \beta}&lt;/script&gt; の次元数は一致しなくても構いません．&lt;/p&gt;

&lt;p&gt;このとき，周辺化ギブスサンプリングにおける&lt;script type=&quot;math/tex&quot;&gt;z_{d,i}^{1}&lt;/script&gt;のサンプリング式を考えます．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
  p(z_{d,i}^{1}=k|
    w_{d,i}^{1}=v, 
    \mathbf{W}^{1}_{\backslash d,i},
    \mathbf{W}^{2},
    \mathbf{Z}^{1}_{\backslash d,i},
    \mathbf{Z}^{2},
    {\boldsymbol \alpha},
    {\boldsymbol \beta}^{1},
    {\boldsymbol \beta}^{2})
  &amp;amp;=&amp;amp; \frac{p(z_{d,i}^{1}=k, w_{d,i}^{1}=v, 
      \mathbf{W}^{1}_{\backslash d,i},
      \mathbf{W}^{2},
      \mathbf{Z}^{1}_{\backslash d,i},
      \mathbf{Z}^{2},
      | {\boldsymbol \alpha},
      {\boldsymbol \beta}^{1},
      {\boldsymbol \beta}^{2})}
    {p(w_{d,i}^{1}=v, 
      \mathbf{W}^{1}_{\backslash d,i},
      \mathbf{W}^{2},
      \mathbf{Z}^{1}_{\backslash d,i},
      \mathbf{Z}^{2},
      | {\boldsymbol \alpha},
      {\boldsymbol \beta}^{1},
      {\boldsymbol \beta}^{2})} \tag{1}\\
  &amp;amp; \propto &amp;amp; p(z_{d,i}^{1}=k, w_{d,i}^{1}=v, 
      \mathbf{W}^{1}_{\backslash d,i},
      \mathbf{W}^{2},
      \mathbf{Z}^{1}_{\backslash d,i},
      \mathbf{Z}^{2},
      | {\boldsymbol \alpha},
      {\boldsymbol \beta}^{1},
      {\boldsymbol \beta}^{2}) \tag{2} \\
  &amp;amp;=&amp;amp; p(w_{d,i}^{1}=v | 
      z_{d,i}^{1}=k,
      \mathbf{W}^{1}_{\backslash d,i},
      \mathbf{W}^{2},
      \mathbf{Z}^{1}_{\backslash d,i},
      \mathbf{Z}^{2},
      {\boldsymbol \alpha},
      {\boldsymbol \beta}^{1},
      {\boldsymbol \beta}^{2}) \\
    &amp;amp; &amp;amp;\times 
      p(z_{d,i}^{1}=k,
      \mathbf{W}^{1}_{\backslash d,i},
      \mathbf{W}^{2},
      \mathbf{Z}^{1}_{\backslash d,i},
      \mathbf{Z}^{2}
      | {\boldsymbol \alpha},
      {\boldsymbol \beta}^{1},
      {\boldsymbol \beta}^{2}) \tag{3} \\
    &amp;amp;=&amp;amp; p(w_{d,i}^{1}=v | 
      z_{d,i}^{1}=k,
      \mathbf{W}^{1}_{\backslash d,i},
      \mathbf{W}^{2},
      \mathbf{Z}^{1}_{\backslash d,i},
      \mathbf{Z}^{2},
      {\boldsymbol \alpha},
      {\boldsymbol \beta}^{1},
      {\boldsymbol \beta}^{2}) \\
    &amp;amp; &amp;amp;\times 
      p(z_{d,i}^{1}=k|
      \mathbf{W}^{1}_{\backslash d,i},
      \mathbf{W}^{2},
      \mathbf{Z}^{1}_{\backslash d,i},
      \mathbf{Z}^{2}
      {\boldsymbol \alpha},
      {\boldsymbol \beta}^{1},
      {\boldsymbol \beta}^{2}) \\
    &amp;amp;&amp;amp; \times
      p(\mathbf{W}^{1}_{\backslash d,i},
      \mathbf{W}^{2},
      \mathbf{Z}^{1}_{\backslash d,i},
      \mathbf{Z}^{2}
      | {\boldsymbol \alpha},
      {\boldsymbol \beta}^{1},
      {\boldsymbol \beta}^{2}) \tag{4} \\
    &amp;amp; \propto &amp;amp; p(w_{d,i}^{1}=v | 
      z_{d,i}^{1}=k,
      \mathbf{W}^{1}_{\backslash d,i},
      \mathbf{W}^{2},
      \mathbf{Z}^{1}_{\backslash d,i},
      \mathbf{Z}^{2},
      {\boldsymbol \alpha},
      {\boldsymbol \beta}^{1},
      {\boldsymbol \beta}^{2}) \\
    &amp;amp; &amp;amp;\times 
      p(z_{d,i}^{1}=k|
      \mathbf{W}^{1}_{\backslash d,i},
      \mathbf{W}^{2},
      \mathbf{Z}^{1}_{\backslash d,i},
      \mathbf{Z}^{2}
      {\boldsymbol \alpha},
      {\boldsymbol \beta}^{1},
      {\boldsymbol \beta}^{2}) \tag{5} \\
    &amp;amp;=&amp;amp; \int p(w_{d,i}^{1}=v|{\boldsymbol \phi}^1_{k}) p({\boldsymbol \phi}^1_{k}| \mathbf{W}^{1}_{\backslash d,i},
      \mathbf{Z}^{1}_{\backslash d,i},
      {\boldsymbol \beta}^{1})  d {\boldsymbol \phi}^1_{k} \\
    &amp;amp;&amp;amp; \times  \int p(z_{d,i}^{1}=k|{\boldsymbol \theta}_{d})
     p({\boldsymbol \theta}_{d}|
      \mathbf{Z}^{1}_{\backslash d,i},
      \mathbf{Z}^{2},
      {\boldsymbol \alpha}) d {\boldsymbol \theta}_{d} \tag{6} \\
    &amp;amp;=&amp;amp; \int \phi_{k,v}^{1} \times p({\boldsymbol \phi}^1_{k}| \mathbf{W}^{1}_{\backslash d,i},
      \mathbf{Z}^{1}_{\backslash d,i},
      {\boldsymbol \beta}^{1})  d {\boldsymbol \phi}^1_{k} \\
    &amp;amp;&amp;amp; \times  \int \theta_{d,k} \times
     p({\boldsymbol \theta}_{d}|
      \mathbf{Z}^{1}_{\backslash d,i},
      \mathbf{Z}^{2},
      {\boldsymbol \alpha}) d {\boldsymbol \theta}_{d} \tag{6} \\ 
    &amp;amp;=&amp;amp; \mathbb{E}_{p({\boldsymbol \phi}^1_{k}| \mathbf{W}^{1}_  {\backslash d,i},\mathbf{Z}^{1}_{\backslash d,i},      {\boldsymbol \beta}^{1})}[\phi_{k,v}^{1}] 
    \mathbb{E}_{p({\boldsymbol \theta}_{d}|
      \mathbf{Z}^{1}_{\backslash d,i}, \mathbf{Z}^{2},{\boldsymbol \alpha}) d {\boldsymbol \theta}_{d}}[\theta_{d,k}] \tag{7} \\
    &amp;amp;=&amp;amp; \frac{n_{k,v, \backslash d,i}^{1}+\beta_{v}}{\sum_{v’} (n_{k,v’, \backslash d,i}^{1}+\beta_{v’})} 
        \frac{n_{d,k, \backslash d,i}^{1} + n_{d,k}^{2} + \alpha_k}{\sum_{k’} (n_{d, k’, \backslash d,i}^{1} + n_{d,k’}^{2} + \alpha_{k’})} \tag{8}
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;z_{d,i}^{2}&lt;/script&gt;の場合も同様に求まります．&lt;/p&gt;

&lt;p&gt;単語の分布(&lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt;)は，文書集合ごとに別なので，トピックモデルと同じ式に帰着します．
&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;は，各文書集合の潜在変数&lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;で繋がっていますが，ディリクレ分布の期待値計算から上記の式のように求まります．&lt;/p&gt;

&lt;h4 id=&quot;section-2&quot;&gt;実行例&lt;/h4&gt;
&lt;p&gt;以下のような2つの文書集合を使ってみました．
それぞれ1行が1文書とみなし，行ごとに対応関係（この例では対訳）であるとします．&lt;/p&gt;

&lt;hr /&gt;

&lt;blockquote&gt;
  &lt;p&gt;lisp lisp lisp scala scala scala clojure clojure&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;java java java scala scala scala&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;book book library library lisp lisp clojure clojure&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;writing writing book book library library&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;scala book lisp book java book clojure&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;movie movie article article book book mars mars&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;movie movie article article mars mars mars&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;blockquote&gt;
  &lt;p&gt;リスプ リスプ リスプ スカラ スカラ スカラ&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;ジャバ ジャバ ジャバ スカラ スカラ スカラ&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;本 本 図書館 図書館 リスプ リスプ&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;書く 書く 本 本 図書館 図書館&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;スカラ ジャバ リスプ 本 本 本&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;映画 映画 論文 論文 本 本 火星 火星&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;映画 映画 論文 論文 火星 火星 火星&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;p&gt;トピック数3くらいで実行したときの， &lt;script type=&quot;math/tex&quot;&gt;\phi_{k,v}&lt;/script&gt;の確率値の高い単語の一覧を示します．&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;英語
    &lt;ul&gt;
      &lt;li&gt;topic=0
        &lt;ul&gt;
          &lt;li&gt;mars 0.382442748092&lt;/li&gt;
          &lt;li&gt;article 0.306106870229&lt;/li&gt;
          &lt;li&gt;movie 0.306106870229&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;topic=1
        &lt;ul&gt;
          &lt;li&gt;book 0.596688741722&lt;/li&gt;
          &lt;li&gt;library 0.265562913907&lt;/li&gt;
          &lt;li&gt;writing 0.133112582781&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;topic=2
        &lt;ul&gt;
          &lt;li&gt;scala 0.317194570136&lt;/li&gt;
          &lt;li&gt;lisp 0.271945701357&lt;/li&gt;
          &lt;li&gt;clojure 0.226696832579&lt;/li&gt;
          &lt;li&gt;java 0.181447963801&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;日本語
    &lt;ul&gt;
      &lt;li&gt;topic=0
        &lt;ul&gt;
          &lt;li&gt;火星 0.382734912147&lt;/li&gt;
          &lt;li&gt;論文 0.306340718105&lt;/li&gt;
          &lt;li&gt;映画 0.306340718105&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;topic=1
        &lt;ul&gt;
          &lt;li&gt;本 0.597084161696&lt;/li&gt;
          &lt;li&gt;図書館 0.265738899934&lt;/li&gt;
          &lt;li&gt;書く 0.133200795229&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;topic=2
        &lt;ul&gt;
          &lt;li&gt;スカラ 0.410181392627&lt;/li&gt;
          &lt;li&gt;リスプ 0.351667641896&lt;/li&gt;
          &lt;li&gt;ジャバ 0.234640140433&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;このときトピックは共通しているため（同じ&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;から&lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;が生成されるため），言語が違っていても共通したトピックから生成される単語分布を求めることができます．&lt;/p&gt;

&lt;p&gt;コードはそのうちgithubで公開すると思います．&lt;/p&gt;

&lt;p&gt;以上です．&lt;/p&gt;

&lt;h4 id=&quot;section-3&quot;&gt;参考文献など&lt;/h4&gt;
&lt;p&gt;&lt;a href=&quot;https://bioinformatics.oxfordjournals.org/content/early/2005/05/26/bioinformatics.bti515.full.pdf&quot;&gt;青いトピックモデル本で言及されているjoint topic modelの論文&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 17 Feb 2016 21:00:00 +0900</pubDate>
        <link>/2016/02/jointTopicModelsEquation</link>
        <guid isPermaLink="true">/2016/02/jointTopicModelsEquation</guid>
        
        
      </item>
    
      <item>
        <title>joint topic modelのサンプリング式導出</title>
        <description>&lt;h4 id=&quot;section&quot;&gt;はじめに&lt;/h4&gt;
&lt;p&gt;間違っていたら&lt;a href=&quot;https://twitter.com/nzw0301&quot;&gt;nzw&lt;/a&gt;までお願いします．&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://amzn.to/1Nbzlq3&quot;&gt;トピックモデルによる統計的潜在意味解析&lt;/a&gt;の3章の周辺化ギブスサンプリングの式を参考にjoint topic modelのサンプリング式を導出します．&lt;/p&gt;

&lt;p&gt;まずグラフィカルモデルを&lt;a href=&quot;https://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf&quot;&gt;Blei+, 2003&lt;/a&gt;のものと合わせて示します．&lt;/p&gt;

&lt;p&gt;topic model
&lt;img src=&quot;/images/topic_model.svg&quot; alt=&quot;gmodel&quot; /&gt;&lt;/p&gt;

&lt;p&gt;joint topic model
&lt;img src=&quot;/images/joint_topic_model.svg&quot; alt=&quot;gmodel&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf&quot;&gt;Blei+, 2003&lt;/a&gt;とほぼ同じ形をしていますが， 観測できる変数が1つ増え，それに合わせて潜在変数も増えています．&lt;/p&gt;

&lt;p&gt;通常のLDAでは1つの文書集合が与えられますが，このモデルでは，さらにそれぞれの文書ごとに補助情報が与えられます．
補助情報ごとに &lt;script type=&quot;math/tex&quot;&gt;N_{*}&lt;/script&gt;のプレートと&lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;と&lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt;が増えます．&lt;/p&gt;

&lt;p&gt;このようなデータの例としては，&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;レシピの手順をひとまとめにしたもとを1文書とし，そのレシピについているつくれぽ&lt;/li&gt;
  &lt;li&gt;日本語のwikipediaの1記事と対応する英語のwikipedia1記事&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;などが挙げられます．
（WSIにこのモデルを使った論文では，context–windowとPOSを使っていました）&lt;/p&gt;

&lt;p&gt;示したグラフィカルモデルでは&lt;script type=&quot;math/tex&quot;&gt;N_{*}&lt;/script&gt;のプレートは2つですが，いくつでも増やせます．（wikipediaの例を使うと，&lt;code class=&quot;highlighter-rouge&quot;&gt;[英語, 日本語, ロシア語]&lt;/code&gt; など）&lt;/p&gt;

&lt;h4 id=&quot;section-1&quot;&gt;本題&lt;/h4&gt;

&lt;p&gt;『トピックモデルによる〜』のp55を踏襲します．&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;z_{d,i}^{1}&lt;/script&gt; は1番目の文書集合の&lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt;番目の文書の&lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;番目の単語のトピックです．
このとき&lt;script type=&quot;math/tex&quot;&gt;w_{d,i}^{1}&lt;/script&gt; は1番目の文書集合の&lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt;番目の文書の&lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;番目の単語です．&lt;/p&gt;

&lt;p&gt;同様に，&lt;script type=&quot;math/tex&quot;&gt;z_{d,i}^{2}&lt;/script&gt;は2番目の文書集合の&lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt;番目の文書の&lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;番目の単語のトピックで，&lt;script type=&quot;math/tex&quot;&gt;w_{d,i}^{2}&lt;/script&gt;は2番目の文書集合の&lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt;番目の文書の&lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;番目の単語です．
文書集合の要素に対応関係が取れていれば，対応する文書長は一致しなくても構いません．&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;{\boldsymbol \beta}^1&lt;/script&gt; は，1番目の文書集合の語彙数次元あるディリクレ分布のパラメータベクトル，&lt;script type=&quot;math/tex&quot;&gt;{\boldsymbol \beta}^2&lt;/script&gt; は，2番目の文書集合の語彙数次元あるディリクレ分布のパラメータベクトルです．
&lt;script type=&quot;math/tex&quot;&gt;{\boldsymbol \beta}&lt;/script&gt; の次元数は一致しなくても構いません．&lt;/p&gt;

&lt;p&gt;このとき，周辺化ギブスサンプリングにおける&lt;script type=&quot;math/tex&quot;&gt;z_{d,i}^{1}&lt;/script&gt;のサンプリング式を考えます．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
  p(z_{d,i}^{1}=k|
    w_{d,i}^{1}=v, 
    \mathbf{W}^{1}_{\backslash d,i},
    \mathbf{W}^{2},
    \mathbf{Z}^{1}_{\backslash d,i},
    \mathbf{Z}^{2},
    {\boldsymbol \alpha},
    {\boldsymbol \beta}^{1},
    {\boldsymbol \beta}^{2})
  &amp;amp;=&amp;amp; \frac{p(z_{d,i}^{1}=k, w_{d,i}^{1}=v, 
      \mathbf{W}^{1}_{\backslash d,i},
      \mathbf{W}^{2},
      \mathbf{Z}^{1}_{\backslash d,i},
      \mathbf{Z}^{2},
      | {\boldsymbol \alpha},
      {\boldsymbol \beta}^{1},
      {\boldsymbol \beta}^{2})}
    {p(w_{d,i}^{1}=v, 
      \mathbf{W}^{1}_{\backslash d,i},
      \mathbf{W}^{2},
      \mathbf{Z}^{1}_{\backslash d,i},
      \mathbf{Z}^{2},
      | {\boldsymbol \alpha},
      {\boldsymbol \beta}^{1},
      {\boldsymbol \beta}^{2})} \tag{1}\\
  &amp;amp; \propto &amp;amp; p(z_{d,i}^{1}=k, w_{d,i}^{1}=v, 
      \mathbf{W}^{1}_{\backslash d,i},
      \mathbf{W}^{2},
      \mathbf{Z}^{1}_{\backslash d,i},
      \mathbf{Z}^{2},
      | {\boldsymbol \alpha},
      {\boldsymbol \beta}^{1},
      {\boldsymbol \beta}^{2}) \tag{2} \\
  &amp;amp;=&amp;amp; p(w_{d,i}^{1}=v | 
      z_{d,i}^{1}=k,
      \mathbf{W}^{1}_{\backslash d,i},
      \mathbf{W}^{2},
      \mathbf{Z}^{1}_{\backslash d,i},
      \mathbf{Z}^{2},
      {\boldsymbol \alpha},
      {\boldsymbol \beta}^{1},
      {\boldsymbol \beta}^{2}) \\
    &amp;amp; &amp;amp;\times 
      p(z_{d,i}^{1}=k,
      \mathbf{W}^{1}_{\backslash d,i},
      \mathbf{W}^{2},
      \mathbf{Z}^{1}_{\backslash d,i},
      \mathbf{Z}^{2}
      | {\boldsymbol \alpha},
      {\boldsymbol \beta}^{1},
      {\boldsymbol \beta}^{2}) \tag{3} \\
    &amp;amp;=&amp;amp; p(w_{d,i}^{1}=v | 
      z_{d,i}^{1}=k,
      \mathbf{W}^{1}_{\backslash d,i},
      \mathbf{W}^{2},
      \mathbf{Z}^{1}_{\backslash d,i},
      \mathbf{Z}^{2},
      {\boldsymbol \alpha},
      {\boldsymbol \beta}^{1},
      {\boldsymbol \beta}^{2}) \\
    &amp;amp; &amp;amp;\times 
      p(z_{d,i}^{1}=k|
      \mathbf{W}^{1}_{\backslash d,i},
      \mathbf{W}^{2},
      \mathbf{Z}^{1}_{\backslash d,i},
      \mathbf{Z}^{2}
      {\boldsymbol \alpha},
      {\boldsymbol \beta}^{1},
      {\boldsymbol \beta}^{2}) \\
    &amp;amp;&amp;amp; \times
      p(\mathbf{W}^{1}_{\backslash d,i},
      \mathbf{W}^{2},
      \mathbf{Z}^{1}_{\backslash d,i},
      \mathbf{Z}^{2}
      | {\boldsymbol \alpha},
      {\boldsymbol \beta}^{1},
      {\boldsymbol \beta}^{2}) \tag{4} \\
    &amp;amp; \propto &amp;amp; p(w_{d,i}^{1}=v | 
      z_{d,i}^{1}=k,
      \mathbf{W}^{1}_{\backslash d,i},
      \mathbf{W}^{2},
      \mathbf{Z}^{1}_{\backslash d,i},
      \mathbf{Z}^{2},
      {\boldsymbol \alpha},
      {\boldsymbol \beta}^{1},
      {\boldsymbol \beta}^{2}) \\
    &amp;amp; &amp;amp;\times 
      p(z_{d,i}^{1}=k|
      \mathbf{W}^{1}_{\backslash d,i},
      \mathbf{W}^{2},
      \mathbf{Z}^{1}_{\backslash d,i},
      \mathbf{Z}^{2}
      {\boldsymbol \alpha},
      {\boldsymbol \beta}^{1},
      {\boldsymbol \beta}^{2}) \tag{5} \\
    &amp;amp;=&amp;amp; \int p(w_{d,i}^{1}=v|{\boldsymbol \phi}^1_{k}) p({\boldsymbol \phi}^1_{k}| \mathbf{W}^{1}_{\backslash d,i},
      \mathbf{Z}^{1}_{\backslash d,i},
      {\boldsymbol \beta}^{1})  d {\boldsymbol \phi}^1_{k} \\
    &amp;amp;&amp;amp; \times  \int p(z_{d,i}^{1}=k|{\boldsymbol \theta}_{d})
     p({\boldsymbol \theta}_{d}|
      \mathbf{Z}^{1}_{\backslash d,i},
      \mathbf{Z}^{2},
      {\boldsymbol \alpha}) d {\boldsymbol \theta}_{d} \tag{6} \\
    &amp;amp;=&amp;amp; \int \phi_{k,v}^{1} \times p({\boldsymbol \phi}^1_{k}| \mathbf{W}^{1}_{\backslash d,i},
      \mathbf{Z}^{1}_{\backslash d,i},
      {\boldsymbol \beta}^{1})  d {\boldsymbol \phi}^1_{k} \\
    &amp;amp;&amp;amp; \times  \int \theta_{d,k} \times
     p({\boldsymbol \theta}_{d}|
      \mathbf{Z}^{1}_{\backslash d,i},
      \mathbf{Z}^{2},
      {\boldsymbol \alpha}) d {\boldsymbol \theta}_{d} \tag{6} \\ 
    &amp;amp;=&amp;amp; \mathbb{E}_{p({\boldsymbol \phi}^1_{k}| \mathbf{W}^{1}_  {\backslash d,i},\mathbf{Z}^{1}_{\backslash d,i},      {\boldsymbol \beta}^{1})}[\phi_{k,v}^{1}] 
    \mathbb{E}_{p({\boldsymbol \theta}_{d}|
      \mathbf{Z}^{1}_{\backslash d,i}, \mathbf{Z}^{2},{\boldsymbol \alpha}) d {\boldsymbol \theta}_{d}}[\theta_{d,k}] \tag{7} \\
    &amp;amp;=&amp;amp; \frac{n_{k,v, \backslash d,i}^{1}+\beta_{v}}{\sum_{v’} (n_{k,v’, \backslash d,i}^{1}+\beta_{v’})} 
        \frac{n_{d,k, \backslash d,i}^{1} + n_{d,k}^{2} + \alpha_k}{\sum_{k’} (n_{d, k’, \backslash d,i}^{1} + n_{d,k’}^{2} + \alpha_{k’})} \tag{8}
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;z_{d,i}^{2}&lt;/script&gt;の場合も同様に求まります．&lt;/p&gt;

&lt;p&gt;単語の分布(&lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt;)は，文書集合ごとに別なので，トピックモデルと同じ式に帰着します．
&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;は，各文書集合の潜在変数&lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;で繋がっていますが，ディリクレ分布の期待値計算から上記の式のように求まります．&lt;/p&gt;

&lt;h4 id=&quot;section-2&quot;&gt;実行例&lt;/h4&gt;
&lt;p&gt;以下のような2つの文書集合を使ってみました．
それぞれ1行が1文書とみなし，行ごとに対応関係（この例では対訳）であるとします．&lt;/p&gt;

&lt;hr /&gt;

&lt;blockquote&gt;
  &lt;p&gt;lisp lisp lisp scala scala scala clojure clojure&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;java java java scala scala scala&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;book book library library lisp lisp clojure clojure&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;writing writing book book library library&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;scala book lisp book java book clojure&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;movie movie article article book book mars mars&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;movie movie article article mars mars mars&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;blockquote&gt;
  &lt;p&gt;リスプ リスプ リスプ スカラ スカラ スカラ&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;ジャバ ジャバ ジャバ スカラ スカラ スカラ&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;本 本 図書館 図書館 リスプ リスプ&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;書く 書く 本 本 図書館 図書館&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;スカラ ジャバ リスプ 本 本 本&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;映画 映画 論文 論文 本 本 火星 火星&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;映画 映画 論文 論文 火星 火星 火星&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;p&gt;トピック数3くらいで実行したときの， &lt;script type=&quot;math/tex&quot;&gt;\phi_{k,v}&lt;/script&gt;の確率値の高い単語の一覧を示します．&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;英語
    &lt;ul&gt;
      &lt;li&gt;topic=0
        &lt;ul&gt;
          &lt;li&gt;mars 0.382442748092&lt;/li&gt;
          &lt;li&gt;article 0.306106870229&lt;/li&gt;
          &lt;li&gt;movie 0.306106870229&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;topic=1
        &lt;ul&gt;
          &lt;li&gt;book 0.596688741722&lt;/li&gt;
          &lt;li&gt;library 0.265562913907&lt;/li&gt;
          &lt;li&gt;writing 0.133112582781&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;topic=2
        &lt;ul&gt;
          &lt;li&gt;scala 0.317194570136&lt;/li&gt;
          &lt;li&gt;lisp 0.271945701357&lt;/li&gt;
          &lt;li&gt;clojure 0.226696832579&lt;/li&gt;
          &lt;li&gt;java 0.181447963801&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;日本語
    &lt;ul&gt;
      &lt;li&gt;topic=0
        &lt;ul&gt;
          &lt;li&gt;火星 0.382734912147&lt;/li&gt;
          &lt;li&gt;論文 0.306340718105&lt;/li&gt;
          &lt;li&gt;映画 0.306340718105&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;topic=1
        &lt;ul&gt;
          &lt;li&gt;本 0.597084161696&lt;/li&gt;
          &lt;li&gt;図書館 0.265738899934&lt;/li&gt;
          &lt;li&gt;書く 0.133200795229&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;topic=2
        &lt;ul&gt;
          &lt;li&gt;スカラ 0.410181392627&lt;/li&gt;
          &lt;li&gt;リスプ 0.351667641896&lt;/li&gt;
          &lt;li&gt;ジャバ 0.234640140433&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;このときトピックは共通しているため（同じ&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;から&lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;が生成されるため），言語が違っていても共通したトピックから生成される単語分布を求めることができます．&lt;/p&gt;

&lt;p&gt;コードはそのうちgithubで公開すると思います．&lt;/p&gt;

&lt;p&gt;以上です．&lt;/p&gt;

&lt;h4 id=&quot;section-3&quot;&gt;参考文献など&lt;/h4&gt;
&lt;p&gt;&lt;a href=&quot;https://bioinformatics.oxfordjournals.org/content/early/2005/05/26/bioinformatics.bti515.full.pdf&quot;&gt;青いトピックモデル本で言及されているjoint topic modelの論文&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 17 Feb 2016 21:00:00 +0900</pubDate>
        <link>/2016/02/jointTopicModelsEquation</link>
        <guid isPermaLink="true">/2016/02/jointTopicModelsEquation</guid>
        
        
      </item>
    
      <item>
        <title>Sequence to Sequence Learning with Neural Networks</title>
        <description>&lt;h4 id=&quot;section&quot;&gt;はじめに&lt;/h4&gt;

&lt;p&gt;対話文を生成する
&lt;a href=&quot;http://arxiv.org/pdf/1506.05869.pdf&quot;&gt;A Neural Conversational Model&lt;/a&gt;の実装がしたくなったんですが，そこで使われるSequence to Sequenceを知らなかったのでその紹介．&lt;/p&gt;

&lt;p&gt;Sequence to Sequenceは，NIPS2014の&lt;a href=&quot;http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf&quot;&gt;論文&lt;/a&gt;で提案されているアーキテクチャ．&lt;/p&gt;

&lt;p&gt;論文ではSequence to Sequenceという入出力が可変長な系列データを扱うためのネットワークの提案を英仏の翻訳のタスクで評価．
なので前回紹介したencoder–decoderとやりたいことは同じ．
例によって翻訳がわかってないので，モデルの紹介だけ．&lt;/p&gt;

&lt;p&gt;encoder–decoderと違うこととしては&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;LSTMを2つくっつけて使う (LSTMを使うことで長期依存を取ろうとしている)
    &lt;ul&gt;
      &lt;li&gt;2つというのは入力系列用のLSTMと出力系列用のLSTM&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;4層のLSTM
    &lt;ul&gt;
      &lt;li&gt;(nzw: 単層ならわかるが，4層の場合に入力系列の入れ方と中間のベクトルをどうやって作るのかわからない)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;入力系列の順序を逆順にする
    &lt;ul&gt;
      &lt;li&gt;(nzw: interactiveにつかえないような)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;section-1&quot;&gt;本題&lt;/h4&gt;

&lt;p&gt;モデルはencoder–decoderよりもさらにシンプル（にみえる）．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/seq2seq.svg&quot; alt=&quot;model&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ただし，&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;ABC&lt;/code&gt; : 入力系列 (ex: 英語の1文)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;EOS&amp;gt;&lt;/code&gt; : 文末記号&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;WXYZ&lt;/code&gt; : 出力系列 (ex: フランス語の1文)&lt;/li&gt;
  &lt;li&gt;出力層 : &lt;code class=&quot;highlighter-rouge&quot;&gt;softmax&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;とする．&lt;/p&gt;

&lt;p&gt;あと，翻訳先ともとでembeddingsは2つ存在する．
(nzw: 会話の場合はembeddinsが1つでいいかもしれない)&lt;/p&gt;

&lt;p&gt;encoder部分とdecoder部分はそれぞれ4層のLSTMからできてるのでもう少し複雑になる．
4層のLSTMに系列いれるのがよくわかってません．&lt;/p&gt;

&lt;h4 id=&quot;section-2&quot;&gt;その他&lt;/h4&gt;
&lt;p&gt;入力系列を逆順に入れると翻訳の性能が上がるらしい．
つまり，1.ではなくて2.をいれる．&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;月 が きれい です ね&lt;/li&gt;
  &lt;li&gt;ね です きれい が 月&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;section-3&quot;&gt;所感&lt;/h4&gt;
&lt;p&gt;モデルをもう少し詳しく書いてほしい．．．&lt;/p&gt;

&lt;p&gt;encoder–decoder (というか翻訳) の時点で思ってたのは，この枠組が使えると翻訳以外でも会話，言い換え，QAあたりもできるので，汎用性が高い印象を持った． (ただし十分なデータ量がない気がしている)&lt;/p&gt;

&lt;p&gt;4層のLSTMの入力のあたりは関連研究を参考にして調べてみようかと思います．&lt;/p&gt;
</description>
        <pubDate>Sun, 14 Feb 2016 16:45:00 +0900</pubDate>
        <link>/2016/02/seq2seq</link>
        <guid isPermaLink="true">/2016/02/seq2seq</guid>
        
        
      </item>
    
      <item>
        <title>トピックモデルによる統計的潜在意味解析のp56の最初の式</title>
        <description>&lt;h4 id=&quot;section&quot;&gt;はじめに&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;http://amzn.to/1Tc0ost&quot;&gt;トピックモデルによる統計的潜在意味解析&lt;/a&gt; を頭から読んでいて3.2.4の周辺化ギブスサンプリングの式が気になったのでその部分の式展開だけをちょっと詳しめに書きます．&lt;/p&gt;

&lt;h4 id=&quot;section-1&quot;&gt;本題&lt;/h4&gt;
&lt;p&gt;p55の最後の式からp56の最初の式変形をやります．
2016年2月10日にアクセスした正誤表には記述がないのですが，
p56の最後の項は同時確率ではなく，条件付き確率であると思われます．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
p(z_{d,i} = k, w_{d,i}=v, \mathbf{w}^{\backslash d,i}, \mathbf{z}^{\backslash d,i} | {\boldsymbol \alpha}, {\boldsymbol \beta})
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;がp55の最後の式です．
これを順番に分解していきます．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
p(z_{d,i} = k, w_{d,i}=v, \mathbf{w}^{\backslash d,i}, \mathbf{z}^{\backslash d,i} | {\boldsymbol \alpha}, {\boldsymbol \beta}) &amp;amp;=&amp;amp;
p( w_{d,i} = v |z_{d,i} = k, \mathbf{w}^{\backslash d,i}, \mathbf{z}^{\backslash d,i},  {\boldsymbol \alpha}, {\boldsymbol \beta})  \\
&amp;amp; &amp;amp; \times
p(z_{d,i} = k, \mathbf{w}^{\backslash d,i}, \mathbf{z}^{\backslash d,i} | {\boldsymbol \alpha}, {\boldsymbol \beta}) \tag{1} \\
&amp;amp;=&amp;amp;
p( w_{d,i} = v |z_{d,i} = k, \mathbf{w}^{\backslash d,i}, \mathbf{z}^{\backslash d,i},  {\boldsymbol \alpha}, {\boldsymbol \beta}) \\
&amp;amp; &amp;amp; \times
p(z_{d,i} = k| \mathbf{w}^{\backslash d,i}, \mathbf{z}^{\backslash d,i} , {\boldsymbol \alpha}, {\boldsymbol \beta}) \\
&amp;amp; &amp;amp;  \times
p(\mathbf{w}^{\backslash d,i}, \mathbf{z}^{\backslash d,i} | {\boldsymbol \alpha}, {\boldsymbol \beta}) \tag{2}
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;式1の左辺から右辺の変形は以下の関係式を使います．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
p(w_{d,i}=v | z_{d,i} = k, \mathbf{w}^{\backslash d,i}, \mathbf{z}^{\backslash d,i} ,  {\boldsymbol \alpha}, {\boldsymbol \beta}) = \frac{
  p(z_{d,i} = k, w_{d,i}=v, \mathbf{w}^{\backslash d,i}, \mathbf{z}^{\backslash d,i} | {\boldsymbol \alpha}, {\boldsymbol \beta})}
{p(z_{d,i} = k, \mathbf{w}^{\backslash d,i}, \mathbf{z}^{\backslash d,i} | {\boldsymbol \alpha}, {\boldsymbol \beta})}
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;式2では式1右辺の第2項目を以下の等式を用いて2つの項（式2の第2項，第3項）に分解しています．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
p(z_{d,i} = k | \mathbf{w}^{\backslash d,i}, \mathbf{z}^{\backslash d,i} , {\boldsymbol \alpha}, {\boldsymbol \beta}) = \frac{
p(z_{d,i} = k, \mathbf{w}^{\backslash d,i}, \mathbf{z}^{\backslash d,i} | {\boldsymbol \alpha}, {\boldsymbol \beta})}
{p(\mathbf{w}^{\backslash d,i}, \mathbf{z}^{\backslash d,i} | {\boldsymbol \alpha}, {\boldsymbol \beta})}
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;以上から本のように同時確率\(p(\mathbf{w}^{\backslash d,i}, \mathbf{z}^{\backslash d,i} , {\boldsymbol \alpha}, {\boldsymbol \beta})\)ではなく条件付き確率になるような気がしています．
ただ，条件付き確率であろうとなかろうとこの項は\(z_{d,i}\)を含んでいないので消去されることから期待値計算には関係してきません．&lt;/p&gt;
</description>
        <pubDate>Wed, 10 Feb 2016 18:36:00 +0900</pubDate>
        <link>/2016/02/collapseGibbsEquation</link>
        <guid isPermaLink="true">/2016/02/collapseGibbsEquation</guid>
        
        
      </item>
    
      <item>
        <title>Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation</title>
        <description>&lt;h4 id=&quot;section&quot;&gt;はじめに&lt;/h4&gt;
&lt;p&gt;このページは研究室の輪講のために作成されたページです．&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://amzn.to/1T9JfPS&quot;&gt;青深層学習の7章&lt;/a&gt; でRNNを扱いました．
本ではCTCを使って，入出力で系列長の異なるデータへの対応をしました（読んでる限りは入力系列より短くはできるが長くはできないような）．&lt;/p&gt;

&lt;p&gt;この &lt;a href=&quot;http://arxiv.org/pdf/1406.1078v3.pdf&quot;&gt;EMNLP2014&lt;/a&gt; の論文では，encoder–decoderの提案，LSTM likeなユニットの提案，それらを使った英仏のフレーズ翻訳のタスクでの評価実験しています．（word embeddingsも少し出てきます）&lt;/p&gt;

&lt;p&gt;nzwは機械翻訳について全くの素人であるので，モデル自体の話しかしません．
また詳しい話は菊池さんの &lt;a href=&quot;http://www.slideshare.net/yutakikuchi927/learning-phrase-representations-using-rnn-encoderdecoder-for-statistical-machine-translation&quot;&gt;slideshare&lt;/a&gt; が非常にわかりやすいのでこちらをお勧めします．&lt;/p&gt;

&lt;h4 id=&quot;section-1&quot;&gt;本題&lt;/h4&gt;

&lt;p&gt;まずタイトルにあるencoder–decoderを別々に説明すると，&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;encoder：可変長の入力系列を固定長ベクトルに変換するRNN&lt;/li&gt;
  &lt;li&gt;decoder：固定長ベクトルを可変長の系列データに変換するRNN&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;です．&lt;/p&gt;

&lt;p&gt;構造自体はシンプルです．
まずencoderでは，入力系列（例えば，英語のフレーズ）を時刻ごとに受け取り，入力系列の最後まで入力したら固定長ベクトルの1つに変換します．
次にdecoderでは，encoderで求めた固定長ベクトルとdecoder部分のRNNを使って時刻ごとに単語を1つずつ出力していき，文末を表す記号を出力するまで繰り返します．&lt;/p&gt;

&lt;p&gt;論文の図1では下がencoder，上がdecoderになります．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/model.png&quot; alt=&quot;nn&quot; /&gt;&lt;/p&gt;

&lt;p&gt;このとき，&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(\mathbf{x}\) ：1単語に対応するone–hotベクトル&lt;/li&gt;
  &lt;li&gt;\(\mathbf{c}\) ：固定長ベクトル&lt;/li&gt;
  &lt;li&gt;\(\mathbf{y}\) ：1単語に対応するone–hotベクトル&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;です．&lt;/p&gt;

&lt;p&gt;encoder–decoderの用途は，&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;入力系列から出力系列の生成&lt;/li&gt;
  &lt;li&gt;入出力の系列ペアのスコアリング&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;と言及されています．&lt;/p&gt;

&lt;h5 id=&quot;encoder&quot;&gt;encoderの計算&lt;/h5&gt;
&lt;p&gt;\(\mathbf{h}_{\langle t \rangle}\) は時刻\(t\) におけるencoderの白いユニット部分の値です．&lt;/p&gt;

&lt;p&gt;計算式はRNNなので以下のようになります．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
\mathbf{h}_{\langle t \rangle} = f( \mathbf{h}_{\langle t-1 \rangle} , x_t)
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;関数 \(f\) はロジスティックシグモイドやLSTMなどをおけます．
この論文ではLSTM likeなユニットを導入しているので，それが\(f\)に対応します．&lt;/p&gt;

&lt;p&gt;encoderのRNNは，青い本のRNNとほぼ同じ形です．&lt;/p&gt;

&lt;h5 id=&quot;decoder&quot;&gt;decoderの計算&lt;/h5&gt;
&lt;p&gt;\begin{eqnarray}
\mathbf{h}_{\langle t \rangle} = f( \mathbf{h}_{\langle t-1 \rangle} , y_{t-1}, \mathbf{c})
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;\(\mathbf{h}_{\langle t \rangle}\) は時刻\(t\) におけるdecoderの白いユニット部分の値です．
encoderと異なり，\(\mathbf{c}\)が隠れ層に常に関係してきます．&lt;/p&gt;

&lt;p&gt;出力層の計算は&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
p(y_{t}|y_{t-1},y_{t-2},…,y_{1},\mathbf{c}) = g(\mathbf{h}_{\langle t \rangle}, y_{t-1}, \mathbf{c})
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;となります．
確率値にしたいので，\(g\)には，例えばsoftmax関数が使われます．&lt;/p&gt;

&lt;h4 id=&quot;lstm-likernn&quot;&gt;LSTM likeなRNNの隠れ層のユニット&lt;/h4&gt;

&lt;p&gt;LSTMよりもシンプルで似た働き（記憶と忘却）を持たせるために導入しています．&lt;/p&gt;

&lt;p&gt;これが1ユニットです（LSTMの1メモリユニットに相当）．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/encoder-decoder-unit.png&quot; alt=&quot;nn&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(r\)：reset gate，確率値で0に近いほど前の層の情報\(h\)を無視，頻繁に活発化すれば短期記憶&lt;/li&gt;
  &lt;li&gt;\(z\)：update gate，確率値で前の層の情報をどれだけ今の層に伝えるか，ずっと活性化していれば長期記憶&lt;/li&gt;
  &lt;li&gt;\(\mathbf{x}\)：入力ベクトル&lt;/li&gt;
  &lt;li&gt;\(\mathbf{h}\)：このユニットが出力する値&lt;/li&gt;
  &lt;li&gt;\(\mathbf{\tilde{h}}\)：hの計算に使う値&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;appendix&quot;&gt;appendixの詳しい式の紹介&lt;/h4&gt;

&lt;h6 id=&quot;encoder-1&quot;&gt;encoderの計算再び&lt;/h6&gt;
&lt;p&gt;入力系列\(\mathbf{x}\)から
固定長ベクトル\(\mathbf{c}\)を求めるまでの計算式です．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
h^{\langle t \rangle}_{j} &amp;amp;=&amp;amp; z_{j} h_{j}^{\langle t-1 \rangle} + (1-z_{j}) \tilde{h}^{\langle t \rangle}_{j} \\
\tilde{h}^{\langle t \rangle}_{j} &amp;amp;=&amp;amp; tanh( [ \mathbf{W} e(\mathbf{x}_t) ]_j + [\mathbf{U}(\mathbf{r} \odot \mathbf{h}_{\langle t-1 \rangle})]_j ) \\
z_j &amp;amp;=&amp;amp; \sigma( [\mathbf{W}_z e(\mathbf{x}_t)  ]_j + [\mathbf{U}_z \mathbf{h}_{\langle t-1 \rangle})]_j) \\
r_j &amp;amp;=&amp;amp; \sigma( [\mathbf{W}_r e(\mathbf{x}_t)  ]_j + [\mathbf{U}_r \mathbf{h}_{\langle t-1 \rangle})]_j), \\
\mathbf{c} &amp;amp;=&amp;amp; tanh(\mathbf{V} \mathbf{h}^{\langle N \rangle})
\end{eqnarray}&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(z_j\)：隠れ層の\(j\)番目のユニットのupdate geteの確率値&lt;/li&gt;
  &lt;li&gt;\(r_j\)：reset geteの確率値&lt;/li&gt;
  &lt;li&gt;\(h^{\langle t \rangle}_{j}\)：時刻\(t\)の隠れ層の\(j\)番目のユニットの値&lt;/li&gt;
  &lt;li&gt;\(e\)：word embeddingsの行列(このモデルで学習できる)&lt;/li&gt;
  &lt;li&gt;\(U\)：前の層に対する重み&lt;/li&gt;
  &lt;li&gt;\(V\)：入力系列を最後まで読みこんで計算した\(h\)にかかる重み&lt;/li&gt;
  &lt;li&gt;\(W\)：入力系列の分散表現にかかる重み&lt;/li&gt;
  &lt;li&gt;\(\sigma\)：ロジスティクスシグモイド関数&lt;/li&gt;
&lt;/ul&gt;

&lt;h6 id=&quot;decoder-1&quot;&gt;decoderの計算再び&lt;/h6&gt;

&lt;p&gt;初期値は \(\mathbf{h}’^{\langle 0 \rangle} =  tanh(\mathbf{V}’ \mathbf{c})\)．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
h’^{\langle t \rangle}_{j} &amp;amp;=&amp;amp; z’_{j} h_{j}’^{\langle t-1 \rangle} + (1-z’_{j}) \tilde{h}’^{\langle t \rangle}_{j} \\
\tilde{h}’^{\langle t \rangle}_{j} &amp;amp;=&amp;amp; tanh( [ \mathbf{W}’ e(\mathbf{y}_{t-1}) ]_j + \mathbf{r}’[\mathbf{U}’\mathbf{h}’_{\langle t-1 \rangle}) + \mathbf{Cc}] ) \\
z’_j &amp;amp;=&amp;amp; \sigma( [\mathbf{W}’_z e(\mathbf{y}_{t-1})  ]_j + [\mathbf{U}’_z \mathbf{h}’_{\langle t-1 \rangle})]_j + [\mathbf{C}_z \mathbf{c}]_j) \\
r’_j &amp;amp;=&amp;amp; \sigma( [\mathbf{W}’_r e(\mathbf{y}_{t-1})  ]_j + [\mathbf{U}’_r \mathbf{h}’_{\langle t-1 \rangle})]_j + [\mathbf{C}_r \mathbf{c}]_j), \\
p(y_{t,j} = 1 | \mathbf{y}_{t-1}, …, \mathbf{y}_1 \mathbf{X}) &amp;amp;=&amp;amp; \frac{exp(\mathbf{g}_j \mathbf{s}_{\langle t \rangle})}{\sum_{j’=1}^{K} exp(\mathbf{g}_{j’} \mathbf{s}_{\langle t \rangle})} \\
s_{\langle t \rangle} &amp;amp;=&amp;amp; max ( s’^{\langle t \rangle}_{2i-1}, s’^{\langle t \rangle}_{2i} ) \\
s’^{\langle t \rangle} &amp;amp;=&amp;amp; \mathbf{O}_h \mathbf{h}’^{\langle t \rangle} + \mathbf{O}_y \mathbf{y}_{t-1} + \mathbf{O}_c \mathbf{c}
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;‘がついているものはencoderと同様です．&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(s\)：maxout関数で計算&lt;/li&gt;
  &lt;li&gt;\(O\)：出力層への入力にかかる重み&lt;/li&gt;
  &lt;li&gt;\(C\)：固定長ベクトル\(\mathbf{c}\)の重み&lt;/li&gt;
  &lt;li&gt;\(g\)：重み行列\(G\)の要素&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;nzw&quot;&gt;その他，nzw的に興味があった箇所&lt;/h4&gt;

&lt;p&gt;word embeddingsがこれでも学習できるらしく，上の式で出てきた\(e\)を可視化した図が出てきている．
NGやsubsampligがないのでw2vの両モデルほど良い結果は出ないと思うけど，お得感がある．&lt;/p&gt;

&lt;p&gt;あと\(\mathbf{c}
\)を可視化するとフレーズが近くにまとまる．
（なので，ベクトル\(\mathbf{c}
\)を求めることは，入力系列のフレーズを埋め込んでるかPCAみたいなことをしていることに相当している？）&lt;/p&gt;
</description>
        <pubDate>Tue, 09 Feb 2016 13:15:00 +0900</pubDate>
        <link>/2016/02/EncoderDecoder</link>
        <guid isPermaLink="true">/2016/02/EncoderDecoder</guid>
        
        
      </item>
    
      <item>
        <title>SENSEMBED：Learning Sense Embeddings for Word and Relational Similarity</title>
        <description>&lt;h4 id=&quot;section&quot;&gt;概要&lt;/h4&gt;
&lt;p&gt;ACL2015の&lt;a href=&quot;http://wwwusers.di.uniroma1.it/~navigli/pubs/ACL_2015_Iacobaccietal.pdf&quot;&gt;論文&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;nzwは，手法の部分が気になったので，その部分をかいつまんで説明します．（といっても数行）&lt;/p&gt;

&lt;p&gt;word2vecやGloVeといったツールで獲得できる分散表現は，多義語でも1つのベクトルとして表現する．
例えば &lt;em&gt;bank&lt;/em&gt; は，土手の意味と銀行の意味をもっているがこれを1つのベクトルで表現している．
この論文は，語義ごとにベクトルを学習できれば，より分散表現として優れるのではという研究．&lt;/p&gt;

&lt;p&gt;モデル自体は単純で，まずWSDのSOTAな手法を使ってコーパスの単語ごとに語義を付与する．
次にCBoWで分散表現を学習する．
このときに同じ単語でも別の語義は付与されていれば別の単語とみなす．
この部分だけ気になっていたのでここらへんで読むのをやめた．&lt;/p&gt;

&lt;p&gt;（コーパスだけを使って語義を取れるものだと勘違いしていた）&lt;/p&gt;

&lt;p&gt;おそらくメインは残りで，類似度計算をCosine similarityではなく，語義を考慮したような式をいくつか提案している．
それをもとに既存手法と順位相関係数で評価．&lt;/p&gt;

&lt;h4 id=&quot;section-1&quot;&gt;気になったこと&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;analogyの評価指標にaccuracyがでてこないので，Mikolov論文やglove論文などの評価と違う&lt;/li&gt;
  &lt;li&gt;分散表現の良さは，ベクトルの演算と単語の演算（？）の対応がうまくできていて，扱いやすく計算が速いこと，という認識を持っていたのでだけど，この手法だと計算が重そう．
    &lt;ul&gt;
      &lt;li&gt;（出してないということは普通にベクトルとして計算してもあんまり性能が上がらなかった？）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Wed, 27 Jan 2016 18:15:04 +0900</pubDate>
        <link>/2016/01/senseembed</link>
        <guid isPermaLink="true">/2016/01/senseembed</guid>
        
        
      </item>
    
      <item>
        <title>Topic Modeling for Word Sense Induction</title>
        <description>&lt;h4 id=&quot;section&quot;&gt;概要&lt;/h4&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Word Sense Induction&lt;/code&gt;（WSI）のタスクに&lt;code class=&quot;highlighter-rouge&quot;&gt;LDA&lt;/code&gt;を使ってみましたという&lt;a href=&quot;http://publications.wim.uni-mannheim.de/informatik/lski/Knopp13TopicModelingforWSI.pdf&quot;&gt;2013年のGSCLの会議論文&lt;/a&gt;．&lt;/p&gt;

&lt;p&gt;WSIでは，出現した単語ごとの文脈などを単位としてベクトルを作り，それを&lt;code class=&quot;highlighter-rouge&quot;&gt;k-means&lt;/code&gt;でクラスタリングすることで，出現した位置における語義を分けられるかというやり方がある．
トピックが語義に対応しているという仮定して，そのベクトルを作る際に&lt;code class=&quot;highlighter-rouge&quot;&gt;LDA&lt;/code&gt;で得られるトピックごとの単語の確率分布\(\phi\)を使ってやろうというもの．
なので&lt;code class=&quot;highlighter-rouge&quot;&gt;LDA&lt;/code&gt;を拡張したわけではないのでわりと直感的である．（&lt;code class=&quot;highlighter-rouge&quot;&gt;gensim&lt;/code&gt;+&lt;code class=&quot;highlighter-rouge&quot;&gt;scikit-learn&lt;/code&gt;+&lt;code class=&quot;highlighter-rouge&quot;&gt;NLTK&lt;/code&gt;さえあればできそう）&lt;/p&gt;

&lt;h4 id=&quot;section-1&quot;&gt;気になったこと&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;トピック数が&lt;code class=&quot;highlighter-rouge&quot;&gt;3~10&lt;/code&gt;と比較的とかなり小さい(この分野ではそうなんだろうか)&lt;/li&gt;
  &lt;li&gt;評価には&lt;code class=&quot;highlighter-rouge&quot;&gt;WordNet&lt;/code&gt;と&lt;code class=&quot;highlighter-rouge&quot;&gt;SeｍEval2010&lt;/code&gt;を使ってる（名詞と動詞だけで評価，評価指標は&lt;code class=&quot;highlighter-rouge&quot;&gt;F-score&lt;/code&gt;と&lt;code class=&quot;highlighter-rouge&quot;&gt;V-measure&lt;/code&gt;）&lt;/li&gt;
  &lt;li&gt;クラスタリングの手法は複数試したほうが良さそう（WSIのsurveyでいくつ言及あり）&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 20 Dec 2015 13:18:00 +0900</pubDate>
        <link>/2015/12/TopicModelingforWordSenseInduction</link>
        <guid isPermaLink="true">/2015/12/TopicModelingforWordSenseInduction</guid>
        
        
      </item>
    
      <item>
        <title>Gadflyの紹介</title>
        <description>&lt;h3 id=&quot;section&quot;&gt;はじめに&lt;/h3&gt;

&lt;p&gt;卒論の時期になりました．
私もクリスマス締め切りの卒論を書いています．&lt;/p&gt;

&lt;p&gt;さて，卒論の実験結果や統計処理した結果をグラフに出す必要がある方はけっこういるのではないでしょうか．&lt;/p&gt;

&lt;p&gt;というわけで&lt;a href=&quot;http://gadflyjl.org/&quot;&gt;Gadfly&lt;/a&gt;を紹介します．
夏休みに書いた論文の図はGadflyで描きましたし，輪講で使うグラフもGadflyで描きました．
ゼミ生の中で好評でした✌️&lt;/p&gt;

&lt;p&gt;実行環境はMacを前提としていますので適宜お手元の環境に読み替えてください．&lt;/p&gt;

&lt;h3 id=&quot;gadfly&quot;&gt;Gadflyって何？&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;http://julialang.org/&quot;&gt;julia&lt;/a&gt;という技術計算向けの高速でイケてる言語の描画パッケージの1つです．&lt;/p&gt;

&lt;p&gt;手軽にかっこいい画像が作れます．&lt;/p&gt;

&lt;p&gt;保存形式は以下の6つに対応しています．&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;SVG&lt;/li&gt;
  &lt;li&gt;SVGJS&lt;/li&gt;
  &lt;li&gt;PNG&lt;/li&gt;
  &lt;li&gt;PDF&lt;/li&gt;
  &lt;li&gt;PS&lt;/li&gt;
  &lt;li&gt;PGF&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;nzwはPDFで出力してTeXやパワーポイントに貼り付けてることが多いです．&lt;/p&gt;

&lt;p&gt;ちなみにGadfly以外にもいろいろとあるようです．-&amp;gt; &lt;a href=&quot;https://en.wikibooks.org/wiki/Introducing_Julia/Plotting&quot;&gt;julia plotting&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;導入&lt;/h3&gt;

&lt;p&gt;juliaの導入は&lt;code class=&quot;highlighter-rouge&quot;&gt;homebrew&lt;/code&gt;なりを使ってやるとして，juliaを起動したら以下のコマンドでGadflyをインストールします．&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-julia&quot; data-lang=&quot;julia&quot;&gt;julia&amp;gt; Pkg.add(&quot;Gadfly&quot;)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;インストールが成功したら，以下のコマンドを実行すればplotができるようになります．&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-julia&quot; data-lang=&quot;julia&quot;&gt;julia&amp;gt; using Gadfly&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;iris&lt;/code&gt;(アヤメという多年草)のデータを例にあげることがあるので&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-julia&quot; data-lang=&quot;julia&quot;&gt;Pkg.add(&quot;RDatasets&quot;)
using RDatasets&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;をあらかじめやっておきます．&lt;/p&gt;

&lt;p&gt;全て紹介すると大変なので，ここでは自分がよく使うものだけ紹介します．&lt;/p&gt;

&lt;h3 id=&quot;plot&quot;&gt;自分がよくつかうplot&lt;/h3&gt;
&lt;p&gt;まずは簡単な散布図から．&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-julia&quot; data-lang=&quot;julia&quot;&gt;plot(x=rand(13),y=rand(13))&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/images/gadfly1.svg&quot; alt=&quot;ex1&quot; /&gt;
はい，綺麗ですね．うっとりします．&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;rand(13)&lt;/code&gt;は13個の乱数がはいった&lt;code class=&quot;highlighter-rouge&quot;&gt;Array&lt;/code&gt;を生成しています．
&lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt;と&lt;code class=&quot;highlighter-rouge&quot;&gt;y&lt;/code&gt;でindexが同じものを1つのデータ点としてplotしています．&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;plotする図によってパラメータが違っていて散布図(point)であれば&lt;code class=&quot;highlighter-rouge&quot;&gt;color&lt;/code&gt;が指定できます．&lt;/p&gt;

&lt;p&gt;この例では，アヤメの品種にごとに色わけしています．
irisのデータは&lt;code class=&quot;highlighter-rouge&quot;&gt;DataFrame&lt;/code&gt;(excelの表みたいなやつ)に格納されているので，x軸，y軸を指定する必要があります．&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-julia&quot; data-lang=&quot;julia&quot;&gt;plot(dataset(&quot;datasets&quot;, &quot;iris&quot;), x=&quot;PetalLength&quot;, y=&quot;PetalWidth&quot;, color=&quot;Species&quot;, Geom.point)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/images/gadfly2.svg&quot; alt=&quot;ex2&quot; /&gt;
イィ…&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;言語処理をやっているのでword2vec(分散表現)の例を示すことがあります．
ざっくり分散表現を説明すると，密なベクトル空間の1点に単語をマッピングすることです．&lt;/p&gt;

&lt;p&gt;例えば，以下の4行からなるCSVファイル&lt;code class=&quot;highlighter-rouge&quot;&gt;w2v.csv&lt;/code&gt;がjuliaのREPLを起動したディレクトリにあったとして描画します．&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-julia&quot; data-lang=&quot;julia&quot;&gt;x,y,word
1.0,2.0,man
2.0,3.0,woman
2.0,2.0,king
3.0,3.0,queen&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-julia&quot; data-lang=&quot;julia&quot;&gt;df = readtable(&quot;w2v.csv&quot;)
plot(df, x=&quot;x&quot;, y=&quot;y&quot;, label=&quot;word&quot;, Geom.point, Geom.label, Scale.x_continuous(minvalue=0.0, maxvalue=4.0),Scale.y_continuous(minvalue=0.0, maxvalue=4.0))&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;df&lt;/code&gt;はCSVファイルを読み込んだDataFrameです．&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;x=&quot;x&quot;&lt;/code&gt;の右辺で&lt;code class=&quot;highlighter-rouge&quot;&gt;df&lt;/code&gt;の列名を指定します．&lt;code class=&quot;highlighter-rouge&quot;&gt;label=&quot;word&quot;&lt;/code&gt;についても同様です．&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Scale.x_continuous(minvalue=0.0, maxvalue=4.0)&lt;/code&gt;は，描画するx軸の最小値最大値の範囲を指定します．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/w2v.svg&quot; alt=&quot;ex1&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;関数の描画も可能です．
例えばlogistic関数とtanhについて \(-5 \leq x \leq 5\) を定義域とした場合で描画します．&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-julia&quot; data-lang=&quot;julia&quot;&gt;f(x) = 1/(1+exp(-x))
plot([f,tanh], -5, 5, color=repeat([&quot;logistic&quot;, &quot;tanh&quot;]))&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/images/func.svg&quot; alt=&quot;ex1&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;subplot_grid&lt;/code&gt;を使うと複数のグラフを並べることができます．
この例ではアルゴリズムごとに棒グラフをわけて描画してます．&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-julia&quot; data-lang=&quot;julia&quot;&gt;data = DataFrame()
data[:Algorithm] = [&quot;methodA&quot;, &quot;methodA&quot;, &quot;methodB&quot;, &quot;methodB&quot;, &quot;proposal&quot;, &quot;proposal&quot;]
data[:Dataset] = [&quot;data1&quot;, &quot;data2&quot;, &quot;data1&quot;, &quot;data2&quot;, &quot;data1&quot;, &quot;data2&quot;]
data[:Precision] = [0.5, 0.4, 0.6, 0.62, 0.8, 0.9]

plot(data, xgroup=&quot;Algorithm&quot;, x=&quot;Dataset&quot;, y=&quot;Precision&quot;, color=&quot;Dataset&quot;,
  Scale.y_continuous(minvalue=0),
  Guide.ylabel(&quot;正解率&quot;),
  Guide.title(&quot;テストデータに対する正解率の比較&quot;),
  Geom.subplot_grid(Geom.bar()))&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/images/bar.svg&quot; alt=&quot;ex&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;おわりに&lt;/h3&gt;
&lt;p&gt;紹介しきれませんでしたが，フォントサイズや表示する目盛などを調整できます．&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://gadflyjl.org/&quot;&gt;Docs&lt;/a&gt;にまとまっているのでそちらをご参照ください．&lt;/p&gt;

&lt;p&gt;以上 &lt;a href=&quot;http://www.adventar.org/calendars/1005&quot;&gt;klis advent calendar 2015&lt;/a&gt; &amp;amp; &lt;a href=&quot;http://www.adventar.org/calendars/1315&quot;&gt;卒研 advent calendar 2015&lt;/a&gt;の8日目の記事でした．&lt;/p&gt;
</description>
        <pubDate>Tue, 08 Dec 2015 12:18:00 +0900</pubDate>
        <link>/2015/12/gadfly</link>
        <guid isPermaLink="true">/2015/12/gadfly</guid>
        
        
      </item>
    
      <item>
        <title>青トピックモデル 混合ユニグラムモデルの周辺化ギブスサンプリングのサンプル式</title>
        <description>&lt;p&gt;ユニグラムモデルにおいて，トピックは文書集合に対して1つだけ存在した．
一方，混合ユニグラムモデルは，トピック \(\phi\) は&lt;code class=&quot;highlighter-rouge&quot;&gt;K&lt;/code&gt;個存在する．(これをまとめたものを\(\Phi\)とかく)
そのため各文書は，&lt;code class=&quot;highlighter-rouge&quot;&gt;K&lt;/code&gt;個のうちの1トピックをもつ．
1つの\(\phi\)は，単語のカテゴリカル分布になる．
加えて\(\theta\)はトピック分布を表す．（各トピックがどれだけ出現しやすいかを表現）
これは混合ユニグラムモデルの中で1つだけ存在する．&lt;/p&gt;

&lt;p&gt;残りはユニグラムモデルやLDAと一緒で&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;α&lt;/code&gt;及び&lt;code class=&quot;highlighter-rouge&quot;&gt;β&lt;/code&gt;：ディリクレ分布のパラメータ&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;D&lt;/code&gt;：文書集合の総数&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;N&lt;/code&gt;：文書\(d\)の単語数&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;混合ユニグラムモデルをグラフィカルモデルで描くと以下のようになる．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/mix.svg&quot; alt=&quot;nn&quot; /&gt;&lt;/p&gt;

&lt;p&gt;新聞記事で例えると，記事はいくつかのトピック（スポーツとか政治とか）のどれか一つをもっていると仮定している．
そして各トピックごとに出現しやすい単語の確率分布は異なる．
（スポーツのトピックであれば”年棒, 野球, サッカー”といった単語が高確率で出現し，”選挙, 違法献金, 年金”は低確率で出現）&lt;/p&gt;

&lt;h3 id=&quot;section&quot;&gt;ベイズ推定&lt;/h3&gt;

&lt;p&gt;ベイズ推定を使う場合，文書の生成確率を定義してから，事後確率分布を求める．
混合ユニグラムモデルの文書の生成確率は以下のようにかける．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
p(\mathbf{W}|\Phi,\theta)
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;事後確率は，\(p(\bf{z},\Phi,\theta|\bf{W})\) となる．
ここで&lt;a href=&quot;http://amzn.to/1SEuqld&quot;&gt;青トピックモデル本&lt;/a&gt;では，崩壊型ギブスサンプリングを導入するが頭がわるくてついていけなかったので少し丁寧にやってみる．&lt;/p&gt;

&lt;p&gt;事後確率\(p(\bf{z},\Phi,\theta|\mathbf{W})\) をギブスサンプリングする場合は，\(\mathbf{z},\Phi,\theta\)のうち2つを固定して1つをサンプルする．
崩壊型ギブスサンプリングでは，\(\Phi,\theta\)を積分消去して\(\mathbf{z}\)を直接求める．
この辺りについては&lt;a href=&quot;http://amzn.to/1Nbzlq3&quot;&gt;白いトピックモデル本&lt;/a&gt;が大変参考になる．&lt;/p&gt;

&lt;p&gt;つまりサンプリング式は \(p(z_d=k|\mathbf{z}_{-d}, \mathbf{W}, \alpha, \beta)\)で，これ計算したい．&lt;/p&gt;

&lt;p&gt;ベイズの定理を使い，比例式を求める(青トピック本p51の式がこれに相当する)．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
p(z_d=k | \mathbf{z}_{-d}, \mathbf{W}, \alpha, \beta) &amp;amp;=&amp;amp;
\frac{p(z_{i} = k, \bf{z}_{-d}, \bf{W} | \alpha, \beta)}{p(\bf{z}_{-d}, \bf{W} | \alpha, \beta)}  \tag{1} \\&lt;br /&gt;
&amp;amp; \propto &amp;amp; p(z_d=k, \mathbf{z}_{-d}, \mathbf{W}| \alpha, \beta)  \tag{2} \\&lt;br /&gt;
&amp;amp;= &amp;amp; p(\mathbf{w}_{d} | \mathbf{W}_{-d}, z_d=k, \mathbf{z}_{-d}, \alpha, \beta) \\\ &amp;amp; &amp;amp; \times p(\mathbf{W}_{-d}, z_d=k, \mathbf{z}_{-d} | \alpha, \beta)  \tag{3} \\&lt;br /&gt;
&amp;amp;= &amp;amp; p(\mathbf{w}_d| \mathbf{W}_{-d}, z_d=k, \mathbf{z}_{-d}, \alpha, \beta) \times p(z_d=k| \mathbf{z}_{-d}, \mathbf{W}_{-d}, \alpha, \beta) \\\ &amp;amp; &amp;amp; \times
p(\mathbf{W}_{-d}, \mathbf{z}_{-d} | \alpha, \beta)  \tag{4} \\&lt;br /&gt;
&amp;amp; \propto &amp;amp; p(\mathbf{w}_d| \mathbf{W}_{-d}, z_d=k, \mathbf{z}_{-d}, \alpha, \beta) \\\ &amp;amp; &amp;amp; \times p(z_d=k| \mathbf{z}_{-d}, \mathbf{W}_{-d}, \alpha, \beta) \tag{5} \\&lt;br /&gt;
&amp;amp;= &amp;amp; p(\mathbf{w}_d| \mathbf{W}_{-d}, z_d=k, \mathbf{z}_{-d}, \beta) \times p(z_d=k| \mathbf{z}_{-d}, \alpha) \tag{6}
\end{eqnarray}&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;式(1)は，ベイズの定理で展開&lt;/li&gt;
  &lt;li&gt;式(2)は，分母に\(z_i=k\)がないので比例式で置き換え&lt;/li&gt;
  &lt;li&gt;式(3)は，ベイズの定理で1つの文書とそれ以外に展開&lt;/li&gt;
  &lt;li&gt;式(4)は，式(3)の2項目をベイズの定理で展開&lt;/li&gt;
  &lt;li&gt;式(5)は，式(4)の3項目に\(z_i=k\)がないので比例式で置き換え&lt;/li&gt;
  &lt;li&gt;式(6)は，条件付き独立を用いる&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;間違っていたらごめんなさい．&lt;/p&gt;

</description>
        <pubDate>Sun, 06 Dec 2015 18:00:00 +0900</pubDate>
        <link>/2015/12/mixgibbs</link>
        <guid isPermaLink="true">/2015/12/mixgibbs</guid>
        
        
      </item>
    
      <item>
        <title>青深層学習 4章 まとめ</title>
        <description>&lt;p&gt;4章誤差逆伝播法のまとめです．&lt;/p&gt;

&lt;p&gt;3章で扱った勾配降下法は，誤差関数の勾配を求めて重みを更新し，ネットワークの性能を高めるために使いました．&lt;/p&gt;

&lt;p&gt;誤差逆伝播法は，誤差関数の重みによる微分を効率良く計算するアルゴリズムです．&lt;/p&gt;

&lt;h4 id=&quot;section&quot;&gt;4.1 勾配計算の難しさ&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/images/nn1.svg&quot; alt=&quot;nn&quot; /&gt;&lt;/p&gt;

&lt;p&gt;順伝播によって得られた出力を使うため，重みが出力層から遠いほど（入力層に近いほど）chain ruleで展開する項の数が増えていきます．&lt;/p&gt;

&lt;p&gt;この場合では&lt;code class=&quot;highlighter-rouge&quot;&gt;hidden layer&lt;/code&gt;と&lt;code class=&quot;highlighter-rouge&quot;&gt;output layer&lt;/code&gt;の間の重みによる微分は比較的楽にできますが，&lt;code class=&quot;highlighter-rouge&quot;&gt;input layer&lt;/code&gt;と&lt;code class=&quot;highlighter-rouge&quot;&gt;hidden layer&lt;/code&gt;の間の重みによる微分は&lt;code class=&quot;highlighter-rouge&quot;&gt;hidden layer&lt;/code&gt;の活性化関数を介しているので複雑になると言えます．&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;4.2&lt;/code&gt;以降で導出する誤差逆伝播法を用いることで効率的に計算を行えるようになります．&lt;/p&gt;

&lt;h4 id=&quot;section-1&quot;&gt;4.2 2層ネットワークでの計算&lt;/h4&gt;
&lt;p&gt;隠れ層が1層のニューラルネットワークにおいて，重みは&lt;code class=&quot;highlighter-rouge&quot;&gt;input layer&lt;/code&gt;と&lt;code class=&quot;highlighter-rouge&quot;&gt;hidden layer&lt;/code&gt;の間と&lt;code class=&quot;highlighter-rouge&quot;&gt;hidden layer&lt;/code&gt;と&lt;code class=&quot;highlighter-rouge&quot;&gt;output layer&lt;/code&gt;の間の2つに分けることができます．&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://nzw0301.github.io/2015/11/blueDeepLearningChapter42/&quot;&gt;詳しい式の導出&lt;/a&gt;はこちらを参照．&lt;/p&gt;

&lt;p&gt;冒頭の&lt;code class=&quot;highlighter-rouge&quot;&gt;4.1&lt;/code&gt;で書いたように出力層から遠いほど式が複雑になっていることが確認できます．&lt;/p&gt;

&lt;h4 id=&quot;section-2&quot;&gt;4.3 多層ネットワークへの一般化&lt;/h4&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;4.2&lt;/code&gt;は2層でしたが，層の数を一般化したときの微分を求めてます．
なので，ここで誤差逆伝播法の本筋の話になります．&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://nzw0301.github.io/2015/11/blueDeepLearningChapter43/&quot;&gt;詳しい式の導出&lt;/a&gt;はこちらをみていただくとして，
肝心なのは， 各層ごとに\(\delta\)を\(L\)層（出力層）から逆順に求めることで，各重みによる微分を効率的に計算できることです．
\(\delta\)が計算できれば，重みの微分は別々に求めることができます．&lt;/p&gt;

&lt;h4 id=&quot;section-3&quot;&gt;4.4 勾配降下法の完全アルゴリズム&lt;/h4&gt;

&lt;p&gt;前半部分では，出力層の\(\delta\)の導出を行います．
&lt;a href=&quot;http://nzw0301.github.io/2015/11/blueDeepLearningChapter44/&quot;&gt;テキストより詳しい式変形をした&lt;/a&gt;ので，これからわかる通り，出力層の\(\delta\)は出力と正解の差です．&lt;/p&gt;

&lt;p&gt;後半部分では，行列表記による&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;順伝播法&lt;/li&gt;
  &lt;li&gt;誤差逆伝播法&lt;/li&gt;
  &lt;li&gt;重み\(w\)の更新&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;の式です．
詳細については，コーディングしやすいように行列の次元を意識して&lt;a href=&quot;http://nzw0301.github.io/2015/11/blueDeepLearningChapter442/&quot;&gt;かきました&lt;/a&gt;．&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;4.4.3 勾配の差分近似計算&lt;/code&gt;  では，多層になった場合に誤差関数の勾配計算の計算が正しいかを確かめるために，近似計算を扱います．
偏微分の定義から
十分小さい\(\epsilon\)を用いた式1で検証できます．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
\frac{\partial E}{\partial w_{ji}^{(l)}} = \frac{E(…,w_{ji}^{(l)}+\epsilon,…)-E(…,w_{ji}^{(l)},…)}{\epsilon} \tag{1}
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;微分なので\(\epsilon\)は十分小さな値を選択する必要があります．
注意点として計算機の特性上，打ち切り誤差や丸めの誤差などで誤差が大きくなるため以下のように
&lt;code class=&quot;highlighter-rouge&quot;&gt;計算機イプシロン&lt;/code&gt;(計算機の浮動小数点数で表現できる1より大きい最小の数と1との差のこと)\(\epsilon_c\)を使い，以下のように近似式の\(\epsilon\)を決定します．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
\epsilon = \sqrt{\epsilon_c} |w_{ji}|
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;python&lt;/code&gt;の&lt;code class=&quot;highlighter-rouge&quot;&gt;numpy&lt;/code&gt;でいえば&lt;code class=&quot;highlighter-rouge&quot;&gt;np.finfo(float).eps&lt;/code&gt;が計算機イプシロンです．
(手元では&lt;code class=&quot;highlighter-rouge&quot;&gt;2.2204460492503131e-16&lt;/code&gt;でした)&lt;/p&gt;

&lt;p&gt;? 実際につかうんでしょうか？&lt;/p&gt;

&lt;h4 id=&quot;section-4&quot;&gt;4.5 勾配消失問題&lt;/h4&gt;

&lt;p&gt;順伝播は活性化関数を経由するので活性化関数が非線形ならば，この層の出力も非線形であり，発散しません．（ロジスティック関数が活性化関数ならば0から1の間に収まる）&lt;/p&gt;

&lt;p&gt;対して逆伝播法は定義(4.13)のように線形な計算なので，層が深くなるほど発散しやすい（あるいは一度0になるとずっと0）になってしまうため学習がうまくいかない&lt;code class=&quot;highlighter-rouge&quot;&gt;勾配消失問題&lt;/code&gt;に直面します．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
\delta_j^{(l)} &amp;amp;=&amp;amp; \sum_k \delta_k^{(l+1)} (w_{kj}^{( l+1)} f’(u_j^{(l)})) \tag{4.12}\\
\end{eqnarray}&lt;/p&gt;

&lt;h4 id=&quot;section-5&quot;&gt;補足&lt;/h4&gt;

&lt;p&gt;十分多い隠れ層が1層あれば任意の入出力の関係を実現できるが，タスクよっては層を増やしたほうが必要なノード数は少なく済む．
ただし，層を増やしすぎると勾配消失が起こる．[深層学習, 人工知能学会]&lt;/p&gt;

&lt;h4 id=&quot;section-6&quot;&gt;参考&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;深層学習. 人工知能学会, 神嶌 敏弘, 麻生 英樹, 安田 宗樹, 前田 新一, 岡野原 大輔, 岡谷 貴之, 久保 陽太郎, Bollegala Danushka. 近代科学社, 2015.&lt;/li&gt;
  &lt;li&gt;深層学習 = Deep learning. 岡谷 貴之. 講談社, MLP機械学習プロフェッショナルシリーズ, 2015.&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Fri, 13 Nov 2015 21:00:00 +0900</pubDate>
        <link>/2015/11/backpropagation</link>
        <guid isPermaLink="true">/2015/11/backpropagation</guid>
        
        
      </item>
    
  </channel>
</rss>
