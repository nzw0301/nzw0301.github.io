<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>nzw page</title>
		<description>メモと個人ページ</description>
		<link></link>
		<atom:link href="/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>ACL 2015で気になったもの一覧</title>
				<description>&lt;p&gt;タイトルの通りである．&lt;/p&gt;

&lt;p&gt;論文の一覧は &lt;a href=&quot;http://www.aclweb.org/anthology/P/P15/&quot; title=&quot;ACL paper list&quot;&gt;ここ&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;sensembed-learning-sense-embeddingsfor-word-and-relational-similarity&quot;&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/P/P15/P15-1010.pdf&quot; title=&quot;pdf link&quot;&gt;SENSEMBED:Learning Sense Embeddingsfor Word and Relational Similarity&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;単語の意味のレベルで分散表現の獲得．意味のついたでかいコーパスがないので，WSDのstate-of-artsな手法を使う&lt;/p&gt;

&lt;h1 id=&quot;learning-continuous-word-embedding-with-metadata-for-question-retrieval-in-community-question-answering&quot;&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/P/P15/P15-1025.pdf&quot; title=&quot;pdf link&quot;&gt;Learning Continuous Word Embedding with Metadata for Question Retrieval in Community Question Answering&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;コミュニティQAサイトで過去に出た質問を探すために，分散表現をQAのタグやカテゴリと一緒に学習する．Skim-gram（word2vecに実装されてるやつ）の拡張．レシピでも同じことできそうだなぁと思った．（類似レシピを探して嬉しいかどうかはよくわからないけど）&lt;/p&gt;

&lt;h1 id=&quot;その他アブスト読んだけどぱっと説明できなかったもの&quot;&gt;その他アブスト読んだけどぱっと説明できなかったもの&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/P/P15/P15-1077.pdf&quot; title=&quot;link&quot;&gt;Gaussian LDA for Topic Models with Word Embeddings&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;LDAの多項分布をガウス分布を使う手法．word embeddingsとの関係がいまいちつかめなかったので後で読む気がする&lt;/p&gt;

&lt;p&gt;knowledge Graph系2つ，もう一個はあったけどクラウドソーシング使ってて多分違うのでリストから外した．&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/P/P15/P15-1067.pdf&quot;&gt;Knowledge Graph Embedding via Dynamic Mapping Matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/P/P15/P15-1009.pdf&quot;&gt;Semantically Smooth Knowledge Graph Embedding&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
				<pubDate>Mon, 28 Sep 2015 16:30:00 +0900</pubDate>
				<link>/2015/09/ACL2015InterestingPaper</link>
				<guid isPermaLink="true">/2015/09/ACL2015InterestingPaper</guid>
			</item>
		
			<item>
				<title>Antisocial Behavior in Online Discussion Communities 読んだ</title>
				<description>&lt;p&gt;当該の論文は&lt;a href=&quot;http://cs.stanford.edu/people/jure/pubs/trolls-icwsm15.pdf&quot; title=&quot;論文リンク&quot;&gt;これ&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;基本的に(nzw:)はnzwが感じことなどを書きます．&lt;/p&gt;

&lt;h1 id=&quot;既存研究との違い&quot;&gt;既存研究との違い&lt;/h1&gt;

&lt;p&gt;既存研究では，少人数のコミュニティを対象に人力で解析していたが，この研究では大規模なデータを解析&lt;/p&gt;

&lt;h1 id=&quot;概要&quot;&gt;概要&lt;/h1&gt;

&lt;p&gt;ニュースサイトやゲームサイトなどのコミュニティでは，ユーザ同士で議論を行うことができる．
この論文でも使われているコミュニティサイトだと &lt;a href=&quot;http://www.breitbart.com/big-government/2015/09/22/reza-aslan-gop-party-xenophobia-anti-muslim-bashing/&quot; title=&quot;breitbart&quot;&gt;例&lt;/a&gt;のように1つの記事に対してコメント書くことができ，それに対してリプライできる．
(nzw:1記事に5000以上のコメントが付いているのでデータ量はかなりある印象)&lt;/p&gt;

&lt;p&gt;これらのコミュニティでは，釣りなど迷惑行為を行うユーザが問題となる．
そこで迷惑行為(antisocial behavior)を行うユーザの特徴を調べ，コミュニティに参加した初期段階でそのユーザが将来的にバンされるかを予測を行う．
永久的にバンされたユーザを迷惑行為を行うユーザの正解データと定義して実験．&lt;/p&gt;

&lt;p&gt;扱うデータの特徴は以下のような感じ&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;記事に対してユーザはコメントをつけることができる&lt;/li&gt;
&lt;li&gt;コメントにvoteやリプライできる&lt;/li&gt;
&lt;li&gt;管理者によってだけ投稿は削除される&lt;/li&gt;
&lt;li&gt;ユーザは他のユーザの投稿を報告できる&lt;/li&gt;
&lt;li&gt;迷惑行為がひどいユーザは管理者からバンされる &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;この研究で使われる手法としては統計解析，機械学習（ロジスティック回帰とランダムフォレスト），クラウドソーシング(mechanical task)．&lt;/p&gt;

&lt;p&gt;以下では，論文に合わせて，バンされるユーザをFBUs，バンされないユーザをNBUsと表記．
実験の結果から明らかになったことは&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;FBUsの投稿文は読みにくい(ARIで計算)&lt;/li&gt;
&lt;li&gt;FBUsはリプライを受けやすい&lt;/li&gt;
&lt;li&gt;FBUsは複数のスレッド(スレッドは記事の単位)ではなく一部のスレッドに集中しやすい&lt;/li&gt;
&lt;li&gt;FBUsの投稿内容は時間経過にともない悪化&lt;/li&gt;
&lt;li&gt;時間経過によってコミュニティから寛容的に見られなくなる&lt;/li&gt;
&lt;li&gt;FBUsの平均投稿回数は264回（一般的なユーザは平均22回)&lt;/li&gt;
&lt;li&gt;FBUsの投稿は同じスレッドの前の投稿文と比べると類似度が低い&lt;/li&gt;
&lt;li&gt;他ユーザと同じような投稿内容でも自分だけ削除されると迷惑行為は悪化&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;あるユーザがバンされるかの予測では，最初の5~10投稿を使うだけで十分な性能であった．
投稿数が増えるほどバンされるかの予測が困難になる．&lt;/p&gt;

&lt;p&gt;また別のコミュニティサイトで学習したモデルを使っても性能が出る（特徴量は論文の表3を参照）&lt;/p&gt;
</description>
				<pubDate>Wed, 23 Sep 2015 16:30:00 +0900</pubDate>
				<link>/2015/09/paper</link>
				<guid isPermaLink="true">/2015/09/paper</guid>
			</item>
		
			<item>
				<title>PRML 演習2.15</title>
				<description>&lt;p&gt;多変量ガウス分布のエントロピーが&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
H[\boldsymbol{x}] = \frac{1}{2} \ln | \boldsymbol{\Sigma}| + \frac{D}{2}(1+\ln(2\pi))
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;になることを示す．&lt;/p&gt;

&lt;p&gt;多変量ガウス分布は連続であるのでエントロピーの定義から&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
H[\boldsymbol{x}] = - \int p(\boldsymbol{x}) \ln p(\boldsymbol{x}) d \boldsymbol{x}.
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;また多変量ガウス分布は，
式(2.43)をエントロピーの式に代入し， \(\ln\) の関数の期待値とみることができるので&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
H[\boldsymbol{x}] &amp;amp;=&amp;amp; - \int p(\boldsymbol{x}) \ln \Bigl(\frac{1}{(2\pi)^\frac{D}{2}}
\frac{1}{|\boldsymbol{\Sigma}|^{\frac{1}{2}}}
exp \bigl({-\frac{1}{2} (\boldsymbol{x}-\boldsymbol{\mu})^{\mathrm{T}} \boldsymbol{\Sigma}^{-1}
(\boldsymbol{x}-\boldsymbol{\mu})}
\bigr)\Bigr) d \boldsymbol{x} \\
&amp;amp;=&amp;amp;-(-\frac{D}{2}\ln{(2\pi)} -
\frac{1}{2}\ln |\boldsymbol{\Sigma}| -
\frac{1}{2}
E[(\boldsymbol{x}-\boldsymbol{\mu})^{\mathrm{T}} \boldsymbol{\Sigma}^{-1} (\boldsymbol{x}-\boldsymbol{\mu})]
) \\
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;ここでトレースを使う（線形代数でトレースを習わなかったので，ここに時間がかかりました）&lt;/p&gt;

&lt;h1 id=&quot;トレース&quot;&gt;トレース&lt;/h1&gt;

&lt;p&gt;トレースは，行列の対角和のことです．
単位行列のトレースは，単位行列の次元数です．&lt;/p&gt;

&lt;p&gt;トレースの性質から
もしAが対称行列であるなら&lt;/p&gt;

&lt;p&gt;\( X^{\mathrm{T}} A X = tr(A X X^{\mathrm{T}} ) \)&lt;/p&gt;

&lt;p&gt;を満たす．&lt;/p&gt;

&lt;p&gt;さきほどの式の最後の項の期待値の中身がこれで書き換えると，&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
(\boldsymbol{x}-\boldsymbol{\mu})^{\mathrm{T}} \boldsymbol{\Sigma}^{-1} (\boldsymbol{x}-\boldsymbol{\mu}) =
tr (\boldsymbol{\Sigma}^{-1} (\boldsymbol{x}-\boldsymbol{\mu}) (\boldsymbol{x}-\boldsymbol{\mu})^{\mathrm{T}})
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;第3項目だけ取り出して変形していく．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
E[(\boldsymbol{x}-\boldsymbol{\mu})^{\mathrm{T}} \boldsymbol{\Sigma}^{-1} (\boldsymbol{x}-\boldsymbol{\mu})] &amp;amp;=&amp;amp;
E[tr (\boldsymbol{\Sigma}^{-1} (\boldsymbol{x}-\boldsymbol{\mu}) (\boldsymbol{x}-\boldsymbol{\mu})^{\mathrm{T}})] \\
&amp;amp;=&amp;amp; tr (\boldsymbol{\Sigma^{-1}} E[(\boldsymbol{x}-\boldsymbol{\mu}) (\boldsymbol{x} -
\boldsymbol{\mu}^{\mathrm{T}})]) \\
&amp;amp;=&amp;amp; tr (\boldsymbol{\Sigma^{-1}} \boldsymbol{\Sigma} ) \\
&amp;amp;=&amp;amp; D
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;よって&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
H[\boldsymbol{x}] = \frac{1}{2} \ln | \boldsymbol{\Sigma}| + \frac{D}{2}(1+\ln(2\pi))
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;以上.&lt;/p&gt;

&lt;h1 id=&quot;所感&quot;&gt;所感&lt;/h1&gt;

&lt;p&gt;期待値は線形性を持っているのでトレースと順番入れ替えたのがちょっと心配です．&lt;/p&gt;

&lt;h1 id=&quot;参考にした記事&quot;&gt;参考にした記事&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://sucrose.hatenablog.com/entry/2013/07/20/190146&quot;&gt;正規分布間のKLダイバージェンスの導出&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf&quot;&gt;Matrix Cookbook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.r.dl.itc.u-tokyo.ac.jp/~nakagawa/SML1/math1.pdf&quot;&gt;付録1．数学の復習&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
				<pubDate>Tue, 21 Jul 2015 01:30:00 +0900</pubDate>
				<link>/2015/07/prml2_15</link>
				<guid isPermaLink="true">/2015/07/prml2_15</guid>
			</item>
		
			<item>
				<title>PRML 演習2.12</title>
				<description>&lt;h1 id=&quot;はじめに&quot;&gt;はじめに&lt;/h1&gt;

&lt;p&gt;9章で混合ガウス分布がこれでもかと出てくるので，一旦2章まで戻りました．
ディリクレ分布までやったので今度はガウス分布です．&lt;/p&gt;

&lt;h4 id=&quot;1-正規化の確認&quot;&gt;1. 正規化の確認&lt;/h4&gt;

&lt;p&gt;連続変数なので，全区間を積分して1になることを確かめればよい．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
\int_a^b \frac{1}{b-a} dx &amp;amp;=&amp;amp; [\frac{x}{b-a}]_a^b \\
&amp;amp;=&amp;amp; \frac{b}{b-a} - \frac{a}{b-a} \\
&amp;amp;=&amp;amp; 1
\end{eqnarray}&lt;/p&gt;

&lt;h4 id=&quot;2-平均&quot;&gt;2. 平均&lt;/h4&gt;

&lt;p&gt;式(1.34)を使うだけ&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
E[x] &amp;amp;=&amp;amp; \int_a^b x \frac{1}{b-a} dx \\
&amp;amp;=&amp;amp; [\frac{x^2}{2(b-a)}]_a^b \\
&amp;amp;=&amp;amp; \frac{b^2}{2(b-a)} - \frac{a^2}{2(b-a)} \\
&amp;amp;=&amp;amp; \frac{a+b}{2}
\end{eqnarray}&lt;/p&gt;

&lt;h4 id=&quot;3-分散&quot;&gt;3. 分散&lt;/h4&gt;

&lt;p&gt;求めた平均と式(1.39)を使う&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
var[x] &amp;amp;=&amp;amp; \int_a^b x^2 \frac{1}{b-a} dx - E[x]^2 \\
&amp;amp;=&amp;amp; [\frac{x^3}{3(b-a)}]_a^b - \frac{(a+b)^2}{4}\\
&amp;amp;=&amp;amp; \frac{b^2+ab+a^2}{3} - \frac{(a+b)^2}{4}\\
&amp;amp;=&amp;amp; \frac{(a-b)^2}{12}
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;一様分布についてのもう少しだけ詳しい説明が付録Bにあります．&lt;/p&gt;
</description>
				<pubDate>Thu, 16 Jul 2015 23:09:00 +0900</pubDate>
				<link>/2015/07/prml2_12</link>
				<guid isPermaLink="true">/2015/07/prml2_12</guid>
			</item>
		
			<item>
				<title>Python勉強メモ</title>
				<description>&lt;h1 id=&quot;はじめに&quot;&gt;はじめに&lt;/h1&gt;

&lt;p&gt;講義の関係で少しだけNLTKとかgensimとかflaskを触りましたが，
全然Pythonわからないので，いい加減勉強します．&lt;/p&gt;

&lt;p&gt;図書館にあったこれをひと通り読むつもりです．&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.amazon.co.jp/gp/product/4873116880/ref=as_li_ss_il?ie=UTF8&amp;camp=247&amp;creative=7399&amp;creativeASIN=4873116880&amp;linkCode=as2&amp;tag=algebrae-22&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;http://ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;ASIN=4873116880&amp;Format=_SL110_&amp;ID=AsinImage&amp;MarketPlace=JP&amp;ServiceVersion=20070822&amp;WS=1&amp;tag=algebrae-22&quot; &gt;&lt;/a&gt;&lt;img src=&quot;http://ir-jp.amazon-adsystem.com/e/ir?t=algebrae-22&amp;l=as2&amp;o=9&amp;a=4873116880&quot; width=&quot;1&quot; height=&quot;1&quot; border=&quot;0&quot; alt=&quot;&quot; style=&quot;border:none !important; margin:0px !important;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;環境&quot;&gt;環境&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;pyenv&lt;/li&gt;
&lt;li&gt;anaconda3-2.1.0&lt;/li&gt;
&lt;li&gt;Jupyter&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Javaをよく書くのでJythonも検討しましたが，Javaとの連携がめんどうそうで手間でやめました．&lt;/p&gt;

&lt;p&gt;速い速いと聞くpypyもやろうとしましたが，パッケージ周りがめんどうだったのでやめました．&lt;/p&gt;

&lt;p&gt;scikit-learnなどをやりたい方はanacondaを入れると必要なパッケージがだいたい入ってて簡単です．&lt;/p&gt;

&lt;h1 id=&quot;本題&quot;&gt;本題&lt;/h1&gt;

&lt;p&gt;まずを環境つくります．
グローバルでパッケージを管理したくないので，virtualenvを使います．&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;pip install virtualenv
virtualenv no-site-packages anaconda3
cd anaconda3
. bin/activate #この環境を使う
deactivate #この環境から抜ける
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;REPLでも十分ですが，jupyterでよりインタラクティブにpython使ってみます．
ちなみにJuliaもつかえます．&lt;/p&gt;

&lt;p&gt;jupyterからhtmlのslideも作れちゃうので，tutorialには便利そうです．&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;pip install ipython
ipython notebook #ブラウザが起動
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
				<pubDate>Thu, 16 Jul 2015 02:25:00 +0900</pubDate>
				<link>/2015/07/pythonIntro</link>
				<guid isPermaLink="true">/2015/07/pythonIntro</guid>
			</item>
		
			<item>
				<title>PRML演習9.9 pi</title>
				<description>&lt;h3 id=&quot;本題&quot;&gt;本題&lt;/h3&gt;

&lt;p&gt;負担率を固定した際に，式(9.40)を
\(\pi_{k}\)
について最大化しようとすると式(9.22)で与えられる陽な解が得られることを示す．&lt;/p&gt;

&lt;p&gt;\( \sum_k \pi_k = 1 \)
という条件があるので，ラグランジュの未定乗数法を利用する．&lt;/p&gt;

&lt;p&gt;式(9.40)を
\(\pi_{k}\)
について最大化しつつ，
\( \sum_k \pi_k = 1 \)
を満たすように最大化を行う．&lt;/p&gt;

&lt;p&gt;ラグランジュ関数は
\( L(\boldsymbol{\pi},\lambda) = f(\boldsymbol{\pi}) + \lambda ( \sum_k \pi_k - 1) \)
となる．&lt;/p&gt;

&lt;p&gt;\( f(\boldsymbol{\pi} ) \)
は式9.40をさします．&lt;/p&gt;

&lt;p&gt;ラグランジュ関数を
\(\boldsymbol{\pi_k}\)
について微分して0と置いた式と制約を満たすことから，&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
\sum^N_{n=1} \gamma(z_{nk}) \frac{1}{\pi_k} + \lambda = 0 \\
\sum_k \pi_k - 1 = 0
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;を使って
\( \lambda \)
について求める．&lt;/p&gt;

&lt;p&gt;微分した式を条件式に代入すると，&lt;/p&gt;

&lt;p&gt;\( - \sum_k \frac{N_k}{\lambda} = 1 \)
なので，&lt;/p&gt;

&lt;p&gt;\( \lambda = -N \)&lt;/p&gt;

&lt;p&gt;よって&lt;/p&gt;

&lt;p&gt;\( \pi_k = \frac{N_k}{N} \)
式(9.22)を導くことができた．&lt;/p&gt;
</description>
				<pubDate>Wed, 15 Jul 2015 12:25:00 +0900</pubDate>
				<link>/2015/07/PRML99pi</link>
				<guid isPermaLink="true">/2015/07/PRML99pi</guid>
			</item>
		
			<item>
				<title>PRML演習9.8</title>
				<description>&lt;h3 id=&quot;本題&quot;&gt;本題&lt;/h3&gt;

&lt;p&gt;負担率を固定した際に，式(9.40)を
\(\boldsymbol{\mu_{k}}\)
について最大化しようとすると式(9.17)で与えられる陽な解が得られることを示す．&lt;/p&gt;

&lt;p&gt;式(9.40)を
\(\boldsymbol{\mu_{k}}\)
で微分する．
式(9.16)を)参考．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
\frac{\partial}{\partial \mu_k} E_\boldsymbol{z}
[\boldsymbol{X},\boldsymbol{Z}|\boldsymbol{\mu},\boldsymbol{\Sigma},\boldsymbol{\pi}]
&amp;amp;=&amp;amp; 
\sum_{n=1}^{N} \gamma(z_{zk}) \boldsymbol{\Sigma_k^{-1}}(\boldsymbol{x_n}-\boldsymbol{\mu_k})
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;微分した式を0とおいて，
\(\mu_k\)
について求める．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
\sum_{n=1}^{N} \gamma(z_{zk}) \boldsymbol{\Sigma_k^{-1}}(\boldsymbol{x_n}-\boldsymbol{\mu_k})
&amp;amp;=&amp;amp; 0 \\
\sum_{n=1}^{N} \gamma(z_{zk})(\boldsymbol{x_n}-\boldsymbol{\mu_k})
&amp;amp;=&amp;amp; 0 \\
\sum_{n=1}^{N} \gamma(z_{zk})\boldsymbol{\mu_k}
&amp;amp;=&amp;amp; 
\sum_{n=1}^{N} \gamma(z_{zk}) \boldsymbol{x_n} \\
\boldsymbol{\mu_k} 
&amp;amp;=&amp;amp;
\frac{1}{\sum_{n=1}^{N} \gamma(z_{zk})}\sum_{n=1}^{N} \gamma(z_{zk}) \boldsymbol{x_n}
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;以上から式(9.17)が得られた．&lt;/p&gt;

&lt;h3 id=&quot;所感&quot;&gt;所感&lt;/h3&gt;

&lt;p&gt;公式の答えは上巻のガウス分布の式を使っていますので，私の解答はちょっとやりかたが違ってるかもしれません．&lt;/p&gt;
</description>
				<pubDate>Sun, 12 Jul 2015 22:25:00 +0900</pubDate>
				<link>/2015/07/PRML98</link>
				<guid isPermaLink="true">/2015/07/PRML98</guid>
			</item>
		
			<item>
				<title>PRML演習9.4</title>
				<description>&lt;h3 id=&quot;おさらい&quot;&gt;おさらい&lt;/h3&gt;

&lt;p&gt;p156にあるように一般のEMアルゴリズムでは，
尤度関数 \(p(\boldsymbol{X}|\boldsymbol{\theta})\) を
\(\boldsymbol{\theta}\)
について最大化を目的とした．&lt;/p&gt;

&lt;p&gt;今回の問いでは尤度関数を\(p(\boldsymbol{\theta}|\boldsymbol{X})\) にして\(\boldsymbol{\theta}\) について最大化する．&lt;/p&gt;

&lt;h3 id=&quot;本題&quot;&gt;本題&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;観測変数は
\(\boldsymbol{X}\)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;\(\boldsymbol{\theta}\) は任意のパラメータ&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;\(\boldsymbol{Z}\)
は潜在変数&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;まず
パラメータの初期値
\(\boldsymbol{\theta^{old}}\)
を選ぶ．&lt;/p&gt;

&lt;p&gt;Eステップ．
\(p(\boldsymbol{Z}|\boldsymbol{X},\boldsymbol{\theta^{old}})\)
を計算する．
ここまではp156のアルゴリズムと同じ．&lt;/p&gt;

&lt;p&gt;Mステップ．
最大化する対数尤度関数は
\(\ln{p(\boldsymbol{\theta}|\boldsymbol{X})} \)
である．&lt;/p&gt;

&lt;p&gt;ベイズの定理から&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
\ln{p(\boldsymbol{\theta}|\boldsymbol{X})}  &amp;amp;=&amp;amp;
\ln{\frac{p(\boldsymbol{X}|\boldsymbol{\theta})p(\boldsymbol{\theta})}{p(\boldsymbol{X})}}  \\
&amp;amp;=&amp;amp; \ln{p(\boldsymbol{X}|\boldsymbol{\theta})}
+ \ln{p(\boldsymbol{\theta})}
- \ln{p(\boldsymbol{X})} \\
&amp;amp;=&amp;amp; \ln{(\sum_{\boldsymbol{Z}} p(\boldsymbol{X,Z}|\boldsymbol{\theta}))}
+ \ln{p(\boldsymbol{\theta})}
- \ln{p(\boldsymbol{X})}\
\end{eqnarray}
と変形する.&lt;/p&gt;

&lt;p&gt;変形した式を
\(\boldsymbol{\theta}\)
について最大化するが，第1項は．Q関数なので，&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
\boldsymbol{\theta^{new}} &amp;amp;=&amp;amp; argmax_{\boldsymbol{\theta}} (Q(\boldsymbol{\theta},\boldsymbol{\theta^{old}}) + \ln{p(\boldsymbol{\theta})})
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;をMステップでは計算する．&lt;/p&gt;

&lt;p&gt;ベイズの定理で変形して出てきた
\(\ln{p(\boldsymbol{X})}\)
は
\(\boldsymbol{\theta}\)
とは，関係ないために定数として扱えるので，最大化する際には無視できる．&lt;/p&gt;

&lt;h3 id=&quot;その他&quot;&gt;その他&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;http://amzn.to/1fki6cc&quot;&gt;言語処理のための機械学習入門 (自然言語処理シリーズ)  &lt;/a&gt;のp88を参考にしました．&lt;/p&gt;
</description>
				<pubDate>Tue, 07 Jul 2015 22:25:06 +0900</pubDate>
				<link>/2015/07/PRML94</link>
				<guid isPermaLink="true">/2015/07/PRML94</guid>
			</item>
		
			<item>
				<title>Hello</title>
				<description>&lt;h1 id=&quot;半分くらい移転しました&quot;&gt;半分くらい移転しました&lt;/h1&gt;

&lt;p&gt;はてなブログで数式を書くと展開されずにつらいかったり，やたら読み込みが重いので，こちらに書いていこうかと思います．&lt;/p&gt;
</description>
				<pubDate>Sun, 05 Jul 2015 18:25:06 +0900</pubDate>
				<link>/2015/07/hello</link>
				<guid isPermaLink="true">/2015/07/hello</guid>
			</item>
		
			<item>
				<title>PRML演習9.3</title>
				<description>&lt;p&gt;\( K=2 \) とすると定義より \(\boldsymbol{z}\) は
\begin{align}
\boldsymbol{z} = [0,1],[1,0]
\end{align}
の2つのベクトルのうちどちらかを取りうる．&lt;/p&gt;

&lt;p&gt;また，以下の式の一番外側のシグマはこの2つの \(\boldsymbol{z}\) を足し合わせる．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
\sum_{\boldsymbol{z}}p(\boldsymbol{z})p(\boldsymbol{x}|\boldsymbol{z}) &amp;amp;=&amp;amp; \sum_{\boldsymbol{z}} \prod_{k=1}^2 \pi_k^{z_k} \prod_{k=1}^2 N(\boldsymbol{x}|\boldsymbol{\mu_k}, \Sigma_{k})^{z_k} \\
&amp;amp;=&amp;amp; \sum_{\boldsymbol{z}}(\pi_1^{z_1}\pi_2^{z_2})(N(\boldsymbol{x}|\boldsymbol{\mu_1},\Sigma_1)^{z_1} N(\boldsymbol{x}|\boldsymbol{\mu_2},\Sigma_2)^{z_2} \\
&amp;amp;=&amp;amp; \pi_1 N(\boldsymbol{x}|\boldsymbol{\mu_1},\Sigma_1) + \pi_2 N(\boldsymbol{x}|\boldsymbol{\mu_2},\Sigma_2) \\
&amp;amp;=&amp;amp; \sum_{k=1}^2 \pi_k N(\boldsymbol{x}|\boldsymbol{\mu_k},\Sigma_k)
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;一般化して \(K\) の場合は，
\(\sum_{\boldsymbol{z}}p(\boldsymbol{z})p(\boldsymbol{x}|\boldsymbol{z})\)
は，内部の同時確率を変形した式を \(\prod\) 展開してから
\(\sum_{\boldsymbol{z}}\) で足しあわせる．&lt;/p&gt;

&lt;p&gt;足し合わせると
\(z_k=1\) を満たす \(k\) での
\(\pi_k N(\boldsymbol{x}|\boldsymbol{\mu_k},\Sigma_k)\) の
添字が1から\(K\)までの和だけが残るために
式(9.12)のように周辺化することで式(9.7)を導くことができる．&lt;/p&gt;
</description>
				<pubDate>Sun, 05 Jul 2015 14:25:06 +0900</pubDate>
				<link>/2015/07/PRML93</link>
				<guid isPermaLink="true">/2015/07/PRML93</guid>
			</item>
		
	</channel>
</rss>
