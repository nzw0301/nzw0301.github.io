<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>η</title>
		<description>η is ...</description>
		<link></link>
		<atom:link href="/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>青深層学習 4章 実装</title>
				<description>&lt;h1 id=&quot;はじめに&quot;&gt;はじめに&lt;/h1&gt;

&lt;p&gt;前回まで数式の展開をしていたので，それをもとにして実装を行いました．&lt;/p&gt;

&lt;p&gt;Pythonを使いました．&lt;/p&gt;

&lt;h1 id=&quot;データとタスク&quot;&gt;データとタスク&lt;/h1&gt;

&lt;p&gt;データは例によって夢野久作の作品(&lt;a href=&quot;http://www.aozora.gr.jp/cards/000096/files/2122_21847.html&quot;&gt;オンチ&lt;/a&gt;)を使います．&lt;/p&gt;

&lt;p&gt;1文が登場人物の発言なのか，地の文なのかの2値分類を行います．
活性化関数はすべてロジスティック関数，損失関数は対数尤度です．
（なので出力層のユニット数は1つ）&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;tag sentence&lt;/p&gt;

&lt;p&gt;1 退け 退け ッ &lt;/p&gt;

&lt;p&gt;0 疎ら に なっ た 群衆 の 背後 から 、 今 出 た ばかり の 旭 が キラキラ と 映し 込ん で 来 た 。 &lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;このように地の文であれば0，発言であれば1をつけます．&lt;/p&gt;

&lt;h1 id=&quot;コード&quot;&gt;コード&lt;/h1&gt;

&lt;p&gt;&lt;code&gt;hidden_layer&lt;/code&gt; で指定する層を変えても問題ないように作ってあるので層の増減とユニット数の増減は好きに試すことができます．&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/nzw0301/363b803268c2ece127f2.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;テストデータは作ってないので，パラメータによる誤差関数の変動のグラフをみて投稿を締めようかと思います．&lt;/p&gt;

&lt;p&gt;上記のコード
1つ目がミニバッチを使った確率的勾配法
2つ目が全データで学習する勾配法&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/nn_err_rate.svg&quot; alt=&quot;nn&quot;&gt; &lt;/p&gt;

&lt;p&gt;学習回数&lt;code&gt;400&lt;/code&gt;，学習率&lt;code&gt;0.2&lt;/code&gt;で固定し，隠れ層をいじってみます．
&lt;img src=&quot;/images/nn_hidden.svg&quot; alt=&quot;nn&quot;&gt; &lt;/p&gt;

&lt;p&gt;学習率&lt;code&gt;0.2&lt;/code&gt;と隠れ層を固定し，確率的勾配法を使います．
ミニバッチによる変化を見ています．
&lt;img src=&quot;/images/nn_minibatch.svg&quot; alt=&quot;nn&quot;&gt; &lt;/p&gt;

&lt;p&gt;以上です．&lt;/p&gt;
</description>
				<pubDate>Mon, 09 Nov 2015 21:00:00 +0900</pubDate>
				<link>/2015/11/blueDeepLearningChapter44code</link>
				<guid isPermaLink="true">/2015/11/blueDeepLearningChapter44code</guid>
			</item>
		
			<item>
				<title>青深層学習 4.4.2 行列表記</title>
				<description>&lt;h1 id=&quot;はじめに&quot;&gt;はじめに&lt;/h1&gt;

&lt;p&gt;前回の続きです．&lt;/p&gt;

&lt;p&gt;順伝播，誤差逆伝播，重みとバイアスの更新を行列の表記で行います．&lt;/p&gt;

&lt;h1 id=&quot;表記&quot;&gt;表記&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;\(N\) ：ミニバッチのデータ数&lt;/li&gt;
&lt;li&gt;\(\boldsymbol{x}_n\)：1件のデータ(例えば1つ文書，行は単語の頻度とか)&lt;/li&gt;
&lt;li&gt;\(\boldsymbol{X} = \left[ \begin{array}{c} \boldsymbol{x}_1 ... \boldsymbol{x}_N \end{array} \right]\)：ミニバッチの行列&lt;/li&gt;
&lt;li&gt;\(\boldsymbol{W^{(l)}} = w_{ji}^{(l)} \)：\(l\)層のおける重みの行列 要素はユニット\(i\)からユニット\(j\)のリンクの重み&lt;/li&gt;
&lt;li&gt;\(\boldsymbol{b}^{(l)}\)：\(l\)層のバイアスのベクトル&lt;/li&gt;
&lt;li&gt;\(\boldsymbol{u}^{(l)}_n\)：データ\(\boldsymbol{x}_n\)のときの，\(l\)層の入力ベクトル 1行目はユニットの1つ目の入力に対応&lt;/li&gt;
&lt;li&gt;\(\boldsymbol{U}^{(l)} = \left[ \begin{array}{c} \boldsymbol{u}_1^{(l)} ... \boldsymbol{u}_N^{(l)} \end{array} \right]\)：\(l\)層の入力の行列，列がデータ1件，行がユニットに対応&lt;/li&gt;
&lt;li&gt;\(\boldsymbol{Z}^{(l)} = \left[ \begin{array}{c} \boldsymbol{z}_1^{(l)} ... \boldsymbol{z}_N^{(l)} \end{array} \right]\)：\(l\)層の出力の行列，列がデータ1件，行がユニットに対応，\(\boldsymbol{U}^{(l)}\)の各要素に活性化を適用しただけ&lt;/li&gt;
&lt;li&gt;\(\boldsymbol{Y} = \left[ \begin{array}{c} \boldsymbol{y}_1 ... \boldsymbol{y}_N \end{array} \right]\)：各データに対する最終的な出力を列にもつ行列&lt;/li&gt;
&lt;li&gt;\(\boldsymbol{\Delta^{(l)}}\)：列がミニバッチのデータ，行がユニットのデルタ\(\delta^{(l)}_j\)の行列&lt;/li&gt;
&lt;li&gt;k：出力層のユニット数&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;順伝播&quot;&gt;順伝播&lt;/h1&gt;

&lt;p&gt;入力は恒等写像なので， \(\boldsymbol{Z}^{(1)}=\boldsymbol{X}\) ．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
\boldsymbol{U}^{(l)} &amp;amp;=&amp;amp;\boldsymbol{W^{(l)}} \boldsymbol{Z^{(l-1)}} + \boldsymbol{b}^{(l)} \boldsymbol{1}_N^{T} \tag{1} \\
\boldsymbol{Z}^{(l)} &amp;amp;=&amp;amp; f^{(l)}(\boldsymbol{U}^{(l)}) \tag{2}
\end{eqnarray}&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;\(j\)：\((l)\)層のユニット数（バイアス除く）&lt;/li&gt;
&lt;li&gt;\(i\)：\((l-1)\)層のユニット数（バイアス除く）&lt;/li&gt;
&lt;li&gt;\(\boldsymbol{W}^{(l)}\)：\(j \times i\)&lt;/li&gt;
&lt;li&gt;\(\boldsymbol{U}^{(l)}\)：\(j \times N\)&lt;/li&gt;
&lt;li&gt;\(\boldsymbol{Z}^{(l)}\)：\(j \times N\)&lt;/li&gt;
&lt;li&gt;\(\boldsymbol{b}^{(l)}\)：\(j \times 1\)&lt;/li&gt;
&lt;li&gt;\(\boldsymbol{1}_N\)：\(1 \times N\)&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;逆伝播&quot;&gt;逆伝播&lt;/h1&gt;

&lt;p&gt;出力層のデルタ\(\boldsymbol{\Delta^{(L)}} = \boldsymbol{D} - \boldsymbol{Y}\)
を\((L-1)\)~\(2\)まで逆伝播させる．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
\boldsymbol{\Delta}^{(l)} = f&amp;#39;^{(l)}(\boldsymbol{U}^{(l)}) \odot (\boldsymbol{W}^{(l+1)T} \boldsymbol{\Delta}^{(l+1)}) \tag{3}
\end{eqnarray}&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;\(\boldsymbol{\Delta^{(L)}} , \boldsymbol{D} , \boldsymbol{Y}\)：\(k \times N\)&lt;/li&gt;
&lt;li&gt;\(\boldsymbol{\Delta^{(l)}} \)：\(j \times N\)&lt;/li&gt;
&lt;li&gt;\(\odot\)：&lt;a href=&quot;https://ja.wikipedia.org/wiki/%E3%82%A2%E3%83%80%E3%83%9E%E3%83%BC%E3%83%AB%E7%A9%8D&quot; title=&quot;アダマール積&quot;&gt;アダマール積&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;重み更新&quot;&gt;重み更新&lt;/h1&gt;

&lt;p&gt;逆伝播で計算したデルタを使って微分し，重み\(\boldsymbol{W}\)を更新．
誤差逆伝播が漸化式で求めたので，重みの更新は並列して計算ができる．&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;\(\partial \boldsymbol{W}^{(l)} \)：\((l)\)層の重み\(w_{ji}^{(l)}\)で誤差関数\(E=\sum_{n=1}^N E_n(\boldsymbol{W})\)を微分した値を\((j,i)\)成分にもつ行列&lt;/li&gt;
&lt;li&gt;\(\partial \boldsymbol{b}^{(l)} \)：\((l)\)層の重み\(b_{j}^{(l)}\)で誤差関数\(E=\sum_{n=1}^N E_n(\boldsymbol{W})\)を微分した値を\((j)\)成分にもつ列ベクトル&lt;/li&gt;
&lt;li&gt;\(\epsilon\)：学習率&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;\begin{eqnarray}
\partial \boldsymbol{W}^{(l)} &amp;amp;=&amp;amp; \frac{1}{N} \boldsymbol{\Delta^{(l)}} \boldsymbol{Z^{(l-1)T}} \tag{4}\\
\partial \boldsymbol{b}^{(l)} &amp;amp;=&amp;amp; \frac{1}{N} \boldsymbol{\Delta^{(l)}} \boldsymbol{1}_N \tag{5}
\end{eqnarray}&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;\(\partial \boldsymbol{W}^{(l)}\)：\(j \times i\)&lt;/li&gt;
&lt;li&gt;\(\partial \boldsymbol{b}^{(l)}\)：\(j \times 1\)&lt;/li&gt;
&lt;li&gt;\(\boldsymbol{Z^{(l-1)}} \)：\(i \times N\)&lt;/li&gt;
&lt;li&gt;\(\boldsymbol{\Delta^{(l)}} \)：\(j \times N\)&lt;/li&gt;
&lt;li&gt;\(j\)：\((l)\)層のユニット数（バイアス除く）&lt;/li&gt;
&lt;li&gt;\(i\)：\((l-1)\)層のユニット数（バイアス除く）&lt;/li&gt;
&lt;li&gt;\(N\)：ミニバッチのデータ数&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;更新式は，&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
\boldsymbol{W}^{(l)} &amp;amp;\leftarrow&amp;amp;
\boldsymbol{W}^{(l)}
- \epsilon \partial \boldsymbol{W}^{(l)} \tag{6} \\ 
\boldsymbol{b}^{(l)} &amp;amp;\leftarrow&amp;amp;
\boldsymbol{b}^{(l)}
- \epsilon \partial \boldsymbol{b}^{(l)} \tag{7} \\ 
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;以上です．&lt;/p&gt;
</description>
				<pubDate>Mon, 09 Nov 2015 01:22:00 +0900</pubDate>
				<link>/2015/11/blueDeepLearningChapter442</link>
				<guid isPermaLink="true">/2015/11/blueDeepLearningChapter442</guid>
			</item>
		
			<item>
				<title>青深層学習 4.4.1 出力層でのデルタ</title>
				<description>&lt;h1 id=&quot;はじめに&quot;&gt;はじめに&lt;/h1&gt;

&lt;p&gt;前回の続きです．&lt;/p&gt;

&lt;p&gt;高校生で培った微分を駆使してがんばります．&lt;/p&gt;

&lt;h2 id=&quot;出力層のデルタ&quot;&gt;出力層のデルタ&lt;/h2&gt;

&lt;p&gt;出力層のデルタを求めます．
出力層のデルタは，誤差関数\(E_n\)を，出力層の入力 \(u_{j}^{(L)}\) で微分(式1)である．&lt;/p&gt;

&lt;p&gt;なので，出力層の活性化関数と誤差関数によって計算が異なる（本書の例の場合結果は全て同じになる）&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
\delta_j^{(L)} = 
  \frac{\partial E_n }{\partial u_{j}^{(L)}} \tag1
\end{eqnarray}&lt;/p&gt;

&lt;h1 id=&quot;回帰&quot;&gt;回帰&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;活性化関数：恒等写像&lt;/li&gt;
&lt;li&gt;誤差関数：2乗誤差&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2乗誤差は以下の式2であり，出力層が恒等写像なので，\(\boldsymbol{y}=\boldsymbol{z}=\boldsymbol{u}\)
より式4に変形できる．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
E_n &amp;amp;=&amp;amp; \frac{1}{2} ||\boldsymbol{y} - \boldsymbol{d} ||^2 \tag{2}\\
&amp;amp;=&amp;amp; \frac{1}{2}\sum_j (y_j-d_j)^2 \tag{3} \\
&amp;amp;=&amp;amp; \frac{1}{2}\sum_j (u_j^{(L)} - d_j)^2 \tag{4} \\
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;式4を\(u_j^{(L)}\)で微分すればいいので\(L\)層のデルタは，
\begin{eqnarray}
\delta_j^{(L)} &amp;amp;=&amp;amp; u_j^{(L)} - d_j \tag{5} \\
&amp;amp;=&amp;amp; y_j - d_j \tag{6}
\end{eqnarray}&lt;/p&gt;

&lt;h1 id=&quot;2値分類&quot;&gt;2値分類&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;活性化関数：ロジスティック関数&lt;/li&gt;
&lt;li&gt;誤差関数：対数尤度&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2値分類なので出力層のユニットは1つだけ．&lt;/p&gt;

&lt;p&gt;対数尤度の総和の内側のデルタ（データ1つに対するデルタ）を求める．&lt;/p&gt;

&lt;p&gt;式7で分子を対数尤度で展開し，式8はchain ruleを適用してyで展開する．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
\delta^{(L)} &amp;amp;=&amp;amp; \frac
{(\partial d \log(y) + (1-d)\log(1-y))}
{\partial u} \tag{7} \\
&amp;amp;=&amp;amp; \frac
{(\partial d \log(y) + (1-d)\log(1-y))}
{\partial y}
\frac{\partial y}
{\partial u} \tag{8} \\
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;式8の第1項目は，対数の微分，2項目は，出力層がロジスティック関数なので， \(y=\frac{1}{1+\exp(-u)}\) を\(u\)で微分する必要がある（高校でいうところの商の微分）．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
\delta^{(L)} &amp;amp;=&amp;amp; (\frac{d}{y} - \frac{1-d}{1-y}) y(1-y) \tag{9}  \\
&amp;amp;=&amp;amp; d(1-y)-(1-d)y \tag{10} \\
&amp;amp;=&amp;amp; d-y \tag{11} \\
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;ロジスティック関数をちゃんと微分すると以下のようになる．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
\frac{\partial y} {\partial u} &amp;amp;=&amp;amp; \frac{\partial}{\partial u} \frac{1}{1+\exp(-u)} \\
&amp;amp;=&amp;amp; \frac{(-1)(1+\exp(-u))(-1)}{(1+\exp(-u))^2}\\
&amp;amp;=&amp;amp; \frac{1+\exp(-u)}{(1+\exp(-u))^2}\\
&amp;amp;=&amp;amp;(\frac{1}{1+\exp(-u)})(1-\frac{1}{1+\exp(-u)})\\
&amp;amp;=&amp;amp;y(1-y)
\end{eqnarray}&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;合成関数の微分&lt;/li&gt;
&lt;li&gt;商の微分&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;って名前がついてたきがする．&lt;/p&gt;

&lt;h1 id=&quot;多値分類&quot;&gt;多値分類&lt;/h1&gt;

&lt;p&gt;（これが一番時間がかかった）&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;活性化関数：ソフトマックス関数&lt;/li&gt;
&lt;li&gt;誤差関数：交差エントロピー&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;誤差関数をソフトマックス関数で展開すると式13になる．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
E_n &amp;amp;=&amp;amp; - \sum_k d_k \log(y_k) \tag{12} \\
&amp;amp;=&amp;amp; - \sum_k d_k \log (\frac{\exp(u_k^{(L)})}{\sum_i \exp(u_i^{(L)})}) \tag{13}
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;\(\delta^{(L)}\)を求めるには，chain ruleを使って\(y\)で展開する．&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;\begin{eqnarray}
\delta^{(L)} &amp;amp;=&amp;amp; \sum_k \frac{\partial E_n}{\partial y_k} \frac{\partial y_k}{u_j^{(L)}} \tag{14} \\
&amp;amp;=&amp;amp; \sum_k (-1) \frac{d_k}{y_k} \frac{\partial }{u_j^{(L)}} \frac{\exp(u_k^{(L)})}{\sum_i \exp(u_i^{(L)})} \tag{15}\\
&amp;amp;=&amp;amp; 
- \frac{d_j}{y_j} \frac{\exp(u_j^{(L)}) \{ \sum_i \exp(u_i^{(L)})\} - \{\exp(u_j^{(L)})\}^2 }{\{ \sum_i \exp(u_i^{(L)})\}^2}
- \sum_{k \neq j} \frac{d_k}{y_k} \frac{-\{\exp(u_k^{(L)})\}\{\exp(u_j^{(L)})\}}{\{\sum_i \exp(u_i^{(L)})\}^2}
 \tag{16}\\
&amp;amp;=&amp;amp;
- d_j \frac{\sum_i \exp(u_i^{(L)}) - 
\exp(u_j^{(L)})
  }{\sum_i \exp(u_i^{(L)})}
+ \sum_{k \neq j} \frac{d_k}{y_k} y_k y_j
 \tag{17}\\
 &amp;amp;=&amp;amp;
- d_j + d_j y_j
+ \sum_{k \neq j} d_k y_j
 \tag{18}\\ 
 &amp;amp;=&amp;amp; - d_j + \sum_{k} d_k y_j \tag{19}\\
 &amp;amp;=&amp;amp; - d_j + y_j \tag{20}\\
\end{eqnarray}&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;式15：第1項目はそのまま誤差関数を \(y_k\)で微分し，2項目はソフトマックスに展開する．&lt;/li&gt;
&lt;li&gt;式16：\(k=j\)と\(k \neq j\)の2つで変わるので，2つに分解して商の微分をする．&lt;/li&gt;
&lt;li&gt;式17,式18：ソフトマックス関数で約分して整理．&lt;/li&gt;
&lt;li&gt;式19：第3項に第2項を合わせる．&lt;/li&gt;
&lt;li&gt;式20：\(\sum_k d_k = 1\)&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;まとめ&quot;&gt;まとめ&lt;/h1&gt;

&lt;p&gt;全部出力と正解の差になる．&lt;/p&gt;
</description>
				<pubDate>Mon, 09 Nov 2015 01:22:00 +0900</pubDate>
				<link>/2015/11/blueDeepLearningChapter44</link>
				<guid isPermaLink="true">/2015/11/blueDeepLearningChapter44</guid>
			</item>
		
			<item>
				<title>青深層学習 4.3 多層ネットワークへの一般化</title>
				<description>&lt;h1 id=&quot;はじめに&quot;&gt;はじめに&lt;/h1&gt;

&lt;p&gt;前回 &lt;a href=&quot;http://nzw0301.github.io/2015/11/blueDeepLearningChapter42/&quot; title=&quot;青深層学習，誤差逆伝播法の計算&quot;&gt;青深層学習，誤差逆伝播法の計算&lt;/a&gt;
の続きです．&lt;/p&gt;

&lt;p&gt;誤差逆伝播を層が全体で3層だけの場合ではなく，\(L\)層に拡張した時の挙動を確認します．&lt;/p&gt;

&lt;p&gt;前回と赤で強調する場所が違うかもしれませんが，ご了承ください．&lt;/p&gt;

&lt;h1 id=&quot;本題&quot;&gt;本題&lt;/h1&gt;

&lt;p&gt;第\(l\)層の重み\(w_{ji}^{(l)}\)で誤差関数を微分することを考える．&lt;/p&gt;

&lt;p&gt;前回と同じくネットワークを示すと赤いリンクの重みで誤差関数を微分する．\(l+1\) 層にもバイアスはある気がするので，ここでは書いている．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/nn431.svg&quot; alt=&quot;nn&quot;&gt;&lt;/p&gt;

&lt;p&gt;前回の中間層の重みの微分と同じく，\(w_{ji}^{(l)}\)は\(l\)層のユニット\(j\)の総入力\(u_j^{(l)}\)の一部なので，chain ruleを使って展開する(式1)．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
\frac{\partial E_n }{\partial w_{ji}^{(l)}} &amp;amp;=&amp;amp;
\frac{\partial E_n }{\partial u_j^{(l)}} \frac{\partial u_j^{(l)} }{\partial w_{ji}^{(l)}} \tag{1}\\
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;式1の第1項\(\frac{\partial E_n }{\partial u_j^{(l)}}\)をまず展開する．
下ぼネットワークで示したように
\(u_j^{(l)}\)は，活性化関数\(f\)が適用され，
出力\(z_j^{(l)}\)になり，赤いリンクの重みがかかって\(l+1\)層のユニット((\(l+1\))層のバイアス以外のユニット)入力の一部になる．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/nn432.svg&quot; alt=&quot;nn&quot;&gt;&lt;/p&gt;

&lt;p&gt;なので，\(E_n\)を微分するために，chain ruleで展開する．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
\frac{\partial E_n }{\partial u_{j}^{(l)}} &amp;amp;=&amp;amp;
\sum_{k=1}^{K} \frac{\partial E_n }{\partial u_k^{(l+1)}} \frac{\partial u_k^{(l+1)} }{\partial u_j^{(l)}} \tag{2}\\
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;ここで式3のようにデルタを定義する．
デルタは各層の各ユニットに存在する．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
\delta_j^{(l)} \equiv 
  \frac{\partial E_n }{\partial u_{j}^{(l)}} \tag{3}
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;式2を式3を使った形で展開する．&lt;/p&gt;

&lt;p&gt;式2の右辺を項ごとに展開する．
\(\frac{\partial E_n }{\partial u_k^{(l+1)}} \frac{\partial u_k^{(l+1)} }{\partial u_j^{(l)}}\)の1項目は，式3のデルタの層が違うだけなので，&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
\frac{\partial E_n }{\partial u_k^{(l+1)}} = \delta_k^{(l+1)} \tag{4}
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;2項目 \(\frac{\partial u_k^{(l+1)} }{\partial u_j^{(l)}}\)は，前回展開やったように定義に戻って展開する．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
\frac{\partial u_k^{(l+1)} }{\partial u_{j}^{(l)}} &amp;amp;=&amp;amp; \frac{\partial \sum_{j&amp;#39;} w_{kj&amp;#39;}^{(l+1)} z_{j&amp;#39;}^{(l)} }{\partial u_j^{(l)}} \tag{5} \\
&amp;amp;=&amp;amp; \frac{\partial \sum_{j&amp;#39;} w_{kj&amp;#39;}^{(l+1)} f(u_{j&amp;#39;}^{(l)}) }{\partial u_j^{(l)}} \tag{6} \\
&amp;amp;=&amp;amp;  w_{kj}^{(l+1)} f&amp;#39;(u_{j}^{(l)})  \tag{7} \\
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;よって式2に式4と式6を代入するとデルタの関係式が得られる．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
\delta_j^{(l)}  &amp;amp;=&amp;amp; \sum_{k=1}^{K} \delta_k^{(l+1)} (w_{kj}^{(l+1)} f&amp;#39;(u_{j}^{(l)})) \tag{8}
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;これは\(l\)層のデルタは(\(l+1\))層の全デルタで計算されることを示す．
（くどいが，最終的に必要なデルタは式1の1項目であり，\(L\)層から(\(l+1\))層までの全デルタを順伝播とは逆向きに求める．これが逆伝播の由来らしい）&lt;/p&gt;

&lt;p&gt;式1の2項目は簡単に求められる．（前回の中間層の重みと同じ）
\begin{eqnarray}
\frac{\partial u_j^{(l)}} {\partial w_{ji}^{(l)}} &amp;amp;=&amp;amp; \frac{\partial \sum_{i&amp;#39;} w_{ji&amp;#39;}^{(l)} z_{i&amp;#39;}^{(l-1)} }{\partial w_{ji}^{(l)}} \tag{9}\\
  &amp;amp;=&amp;amp; z_i^{(l-1)} \tag{10}
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;式1に式8と式10を代入すれば任意の重みがデルタと\(z\)で表現できる．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
\frac{\partial E_n }{\partial w_{ji}^{(l)}} &amp;amp;=&amp;amp;
\delta_j^{(l)} z_i^{(l-1)} \tag{11}\\
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;式8の通り，デルタは漸化式の形をしている．&lt;/p&gt;

&lt;p&gt;最初のデルタは第\(L\)層のデルタであり，
つまりは，誤差関数を出力層の入力\(u_j^{(L)}\)の微分である．&lt;/p&gt;
</description>
				<pubDate>Mon, 09 Nov 2015 01:22:00 +0900</pubDate>
				<link>/2015/11/blueDeepLearningChapter43</link>
				<guid isPermaLink="true">/2015/11/blueDeepLearningChapter43</guid>
			</item>
		
			<item>
				<title>青深層学習，誤差逆伝播法の計算</title>
				<description>&lt;h1 id=&quot;はじめに&quot;&gt;はじめに&lt;/h1&gt;

&lt;p&gt;青いほうの深層学習を読んでいたら誤差逆伝播法の式変形が意味不明だったので，自分なりのやり方で書いてみます．&lt;/p&gt;

&lt;p&gt;（個人的には中間層の重みを計算するやり方のほうがしっくりきて出力層の重みで転置がでてくるのがちょっと天下りっぽくて理解しにくかった，ベクトル解析をやってないせいだろうか）&lt;/p&gt;

&lt;h1 id=&quot;本題&quot;&gt;本題&lt;/h1&gt;

&lt;p&gt;2つあるので片方ずつやっていきます．&lt;/p&gt;

&lt;p&gt;このような3層のネットワークを考えます．(p42)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/nn.svg&quot; alt=&quot;nn&quot;&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;回帰を扱うネットワークなので，output layerの活性化関数は恒等写像&lt;/li&gt;
&lt;li&gt;\(E_n\) は，データ \(\boldsymbol{x_n}\) に対する誤差関数（ここでは自乗誤差），対応する正解データは， \(\boldsymbol{d_n}\) &lt;/li&gt;
&lt;li&gt;\(w_{ji}^{(3)}\) 3層目の重みでhidden layerのユニット \(i\) からoutput layerのユニット \(j\) へのリンクの重み &lt;/li&gt;
&lt;li&gt;+1 はバイアスのユニット&lt;/li&gt;
&lt;li&gt;Input layerの x1~x4 は1つデータ各次元の値，この場合は \(\boldsymbol{x}_n\) は4次元のベクトル&lt;/li&gt;
&lt;li&gt;出力は，3次元のベクトル \(\boldsymbol{y(x_n)}\)&lt;/li&gt;
&lt;li&gt;\(u_i^{(2)}\) は hidden layerのユニット \(i\) の入力&lt;/li&gt;
&lt;li&gt;\(z_i^{(2)}\) は hidden layerのユニット \(i\) の出力&lt;/li&gt;
&lt;li&gt;\(f\) は活性化関数&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;です．&lt;/p&gt;

&lt;p&gt;ちなみに，そもそもなんで微分を求めるかというと誤差関数を最小とするような重みを2章の勾配法によって求めるのに必要なためです．&lt;/p&gt;

&lt;h1 id=&quot;出力層の重み&quot;&gt;出力層の重み&lt;/h1&gt;

&lt;p&gt;数学がよくわからないので微分したらなんで転置になるのかわからなくてこれが手間取った．&lt;/p&gt;

&lt;p&gt;とりあえず愚直に総和を展開します．&lt;/p&gt;

&lt;p&gt;微分するほうもされるほうもスカラなので結果もスカラになる（はず）．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
\frac{\partial E_n }{\partial w_{ji}^{(3)}} &amp;amp;=&amp;amp;
\sum_{k=1}^3 \frac{\partial E_n }{\partial y_{k}} \frac{\partial y_k }{\partial w_{ji}^{(3)}} \tag{1}\\
&amp;amp;=&amp;amp; \sum_{k=1}^3 \frac{\partial ( \frac{1}{2} \sum^3_{l=1} (y_l(\boldsymbol{x}) - d_l)^2) }{\partial y_{k}}\frac{\partial y_k}{\partial w_{ji}^{(3)}} \tag{2}\\
&amp;amp;=&amp;amp; \sum_{k=1}^3 \frac{\partial ( \frac{1}{2} \sum^3_{l=1} (y_l(\boldsymbol{x}) - d_l)^2) }{\partial y_{k}}
\frac{\sum_{i&amp;#39;} w_{ki&amp;#39;}^{(3)}z_{i&amp;#39;}^{(2)}  }{\partial w_{ji}^{(3)}} \tag{3}\\
&amp;amp;=&amp;amp; (y_j(\boldsymbol{x})-d_j) z_i^{(2)} \tag{4}
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/nn1.svg&quot; alt=&quot;nn&quot;&gt;
式1の左辺，この図の赤いリンクの重みが \(w_{ji}^{(3)}\) ．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/nn3.svg&quot; alt=&quot;nn&quot;&gt; 式1右辺，chain ruleで展開 \(y_k\) を間に挟む&lt;/p&gt;

&lt;p&gt;式2，誤差関数 \(E_n\) を定義通り展開．&lt;/p&gt;

&lt;p&gt;式3，chain ruleで展開したもう一方も定義に従って展開する．出力層なので，テキスト式(4.3)に対応．&lt;/p&gt;

&lt;p&gt;式4，最終的には \(k=l=j , i=i&amp;#39;\)でない項はすべて0になる（ \(y_1, y_2\) は無関係）．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/nn4.svg&quot; alt=&quot;nn&quot;&gt;&lt;/p&gt;

&lt;p&gt;最終的にこの赤い部分についてだけ微分すればよいとわかる&lt;/p&gt;

&lt;p&gt;（直感的には出力層から入力層に向かって重み \(w_{ji}^{(3)}\) に関係する部分だけ微分してる）&lt;/p&gt;

&lt;h1 id=&quot;中間層の重みの微分&quot;&gt;中間層の重みの微分&lt;/h1&gt;

&lt;p&gt;(出力層から遠いほど計算がしんどくなってくるので，出力層の重みより計算は多くなりますが，がんばります．)&lt;/p&gt;

&lt;p&gt;中間層に入る重みでの微分は
\( \frac{\partial E_n }{\partial w_{ji}^{(2)}} \)
で表される．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/nn5.svg&quot; alt=&quot;nn&quot;&gt; &lt;/p&gt;

&lt;p&gt;つまり赤いリンクの重みで自乗誤差を微分する．&lt;/p&gt;

&lt;p&gt;\(w_{ij}^{(2)}\) はhidden layerのユニット \(j\) の入力の一部として伝わるので，chain ruleを適用する．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
\frac{\partial E_n }{\partial w_{ji}^{(2)}} = \frac{\partial E_n }{\partial u_j^{(2)}}  \frac{\partial u_j^{(2)}} {\partial w_{ji}^{(2)}} 
\end{eqnarray} &lt;/p&gt;

&lt;p&gt;このとき，2項目 \(\frac{\partial u_j^{(2)}} {\partial w_{ji}^{(2)}}\) は，&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
\frac{\partial u_j^{(2)}} {\partial w_{ji}^{(2)}} &amp;amp;=&amp;amp; \frac{\partial \sum_{i&amp;#39;} w_{ji&amp;#39;}^{(2)} z_{i&amp;#39;}^{(1)} }{\partial w_{ji}^{(2)}} \tag{1}\\
  &amp;amp;=&amp;amp; z_i^{(1)} \tag{2}
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/nn6.svg&quot; alt=&quot;nn&quot;&gt; &lt;/p&gt;

&lt;p&gt;式1の右辺の分子はInput layerの各出力 \(z_i^{(1)}\) と赤いリンクの重みの積の総和．&lt;/p&gt;

&lt;p&gt;これを \(w_{ij}^{(2)}\) で微分するので \(z_i^{(1)}\) 以外は0になる．&lt;/p&gt;

&lt;p&gt;つまり第1層の \(i\) 番目の出力．（今回は \(x_i\) に相当）&lt;/p&gt;

&lt;hr&gt;

&lt;hr&gt;

&lt;hr&gt;

&lt;hr&gt;

&lt;p&gt;続いて第1項目 \(\frac{\partial E_n }{\partial u_j^{(2)}}\) ．&lt;/p&gt;

&lt;p&gt;\(u_j^{(2)}\) (hidden layerのユニット\(j\) の入力) は活性化関数 \(f\)が適用され，重み \(w_{kj}^{(3)}  (k=1,2,3)\) (下図の赤線の重み) がかかってoutput layerの全てのユニットの入力の一部となる．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/nn2.svg&quot; alt=&quot;nn&quot;&gt;&lt;/p&gt;

&lt;p&gt;式4.3から \(E_n\) が \(u_k^{(3)}(k=1,2,3)\) の関数であるので，chain ruleを適用する．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
\frac{\partial E_n}{\partial u_j^{(2)}} &amp;amp;=&amp;amp; \sum_k \frac{\partial E_n}{\partial u_k^{(3)}} \frac{\partial u_k^{(3)}}{\partial u_j^{(2)}}
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;図で表すと赤色のユニットの入力によって微分する．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/nn7.svg&quot; alt=&quot;nn&quot;&gt;&lt;/p&gt;

&lt;p&gt;項ごとに展開を行う．出力層と同じく，第1項は，\(E_n\)を求めていく．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
\frac{\partial E_n}{\partial u_k^{(3)}} &amp;amp;=&amp;amp; \frac{\partial \frac{1}{2} \sum_{k^{&amp;#39;}} (u_{k^{&amp;#39;}}^{(3)} - d_{k^{&amp;#39;}})^2}{\partial u_k^{(3)}} \\
&amp;amp;=&amp;amp; u_k^{(3)} - d_k
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;となる．&lt;/p&gt;

&lt;p&gt;\(y1\) ~ \(y3\) (図の赤い\(y1\) ~ \(y3\)) からそれぞれの正解の値 \(d_k\) との差の総和が分子で，それ \(k\) の入力で微分しているので，結局図の\(y1\)しか残らない．&lt;/p&gt;

&lt;p&gt;今回は， \( y_k = z_k^{(3)} = u_k^{(3)} \) であるので簡単に求められている．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/nn8.svg&quot; alt=&quot;nn&quot;&gt;&lt;/p&gt;

&lt;hr&gt;

&lt;hr&gt;

&lt;hr&gt;

&lt;hr&gt;

&lt;p&gt;2項目\(\frac{\partial u_k^{(3)}}{\partial u_j^{(2)}}\)も同様に分子を展開して微分する．&lt;/p&gt;

&lt;p&gt;微分する前に確認すると，\(k\)の入力を \(j\)の入力で微分するので，赤い部分に注目していることになる．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/nn9.svg&quot; alt=&quot;nn&quot;&gt; &lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
\frac{\partial u_k^{(3)}}{\partial u_j^{(2)}} &amp;amp;=&amp;amp; \frac{\partial \sum_{j&amp;#39;} w_{k{j&amp;#39;}}^{(3)} f(u_{j&amp;#39;}^{(2)}) }{\partial u_j^{(2)}} \tag{1}\\
&amp;amp;=&amp;amp; w_{kj}^{(3)} f&amp;#39;(u_j^{(2)}) \tag{2}
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;式1は分子にあるユニット \(k\) の入力を展開しているので，下図の赤いリンクの重みと赤いノードのユニットの出力の形で書き直しているだけ．&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/nn10.svg&quot; alt=&quot;nn&quot;&gt; &lt;/p&gt;

&lt;p&gt;\(u_j^{(2)}\) で微分するので，結局分子が \(j=j&amp;#39;\)のときだけが残るので，式2となる．&lt;/p&gt;

&lt;p&gt;以上をまとめると&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
\frac{\partial E_n }{\partial w_{ji}^{(2)}} = (f&amp;#39;(u_j^{(2)}) \sum_k w_{kj}^{(3)} (u_k^{(3)} -d_k)) z_i^{(1)}
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;となる．&lt;/p&gt;

&lt;h1 id=&quot;所感&quot;&gt;所感&lt;/h1&gt;

&lt;p&gt;青い深層学習も紫の深層学習も数式が丁寧におえないような印象があるので，適宜
&lt;a href=&quot;http://goodfeli.github.io/dlbook/&quot;&gt;Yoshua Bengio, Ian Goodfellow and Aaron CourvilleさんたちのDeep Learning&lt;/a&gt;を参照するといいと思った．
Chain ruleから説明があるしかなり分かりやすかった．&lt;/p&gt;
</description>
				<pubDate>Tue, 03 Nov 2015 06:30:00 +0900</pubDate>
				<link>/2015/11/blueDeepLearningChapter42</link>
				<guid isPermaLink="true">/2015/11/blueDeepLearningChapter42</guid>
			</item>
		
			<item>
				<title>ACL 2015で気になったもの一覧</title>
				<description>&lt;p&gt;タイトルの通りである．&lt;/p&gt;

&lt;p&gt;論文の一覧は &lt;a href=&quot;http://www.aclweb.org/anthology/P/P15/&quot; title=&quot;ACL paper list&quot;&gt;ここ&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;sensembed-learning-sense-embeddingsfor-word-and-relational-similarity&quot;&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/P/P15/P15-1010.pdf&quot; title=&quot;pdf link&quot;&gt;SENSEMBED:Learning Sense Embeddingsfor Word and Relational Similarity&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;単語の意味のレベルで分散表現の獲得．意味のついたでかいコーパスがないので，WSDのstate-of-artsな手法を使う&lt;/p&gt;

&lt;h1 id=&quot;learning-continuous-word-embedding-with-metadata-for-question-retrieval-in-community-question-answering&quot;&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/P/P15/P15-1025.pdf&quot; title=&quot;pdf link&quot;&gt;Learning Continuous Word Embedding with Metadata for Question Retrieval in Community Question Answering&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;コミュニティQAサイトで過去に出た質問を探すために，分散表現をQAのタグやカテゴリと一緒に学習する．Skim-gram（word2vecに実装されてるやつ）の拡張．レシピでも同じことできそうだなぁと思った．（類似レシピを探して嬉しいかどうかはよくわからないけど）&lt;/p&gt;

&lt;h1 id=&quot;その他アブスト読んだけどぱっと説明できなかったもの&quot;&gt;その他アブスト読んだけどぱっと説明できなかったもの&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/P/P15/P15-1077.pdf&quot; title=&quot;link&quot;&gt;Gaussian LDA for Topic Models with Word Embeddings&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;LDAの多項分布をガウス分布を使う手法．word embeddingsとの関係がいまいちつかめなかったので後で読む気がする&lt;/p&gt;

&lt;p&gt;knowledge Graph系2つ，もう一個はあったけどクラウドソーシング使ってて多分違うのでリストから外した．&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/P/P15/P15-1067.pdf&quot;&gt;Knowledge Graph Embedding via Dynamic Mapping Matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.aclweb.org/anthology/P/P15/P15-1009.pdf&quot;&gt;Semantically Smooth Knowledge Graph Embedding&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
				<pubDate>Mon, 28 Sep 2015 16:30:00 +0900</pubDate>
				<link>/2015/09/ACL2015InterestingPaper</link>
				<guid isPermaLink="true">/2015/09/ACL2015InterestingPaper</guid>
			</item>
		
			<item>
				<title>Antisocial Behavior in Online Discussion Communities 読んだ</title>
				<description>&lt;p&gt;当該の論文は&lt;a href=&quot;http://cs.stanford.edu/people/jure/pubs/trolls-icwsm15.pdf&quot; title=&quot;論文リンク&quot;&gt;これ&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;基本的に(nzw:)はnzwが感じことなどを書きます．&lt;/p&gt;

&lt;h1 id=&quot;既存研究との違い&quot;&gt;既存研究との違い&lt;/h1&gt;

&lt;p&gt;既存研究では，少人数のコミュニティを対象に人力で解析していたが，この研究では大規模なデータを解析&lt;/p&gt;

&lt;h1 id=&quot;概要&quot;&gt;概要&lt;/h1&gt;

&lt;p&gt;ニュースサイトやゲームサイトなどのコミュニティでは，ユーザ同士で議論を行うことができる．
この論文でも使われているコミュニティサイトだと &lt;a href=&quot;http://www.breitbart.com/big-government/2015/09/22/reza-aslan-gop-party-xenophobia-anti-muslim-bashing/&quot; title=&quot;breitbart&quot;&gt;例&lt;/a&gt;のように1つの記事に対してコメント書くことができ，それに対してリプライできる．
(nzw:1記事に5000以上のコメントが付いているのでデータ量はかなりある印象)&lt;/p&gt;

&lt;p&gt;これらのコミュニティでは，釣りなど迷惑行為を行うユーザが問題となる．
そこで迷惑行為(antisocial behavior)を行うユーザの特徴を調べ，コミュニティに参加した初期段階でそのユーザが将来的にバンされるかを予測を行う．
永久的にバンされたユーザを迷惑行為を行うユーザの正解データと定義して実験．&lt;/p&gt;

&lt;p&gt;扱うデータの特徴は以下のような感じ&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;記事に対してユーザはコメントをつけることができる&lt;/li&gt;
&lt;li&gt;コメントにvoteやリプライできる&lt;/li&gt;
&lt;li&gt;管理者によってだけ投稿は削除される&lt;/li&gt;
&lt;li&gt;ユーザは他のユーザの投稿を報告できる&lt;/li&gt;
&lt;li&gt;迷惑行為がひどいユーザは管理者からバンされる &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;この研究で使われる手法としては統計解析，機械学習（ロジスティック回帰とランダムフォレスト），クラウドソーシング(mechanical task)．&lt;/p&gt;

&lt;p&gt;以下では，論文に合わせて，バンされるユーザをFBUs，バンされないユーザをNBUsと表記．
実験の結果から明らかになったことは&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;FBUsの投稿文は読みにくい(ARIで計算)&lt;/li&gt;
&lt;li&gt;FBUsはリプライを受けやすい&lt;/li&gt;
&lt;li&gt;FBUsは複数のスレッド(スレッドは記事の単位)ではなく一部のスレッドに集中しやすい&lt;/li&gt;
&lt;li&gt;FBUsの投稿内容は時間経過にともない悪化&lt;/li&gt;
&lt;li&gt;時間経過によってコミュニティから寛容的に見られなくなる&lt;/li&gt;
&lt;li&gt;FBUsの平均投稿回数は264回（一般的なユーザは平均22回)&lt;/li&gt;
&lt;li&gt;FBUsの投稿は同じスレッドの前の投稿文と比べると類似度が低い&lt;/li&gt;
&lt;li&gt;他ユーザと同じような投稿内容でも自分だけ削除されると迷惑行為は悪化&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;あるユーザがバンされるかの予測では，最初の5~10投稿を使うだけで十分な性能であった．
投稿数が増えるほどバンされるかの予測が困難になる．&lt;/p&gt;

&lt;p&gt;また別のコミュニティサイトで学習したモデルを使っても性能が出る（特徴量は論文の表3を参照）&lt;/p&gt;
</description>
				<pubDate>Wed, 23 Sep 2015 16:30:00 +0900</pubDate>
				<link>/2015/09/paper</link>
				<guid isPermaLink="true">/2015/09/paper</guid>
			</item>
		
			<item>
				<title>PRML 演習2.15</title>
				<description>&lt;p&gt;多変量ガウス分布のエントロピーが&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
H[\boldsymbol{x}] = \frac{1}{2} \ln | \boldsymbol{\Sigma}| + \frac{D}{2}(1+\ln(2\pi))
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;になることを示す．&lt;/p&gt;

&lt;p&gt;多変量ガウス分布は連続であるのでエントロピーの定義から&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
H[\boldsymbol{x}] = - \int p(\boldsymbol{x}) \ln p(\boldsymbol{x}) d \boldsymbol{x}.
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;また多変量ガウス分布は，
式(2.43)をエントロピーの式に代入し， \(\ln\) の関数の期待値とみることができるので&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
H[\boldsymbol{x}] &amp;amp;=&amp;amp; - \int p(\boldsymbol{x}) \ln \Bigl(\frac{1}{(2\pi)^\frac{D}{2}}
\frac{1}{|\boldsymbol{\Sigma}|^{\frac{1}{2}}}
exp \bigl({-\frac{1}{2} (\boldsymbol{x}-\boldsymbol{\mu})^{\mathrm{T}} \boldsymbol{\Sigma}^{-1}
(\boldsymbol{x}-\boldsymbol{\mu})}
\bigr)\Bigr) d \boldsymbol{x} \\
&amp;amp;=&amp;amp;-(-\frac{D}{2}\ln{(2\pi)} -
\frac{1}{2}\ln |\boldsymbol{\Sigma}| -
\frac{1}{2}
E[(\boldsymbol{x}-\boldsymbol{\mu})^{\mathrm{T}} \boldsymbol{\Sigma}^{-1} (\boldsymbol{x}-\boldsymbol{\mu})]
) \\
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;ここでトレースを使う（線形代数でトレースを習わなかったので，ここに時間がかかりました）&lt;/p&gt;

&lt;h1 id=&quot;トレース&quot;&gt;トレース&lt;/h1&gt;

&lt;p&gt;トレースは，行列の対角和のことです．
単位行列のトレースは，単位行列の次元数です．&lt;/p&gt;

&lt;p&gt;トレースの性質から
もしAが対称行列であるなら&lt;/p&gt;

&lt;p&gt;\( X^{\mathrm{T}} A X = tr(A X X^{\mathrm{T}} ) \)&lt;/p&gt;

&lt;p&gt;を満たす．&lt;/p&gt;

&lt;p&gt;さきほどの式の最後の項の期待値の中身がこれで書き換えると，&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
(\boldsymbol{x}-\boldsymbol{\mu})^{\mathrm{T}} \boldsymbol{\Sigma}^{-1} (\boldsymbol{x}-\boldsymbol{\mu}) =
tr (\boldsymbol{\Sigma}^{-1} (\boldsymbol{x}-\boldsymbol{\mu}) (\boldsymbol{x}-\boldsymbol{\mu})^{\mathrm{T}})
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;第3項目だけ取り出して変形していく．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
E[(\boldsymbol{x}-\boldsymbol{\mu})^{\mathrm{T}} \boldsymbol{\Sigma}^{-1} (\boldsymbol{x}-\boldsymbol{\mu})] &amp;amp;=&amp;amp;
E[tr (\boldsymbol{\Sigma}^{-1} (\boldsymbol{x}-\boldsymbol{\mu}) (\boldsymbol{x}-\boldsymbol{\mu})^{\mathrm{T}})] \\
&amp;amp;=&amp;amp; tr (\boldsymbol{\Sigma^{-1}} E[(\boldsymbol{x}-\boldsymbol{\mu}) (\boldsymbol{x} -
\boldsymbol{\mu}^{\mathrm{T}})]) \\
&amp;amp;=&amp;amp; tr (\boldsymbol{\Sigma^{-1}} \boldsymbol{\Sigma} ) \\
&amp;amp;=&amp;amp; D
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;よって&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
H[\boldsymbol{x}] = \frac{1}{2} \ln | \boldsymbol{\Sigma}| + \frac{D}{2}(1+\ln(2\pi))
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;以上.&lt;/p&gt;

&lt;h1 id=&quot;所感&quot;&gt;所感&lt;/h1&gt;

&lt;p&gt;期待値は線形性を持っているのでトレースと順番入れ替えたのがちょっと心配です．&lt;/p&gt;

&lt;h1 id=&quot;参考にした記事&quot;&gt;参考にした記事&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://sucrose.hatenablog.com/entry/2013/07/20/190146&quot;&gt;正規分布間のKLダイバージェンスの導出&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf&quot;&gt;Matrix Cookbook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.r.dl.itc.u-tokyo.ac.jp/~nakagawa/SML1/math1.pdf&quot;&gt;付録1．数学の復習&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
				<pubDate>Tue, 21 Jul 2015 01:30:00 +0900</pubDate>
				<link>/2015/07/prml2_15</link>
				<guid isPermaLink="true">/2015/07/prml2_15</guid>
			</item>
		
			<item>
				<title>PRML 演習2.12</title>
				<description>&lt;h1 id=&quot;はじめに&quot;&gt;はじめに&lt;/h1&gt;

&lt;p&gt;9章で混合ガウス分布がこれでもかと出てくるので，一旦2章まで戻りました．
ディリクレ分布までやったので今度はガウス分布です．&lt;/p&gt;

&lt;h4 id=&quot;1-正規化の確認&quot;&gt;1. 正規化の確認&lt;/h4&gt;

&lt;p&gt;連続変数なので，全区間を積分して1になることを確かめればよい．&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
\int_a^b \frac{1}{b-a} dx &amp;amp;=&amp;amp; [\frac{x}{b-a}]_a^b \\
&amp;amp;=&amp;amp; \frac{b}{b-a} - \frac{a}{b-a} \\
&amp;amp;=&amp;amp; 1
\end{eqnarray}&lt;/p&gt;

&lt;h4 id=&quot;2-平均&quot;&gt;2. 平均&lt;/h4&gt;

&lt;p&gt;式(1.34)を使うだけ&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
E[x] &amp;amp;=&amp;amp; \int_a^b x \frac{1}{b-a} dx \\
&amp;amp;=&amp;amp; [\frac{x^2}{2(b-a)}]_a^b \\
&amp;amp;=&amp;amp; \frac{b^2}{2(b-a)} - \frac{a^2}{2(b-a)} \\
&amp;amp;=&amp;amp; \frac{a+b}{2}
\end{eqnarray}&lt;/p&gt;

&lt;h4 id=&quot;3-分散&quot;&gt;3. 分散&lt;/h4&gt;

&lt;p&gt;求めた平均と式(1.39)を使う&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
var[x] &amp;amp;=&amp;amp; \int_a^b x^2 \frac{1}{b-a} dx - E[x]^2 \\
&amp;amp;=&amp;amp; [\frac{x^3}{3(b-a)}]_a^b - \frac{(a+b)^2}{4}\\
&amp;amp;=&amp;amp; \frac{b^2+ab+a^2}{3} - \frac{(a+b)^2}{4}\\
&amp;amp;=&amp;amp; \frac{(a-b)^2}{12}
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;一様分布についてのもう少しだけ詳しい説明が付録Bにあります．&lt;/p&gt;
</description>
				<pubDate>Thu, 16 Jul 2015 23:09:00 +0900</pubDate>
				<link>/2015/07/prml2_12</link>
				<guid isPermaLink="true">/2015/07/prml2_12</guid>
			</item>
		
			<item>
				<title>PRML演習9.9 pi</title>
				<description>&lt;h3 id=&quot;本題&quot;&gt;本題&lt;/h3&gt;

&lt;p&gt;負担率を固定した際に，式(9.40)を
\(\pi_{k}\)
について最大化しようとすると式(9.22)で与えられる陽な解が得られることを示す．&lt;/p&gt;

&lt;p&gt;\( \sum_k \pi_k = 1 \)
という条件があるので，ラグランジュの未定乗数法を利用する．&lt;/p&gt;

&lt;p&gt;式(9.40)を
\(\pi_{k}\)
について最大化しつつ，
\( \sum_k \pi_k = 1 \)
を満たすように最大化を行う．&lt;/p&gt;

&lt;p&gt;ラグランジュ関数は
\( L(\boldsymbol{\pi},\lambda) = f(\boldsymbol{\pi}) + \lambda ( \sum_k \pi_k - 1) \)
となる．&lt;/p&gt;

&lt;p&gt;\( f(\boldsymbol{\pi} ) \)
は式9.40をさします．&lt;/p&gt;

&lt;p&gt;ラグランジュ関数を
\(\boldsymbol{\pi_k}\)
について微分して0と置いた式と制約を満たすことから，&lt;/p&gt;

&lt;p&gt;\begin{eqnarray}
\sum^N_{n=1} \gamma(z_{nk}) \frac{1}{\pi_k} + \lambda = 0 \\
\sum_k \pi_k - 1 = 0
\end{eqnarray}&lt;/p&gt;

&lt;p&gt;を使って
\( \lambda \)
について求める．&lt;/p&gt;

&lt;p&gt;微分した式を条件式に代入すると，&lt;/p&gt;

&lt;p&gt;\( - \sum_k \frac{N_k}{\lambda} = 1 \)
なので，&lt;/p&gt;

&lt;p&gt;\( \lambda = -N \)&lt;/p&gt;

&lt;p&gt;よって&lt;/p&gt;

&lt;p&gt;\( \pi_k = \frac{N_k}{N} \)
式(9.22)を導くことができた．&lt;/p&gt;
</description>
				<pubDate>Wed, 15 Jul 2015 12:25:00 +0900</pubDate>
				<link>/2015/07/PRML99pi</link>
				<guid isPermaLink="true">/2015/07/PRML99pi</guid>
			</item>
		
	</channel>
</rss>
