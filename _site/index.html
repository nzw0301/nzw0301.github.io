<!DOCTYPE html>
<html lang="ja">

<head>
    <meta charset="UTF-8">
    <title>鴨川η</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script src="/js/jquery.min.js"></script>
    <script src="/js/bootstrap.min.js"></script>
    <link href="/css/bootstrap.min.css" rel="stylesheet">
    <link href="/css/theme.css" rel="stylesheet">
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>

<body>

<div class="container-fluid">
    <div class="row-fluid">
        <div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                  <span class="sr-only">Toggle navigation</span>
                  <span class="icon-bar"></span>
                  <span class="icon-bar"></span>
                  <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand" href="/">鴨川η</a>
              </div>
              <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav">
                    <li class="active"><a href="/">Home</a></li>
                    <li class="active visible-xs-block"><a href="/links.html">Links</a></li>
                    <li class="active"><a href="/archive.html">Archive</a></li>
                    <li class="active"><a href="/about.html">About</a></li>
                    <li class="active"><a href="/memo.html">Memo</a></li>
                    <li class="active"><a href="/feed.xml">RSS</a></li>

                </ul>
              </div>
        </div>
    </div>
</div>


<div class="container container-left">
    <div class="row">
        <div class="col-md-3 hidden-xs">
            <div class="sidebar well">
数式がメイン
</div>

<div class="sidebar well">
    <h1>Recent Posts</h1>
    <ul>
        
          <li><a href="/2015/11/blueDeepLearningChapter442">青深層学習 4.4.2 行列表記</a></li>
        
          <li><a href="/2015/11/blueDeepLearningChapter44">青深層学習 4.4.1 出力層でのデルタ</a></li>
        
          <li><a href="/2015/11/blueDeepLearningChapter43">青深層学習 4.3 多層ネットワークへの一般化</a></li>
        
          <li><a href="/2015/11/blueDeepLearningChapter42">青深層学習，誤差逆伝播法の計算</a></li>
        
          <li><a href="/2015/09/ACL2015InterestingPaper">ACL 2015で気になったもの一覧</a></li>
        
    </ul>
</div>

<div class="sidebar well">
<h1>Links</h1>
<ul>
  <li><a href="https://github.com/nzw0301">github</a></li>
  <li><a href="https://twitter.com/nozawa0301">Twitter</a></li>
  <li><a href="http://nzw.hatenablog.jp/">Hatena</a></li>
  <li><a href="http://wkblab.github.io/member/nzw.html">Research</a></li>
  <li><a href="http://www.amazon.co.jp/gp/registry/wishlist/?ie=UTF8&camp=1207&creative=8415&linkCode=shr&tag=algebrae-22&requiresSignIn=1">wish list</a></li>
</ul>

</div>

        </div>
        <div class="col-md-9">
            
<div class="article">
    <div class="well">
        <h1><a href="/2015/11/blueDeepLearningChapter442">Nov 9, 2015 - 青深層学習 4.4.2 行列表記</a></h1>
        
        <div class="content">
            <h1 id="はじめに">はじめに</h1>

<p>前回の続きです．</p>

<p>順伝播，誤差逆伝播，重みとバイアスの更新を行列の表記で行います．</p>

<h1 id="表記">表記</h1>

<ul>
<li>\(N\) ：ミニバッチのデータ数</li>
<li>\(\boldsymbol{x}_n\)：1件のデータ(例えば1つ文書，行は単語の頻度とか)</li>
<li>\(\boldsymbol{X} = \left[ \begin{array}{c} \boldsymbol{x}_1 ... \boldsymbol{x}_N \end{array} \right]\)：ミニバッチの行列</li>
<li>\(\boldsymbol{W^{(l)}} = w_{ji}^{(l)} \)：\(l\)層のおける重みの行列 要素はユニット\(i\)からユニット\(j\)のリンクの重み</li>
<li>\(\boldsymbol{b}^{(l)}\)：\(l\)層のバイアスのベクトル</li>
<li>\(\boldsymbol{u}^{(l)}_n\)：データ\(\boldsymbol{x}_n\)のときの，\(l\)層の入力ベクトル 1行目はユニットの1つ目の入力に対応</li>
<li>\(\boldsymbol{U}^{(l)} = \left[ \begin{array}{c} \boldsymbol{u}_1^{(l)} ... \boldsymbol{u}_N^{(l)} \end{array} \right]\)：\(l\)層の入力の行列，列がデータ1件，行がユニットに対応</li>
<li>\(\boldsymbol{Z}^{(l)} = \left[ \begin{array}{c} \boldsymbol{z}_1^{(l)} ... \boldsymbol{z}_N^{(l)} \end{array} \right]\)：\(l\)層の出力の行列，列がデータ1件，行がユニットに対応，\(\boldsymbol{U}^{(l)}\)の各要素に活性化を適用しただけ</li>
<li>\(\boldsymbol{Y} = \left[ \begin{array}{c} \boldsymbol{y}_1 ... \boldsymbol{y}_N \end{array} \right]\)：各データに対する最終的な出力を列にもつ行列</li>
<li>\(\boldsymbol{\Delta^{(l)}}\)：列がミニバッチのデータ，行がユニットのデルタ\(\delta^{(l)}_j\)の行列</li>
<li>k：出力層のユニット数</li>
</ul>

<h1 id="順伝播">順伝播</h1>

<p>入力は恒等写像なので， \(\boldsymbol{Z}^{(1)}=\boldsymbol{X}\) ．</p>

<p>\begin{eqnarray}
\boldsymbol{U}^{(l)} &amp;=&amp;\boldsymbol{W^{(l)}} \boldsymbol{Z^{(l-1)}} + \boldsymbol{b}^{(l)} \boldsymbol{1}_N^{T} \tag{1} \\
\boldsymbol{Z}^{(l)} &amp;=&amp; f^{(l)}(\boldsymbol{U}^{(l)}) \tag{2}
\end{eqnarray}</p>

<ul>
<li>\(j\)：\((l)\)層のユニット数（バイアス除く）</li>
<li>\(i\)：\((l-1)\)層のユニット数（バイアス除く）</li>
<li>\(\boldsymbol{W}^{(l)}\)：\(j \times i\)</li>
<li>\(\boldsymbol{U}^{(l)}\)：\(j \times N\)</li>
<li>\(\boldsymbol{Z}^{(l)}\)：\(j \times N\)</li>
<li>\(\boldsymbol{b}^{(l)}\)：\(j \times 1\)</li>
<li>\(\boldsymbol{1}_N\)：\(1 \times N\)</li>
</ul>

<h1 id="逆伝播">逆伝播</h1>

<p>出力層のデルタ\(\boldsymbol{\Delta^{(L)}} = \boldsymbol{D} - \boldsymbol{Y}\)
を\((L-1)\)~\(2\)まで逆伝播させる．</p>

<p>\begin{eqnarray}
\boldsymbol{\Delta}^{(l)} = f&#39;^{(l)}(\boldsymbol{U}^{(l)}) \odot (\boldsymbol{W}^{(l+1)T} \boldsymbol{\Delta}^{(l+1)}) \tag{3}
\end{eqnarray}</p>

<ul>
<li>\(\boldsymbol{\Delta^{(L)}} , \boldsymbol{D} , \boldsymbol{Y}\)：\(k \times N\)</li>
<li>\(\boldsymbol{\Delta^{(l)}} \)：\(j \times N\)</li>
<li>\(\odot\)：<a href="https://ja.wikipedia.org/wiki/%E3%82%A2%E3%83%80%E3%83%9E%E3%83%BC%E3%83%AB%E7%A9%8D" title="アダマール積">アダマール積</a></li>
</ul>

<h1 id="重み更新">重み更新</h1>

<p>逆伝播で計算したデルタを使って微分し，重み\(\boldsymbol{W}\)を更新．
誤差逆伝播が漸化式で求めたので，重みの更新は並列して計算ができる．</p>

<ul>
<li>\(\partial \boldsymbol{W}^{(l)} \)：\((l)\)層の重み\(w_{ji}^{(l)}\)で誤差関数\(E=\sum_{n=1}^N E_n(\boldsymbol{W})\)を微分した値を\((j,i)\)成分にもつ行列</li>
<li>\(\partial \boldsymbol{b}^{(l)} \)：\((l)\)層の重み\(b_{j}^{(l)}\)で誤差関数\(E=\sum_{n=1}^N E_n(\boldsymbol{W})\)を微分した値を\((j)\)成分にもつ列ベクトル</li>
<li>\(\epsilon\)：学習率</li>
</ul>

<p>\begin{eqnarray}
\partial \boldsymbol{W}^{(l)} &amp;=&amp; \frac{1}{N} \boldsymbol{\Delta^{(l)}} \boldsymbol{Z^{(l-1)T}} \tag{4}\\
\partial \boldsymbol{b}^{(l)} &amp;=&amp; \frac{1}{N} \boldsymbol{\Delta^{(l)}} \boldsymbol{1}_N \tag{5}
\end{eqnarray}</p>

<ul>
<li>\(\partial \boldsymbol{W}^{(l)}\)：\(j \times i\)</li>
<li>\(\partial \boldsymbol{b}^{(l)}\)：\(j \times 1\)</li>
<li>\(\boldsymbol{Z^{(l-1)}} \)：\(i \times N\)</li>
<li>\(\boldsymbol{\Delta^{(l)}} \)：\(j \times N\)</li>
<li>\(j\)：\((l)\)層のユニット数（バイアス除く）</li>
<li>\(i\)：\((l-1)\)層のユニット数（バイアス除く）</li>
<li>\(N\)：ミニバッチのデータ数</li>
</ul>

<p>更新式は，</p>

<p>\begin{eqnarray}
\boldsymbol{W}^{(l)} &amp;\leftarrow&amp;
\boldsymbol{W}^{(l)}
- \epsilon \partial \boldsymbol{W}^{(l)} \tag{6} \\ 
\boldsymbol{b}^{(l)} &amp;\leftarrow&amp;
\boldsymbol{b}^{(l)}
- \epsilon \partial \boldsymbol{b}^{(l)} \tag{7} \\ 
\end{eqnarray}</p>

<p>以上です．</p>

        </div>
    </div>
</div>

<div class="article">
    <div class="well">
        <h1><a href="/2015/11/blueDeepLearningChapter44">Nov 9, 2015 - 青深層学習 4.4.1 出力層でのデルタ</a></h1>
        
        <div class="content">
            <h1 id="はじめに">はじめに</h1>

<p>前回の続きです．</p>

<p>高校生で培った微分を駆使してがんばります．</p>

<h2 id="出力層のデルタ">出力層のデルタ</h2>

<p>出力層のデルタを求めます．
出力層のデルタは，誤差関数\(E_n\)を，出力層の入力 \(u_{j}^{(L)}\) で微分(式1)である．</p>

<p>なので，出力層の活性化関数と誤差関数によって計算が異なる（本書の例の場合結果は全て同じになる）</p>

<p>\begin{eqnarray}
\delta_j^{(L)} = 
  \frac{\partial E_n }{\partial u_{j}^{(L)}} \tag1
\end{eqnarray}</p>

<h1 id="回帰">回帰</h1>

<ul>
<li>活性化関数：恒等写像</li>
<li>誤差関数：2乗誤差</li>
</ul>

<p>2乗誤差は以下の式2であり，出力層が恒等写像なので，\(\boldsymbol{y}=\boldsymbol{z}=\boldsymbol{u}\)
より式4に変形できる．</p>

<p>\begin{eqnarray}
E_n &amp;=&amp; \frac{1}{2} ||\boldsymbol{y} - \boldsymbol{d} ||^2 \tag{2}\\
&amp;=&amp; \frac{1}{2}\sum_j (y_j-d_j)^2 \tag{3} \\
&amp;=&amp; \frac{1}{2}\sum_j (u_j^{(L)} - d_j)^2 \tag{4} \\
\end{eqnarray}</p>

<p>式4を\(u_j^{(L)}\)で微分すればいいので\(L\)層のデルタは，
\begin{eqnarray}
\delta_j^{(L)} &amp;=&amp; u_j^{(L)} - d_j \tag{5} \\
&amp;=&amp; y_j - d_j \tag{6}
\end{eqnarray}</p>

<h1 id="2値分類">2値分類</h1>

<ul>
<li>活性化関数：ロジスティック関数</li>
<li>誤差関数：対数尤度</li>
</ul>

<p>2値分類なので出力層のユニットは1つだけ．</p>

<p>対数尤度の総和の内側のデルタ（データ1つに対するデルタ）を求める．</p>

<p>式7で分子を対数尤度で展開し，式8はchain ruleを適用してyで展開する．</p>

<p>\begin{eqnarray}
\delta^{(L)} &amp;=&amp; \frac
{(\partial d \log(y) + (1-d)\log(1-y))}
{\partial u} \tag{7} \\
&amp;=&amp; \frac
{(\partial d \log(y) + (1-d)\log(1-y))}
{\partial y}
\frac{\partial y}
{\partial u} \tag{8} \\
\end{eqnarray}</p>

<p>式8の第1項目は，対数の微分，2項目は，出力層がロジスティック関数なので， \(y=\frac{1}{1+\exp(-u)}\) を\(u\)で微分する必要がある（高校でいうところの商の微分）．</p>

<p>\begin{eqnarray}
\delta^{(L)} &amp;=&amp; (\frac{d}{y} - \frac{1-d}{1-y}) y(1-y) \tag{9}  \\
&amp;=&amp; d(1-y)-(1-d)y \tag{10} \\
&amp;=&amp; d-y \tag{11} \\
\end{eqnarray}</p>

<p>ロジスティック関数をちゃんと微分すると以下のようになる．</p>

<p>\begin{eqnarray}
\frac{\partial y} {\partial u} &amp;=&amp; \frac{\partial}{\partial u} \frac{1}{1+\exp(-u)} \\
&amp;=&amp; \frac{(-1)(1+\exp(-u))(-1)}{(1+\exp(-u))^2}\\
&amp;=&amp; \frac{1+\exp(-u)}{(1+\exp(-u))^2}\\
&amp;=&amp;(\frac{1}{1+\exp(-u)})(1-\frac{1}{1+\exp(-u)})\\
&amp;=&amp;y(1-y)
\end{eqnarray}</p>

<ul>
<li>合成関数の微分</li>
<li>商の微分</li>
</ul>

<p>って名前がついてたきがする．</p>

<h1 id="多値分類">多値分類</h1>

<p>（これが一番時間がかかった）</p>

<ul>
<li>活性化関数：ソフトマックス関数</li>
<li>誤差関数：交差エントロピー</li>
</ul>

<p>誤差関数をソフトマックス関数で展開すると式13になる．</p>

<p>\begin{eqnarray}
E_n &amp;=&amp; - \sum_k d_k \log(y_k) \tag{12} \\
&amp;=&amp; - \sum_k d_k \log (\frac{\exp(u_k^{(L)})}{\sum_i \exp(u_i^{(L)})}) \tag{13}
\end{eqnarray}</p>

<p>\(\delta^{(L)}\)を求めるには，chain ruleを使って\(y\)で展開する．</p>

<hr>

<p>\begin{eqnarray}
\delta^{(L)} &amp;=&amp; \sum_k \frac{\partial E_n}{\partial y_k} \frac{\partial y_k}{u_j^{(L)}} \tag{14} \\
&amp;=&amp; \sum_k (-1) \frac{d_k}{y_k} \frac{\partial }{u_j^{(L)}} \frac{\exp(u_k^{(L)})}{\sum_i \exp(u_i^{(L)})} \tag{15}\\
&amp;=&amp; 
- \frac{d_j}{y_j} \frac{\exp(u_j^{(L)}) \{ \sum_i \exp(u_i^{(L)})\} - \{\exp(u_j^{(L)})\}^2 }{\{ \sum_i \exp(u_i^{(L)})\}^2}
- \sum_{k \neq j} \frac{d_k}{y_k} \frac{-\{\exp(u_k^{(L)})\}\{\exp(u_j^{(L)})\}}{\{\sum_i \exp(u_i^{(L)})\}^2}
 \tag{16}\\
&amp;=&amp;
- d_j \frac{\sum_i \exp(u_i^{(L)}) - 
\exp(u_j^{(L)})
  }{\sum_i \exp(u_i^{(L)})}
+ \sum_{k \neq j} \frac{d_k}{y_k} y_k y_j
 \tag{17}\\
 &amp;=&amp;
- d_j + d_j y_j
+ \sum_{k \neq j} d_k y_j
 \tag{18}\\ 
 &amp;=&amp; - d_j + \sum_{k} d_k y_j \tag{19}\\
 &amp;=&amp; - d_j + y_j \tag{20}\\
\end{eqnarray}</p>

<ul>
<li>式15：第1項目はそのまま誤差関数を \(y_k\)で微分し，2項目はソフトマックスに展開する．</li>
<li>式16：\(k=j\)と\(k \neq j\)の2つで変わるので，2つに分解して商の微分をする．</li>
<li>式17,式18：ソフトマックス関数で約分して整理．</li>
<li>式19：第3項に第2項を合わせる．</li>
<li>式20：\(\sum_k d_k = 1\)</li>
</ul>

<h1 id="まとめ">まとめ</h1>

<p>全部出力と正解の差になる．</p>

        </div>
    </div>
</div>

<div class="article">
    <div class="well">
        <h1><a href="/2015/11/blueDeepLearningChapter43">Nov 9, 2015 - 青深層学習 4.3 多層ネットワークへの一般化</a></h1>
        
        <div class="content">
            <h1 id="はじめに">はじめに</h1>

<p>前回 <a href="http://nzw0301.github.io/2015/11/blueDeepLearningChapter42/" title="青深層学習，誤差逆伝播法の計算">青深層学習，誤差逆伝播法の計算</a>
の続きです．</p>

<p>誤差逆伝播を層が全体で3層だけの場合ではなく，\(L\)層に拡張した時の挙動を確認します．</p>

<p>前回と赤で強調する場所が違うかもしれませんが，ご了承ください．</p>

<h1 id="本題">本題</h1>

<p>第\(l\)層の重み\(w_{ji}^{(l)}\)で誤差関数を微分することを考える．</p>

<p>前回と同じくネットワークを示すと赤いリンクの重みで誤差関数を微分する．\(l+1\) 層にもバイアスはある気がするので，ここでは書いている．</p>

<p><img src="/images/nn431.svg" alt="nn"></p>

<p>前回の中間層の重みの微分と同じく，\(w_{ji}^{(l)}\)は\(l\)層のユニット\(j\)の総入力\(u_j^{(l)}\)の一部なので，chain ruleを使って展開する(式1)．</p>

<p>\begin{eqnarray}
\frac{\partial E_n }{\partial w_{ji}^{(l)}} &amp;=&amp;
\frac{\partial E_n }{\partial u_j^{(l)}} \frac{\partial u_j^{(l)} }{\partial w_{ji}^{(l)}} \tag{1}\\
\end{eqnarray}</p>

<p>式1の第1項\(\frac{\partial E_n }{\partial u_j^{(l)}}\)をまず展開する．
下ぼネットワークで示したように
\(u_j^{(l)}\)は，活性化関数\(f\)が適用され，
出力\(z_j^{(l)}\)になり，赤いリンクの重みがかかって\(l+1\)層のユニット((\(l+1\))層のバイアス以外のユニット)入力の一部になる．</p>

<p><img src="/images/nn432.svg" alt="nn"></p>

<p>なので，\(E_n\)を微分するために，chain ruleで展開する．</p>

<p>\begin{eqnarray}
\frac{\partial E_n }{\partial u_{j}^{(l)}} &amp;=&amp;
\sum_{k=1}^{K} \frac{\partial E_n }{\partial u_k^{(l+1)}} \frac{\partial u_k^{(l+1)} }{\partial u_j^{(l)}} \tag{2}\\
\end{eqnarray}</p>

<p>ここで式3のようにデルタを定義する．
デルタは各層の各ユニットに存在する．</p>

<p>\begin{eqnarray}
\delta_j^{(l)} \equiv 
  \frac{\partial E_n }{\partial u_{j}^{(l)}} \tag{3}
\end{eqnarray}</p>

<p>式2を式3を使った形で展開する．</p>

<p>式2の右辺を項ごとに展開する．
\(\frac{\partial E_n }{\partial u_k^{(l+1)}} \frac{\partial u_k^{(l+1)} }{\partial u_j^{(l)}}\)の1項目は，式3のデルタの層が違うだけなので，</p>

<p>\begin{eqnarray}
\frac{\partial E_n }{\partial u_k^{(l+1)}} = \delta_k^{(l+1)} \tag{4}
\end{eqnarray}</p>

<p>2項目 \(\frac{\partial u_k^{(l+1)} }{\partial u_j^{(l)}}\)は，前回展開やったように定義に戻って展開する．</p>

<p>\begin{eqnarray}
\frac{\partial u_k^{(l+1)} }{\partial u_{j}^{(l)}} &amp;=&amp; \frac{\partial \sum_{j&#39;} w_{kj&#39;}^{(l+1)} z_{j&#39;}^{(l)} }{\partial u_j^{(l)}} \tag{5} \\
&amp;=&amp; \frac{\partial \sum_{j&#39;} w_{kj&#39;}^{(l+1)} f(u_{j&#39;}^{(l)}) }{\partial u_j^{(l)}} \tag{6} \\
&amp;=&amp;  w_{kj}^{(l+1)} f&#39;(u_{j}^{(l)})  \tag{7} \\
\end{eqnarray}</p>

<p>よって式2に式4と式6を代入するとデルタの関係式が得られる．</p>

<p>\begin{eqnarray}
\delta_j^{(l)}  &amp;=&amp; \sum_{k=1}^{K} \delta_k^{(l+1)} (w_{kj}^{(l+1)} f&#39;(u_{j}^{(l)})) \tag{8}
\end{eqnarray}</p>

<p>これは\(l\)層のデルタは(\(l+1\))層の全デルタで計算されることを示す．
（くどいが，最終的に必要なデルタは式1の1項目であり，\(L\)層から(\(l+1\))層までの全デルタを順伝播とは逆向きに求める．これが逆伝播の由来らしい）</p>

<p>式1の2項目は簡単に求められる．（前回の中間層の重みと同じ）
\begin{eqnarray}
\frac{\partial u_j^{(l)}} {\partial w_{ji}^{(l)}} &amp;=&amp; \frac{\partial \sum_{i&#39;} w_{ji&#39;}^{(l)} z_{i&#39;}^{(l-1)} }{\partial w_{ji}^{(l)}} \tag{9}\\
  &amp;=&amp; z_i^{(l-1)} \tag{10}
\end{eqnarray}</p>

<p>式1に式8と式10を代入すれば任意の重みがデルタと\(z\)で表現できる．</p>

<p>\begin{eqnarray}
\frac{\partial E_n }{\partial w_{ji}^{(l)}} &amp;=&amp;
\delta_j^{(l)} z_i^{(l-1)} \tag{11}\\
\end{eqnarray}</p>

<p>式8の通り，デルタは漸化式の形をしている．</p>

<p>最初のデルタは第\(L\)層のデルタであり，
つまりは，誤差関数を出力層の入力\(u_j^{(L)}\)の微分である．</p>

        </div>
    </div>
</div>


<div class="pagination">
  
  <span class="page_number ">Page: 1 of 5</span>
  
    <a class="btn btn-default" href="/page2" class="next">Older</a>
  
</div>

        </div>
        
    </div>
</div>

<div class="container-fluid">
    <div class="row-fluid">
        <div class="span12 footer navbar-inverse navbar-fixed-bottom">
            <p class="copyright">&copy;2015 鴨川η. Powered by <a href="http://jekyllrb.com">Jekyll</a>, theme by <a href="https://github.com/scotte/jekyll-clean">Scott Emmons</a>
            under
            <a href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution</a></p>
        </div>
    </div>
</div>




  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-56934733-2', 'auto');
    ga('send', 'pageview');
  </script>



</body>
</html>

