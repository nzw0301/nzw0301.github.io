<!DOCTYPE html>
<html lang="ja">

<head>
    <meta charset="UTF-8">
    <title>η</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script src="/js/jquery.min.js"></script>
    <script src="/js/bootstrap.min.js"></script>
    <link href="/css/bootstrap.min.css" rel="stylesheet">
    <link href="/css/theme.css" rel="stylesheet">
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>

<body>

<div class="container-fluid">
    <div class="row-fluid">
        <div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                  <span class="sr-only">Toggle navigation</span>
                  <span class="icon-bar"></span>
                  <span class="icon-bar"></span>
                  <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand" href="/">η</a>
              </div>
              <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav">
                    <li class="active"><a href="/">Home</a></li>
                    <li class="active visible-xs-block"><a href="/links.html">Links</a></li>
                    <li class="active"><a href="/archive.html">Archive</a></li>
                    <li class="active"><a href="/about.html">About</a></li>
                    <li class="active"><a href="/memo.html">Memo</a></li>
                    <li class="active"><a href="/feed.xml">RSS</a></li>

                </ul>
              </div>
        </div>
    </div>
</div>


<div class="container container-left">
    <div class="row">
        <div class="col-md-3 hidden-xs">
            <div class="sidebar well">
η is ...
</div>

<div class="sidebar well">
    <h1>Recent Posts</h1>
    <ul>
        
          <li><a href="/2015/11/blueDeepLearningChapter44code">青深層学習 4章 実装</a></li>
        
          <li><a href="/2015/11/blueDeepLearningChapter442">青深層学習 4.4.2 行列表記</a></li>
        
          <li><a href="/2015/11/blueDeepLearningChapter44">青深層学習 4.4.1 出力層でのデルタ</a></li>
        
          <li><a href="/2015/11/blueDeepLearningChapter43">青深層学習 4.3 多層ネットワークへの一般化</a></li>
        
          <li><a href="/2015/11/blueDeepLearningChapter42">青深層学習，誤差逆伝播法の計算</a></li>
        
    </ul>
</div>

<div class="sidebar well">
<h1>Links</h1>
<ul>
  <li><a href="https://github.com/nzw0301">github</a></li>
  <li><a href="https://twitter.com/nozawa0301">Twitter</a></li>
  <li><a href="http://nzw.hatenablog.jp/">Hatena</a></li>
  <li><a href="http://wkblab.github.io/member/nzw.html">Research</a></li>
  <li><a href="http://www.amazon.co.jp/gp/registry/wishlist/?ie=UTF8&camp=1207&creative=8415&linkCode=shr&tag=algebrae-22&requiresSignIn=1">wish list</a></li>
</ul>

</div>

        </div>
        <div class="col-md-9">
            
<div class="article">
    <div class="well">
        <h1><a href="/2015/11/blueDeepLearningChapter44code">Nov 9, 2015 - 青深層学習 4章 実装</a></h1>
        
        <div class="content">
            <h1 id="はじめに">はじめに</h1>

<p>前回まで数式の展開をしていたので，それをもとにして実装を行いました．</p>

<p>Pythonを使いました．</p>

<h1 id="データとタスク">データとタスク</h1>

<p>データは例によって夢野久作の作品(<a href="http://www.aozora.gr.jp/cards/000096/files/2122_21847.html">オンチ</a>)を使います．</p>

<p>1文が登場人物の発言なのか，地の文なのかの2値分類を行います．
活性化関数はすべてロジスティック関数，損失関数は対数尤度です．
（なので出力層のユニット数は1つ）</p>

<blockquote>
<p>tag sentence</p>

<p>1 退け 退け ッ </p>

<p>0 疎ら に なっ た 群衆 の 背後 から 、 今 出 た ばかり の 旭 が キラキラ と 映し 込ん で 来 た 。 </p>
</blockquote>

<p>このように地の文であれば0，発言であれば1をつけます．</p>

<h1 id="コード">コード</h1>

<p><code>hidden_layer</code> で指定する層を変えても問題ないように作ってあるので層の増減とユニット数の増減は好きに試すことができます．</p>

<script src="https://gist.github.com/nzw0301/363b803268c2ece127f2.js"></script>

<p>テストデータは作ってないので，パラメータによる誤差関数の変動のグラフをみて投稿を締めようかと思います．</p>

<p>上記のコード
1つ目がミニバッチを使った確率的勾配法
2つ目が全データで学習する勾配法</p>

<p><img src="/images/nn_err_rate.svg" alt="nn"> </p>

<p>学習回数<code>400</code>，学習率<code>0.2</code>で固定し，隠れ層をいじってみます．
<img src="/images/nn_hidden.svg" alt="nn"> </p>

<p>学習率<code>0.2</code>と隠れ層を固定し，確率的勾配法を使います．
ミニバッチによる変化を見ています．
<img src="/images/nn_minibatch.svg" alt="nn"> </p>

<p>以上です．</p>

        </div>
    </div>
</div>

<div class="article">
    <div class="well">
        <h1><a href="/2015/11/blueDeepLearningChapter442">Nov 9, 2015 - 青深層学習 4.4.2 行列表記</a></h1>
        
        <div class="content">
            <h1 id="はじめに">はじめに</h1>

<p>前回の続きです．</p>

<p>順伝播，誤差逆伝播，重みとバイアスの更新を行列の表記で行います．</p>

<h1 id="表記">表記</h1>

<ul>
<li>\(N\) ：ミニバッチのデータ数</li>
<li>\(\boldsymbol{x}_n\)：1件のデータ(例えば1つ文書，行は単語の頻度とか)</li>
<li>\(\boldsymbol{X} = \left[ \begin{array}{c} \boldsymbol{x}_1 ... \boldsymbol{x}_N \end{array} \right]\)：ミニバッチの行列</li>
<li>\(\boldsymbol{W^{(l)}} = w_{ji}^{(l)} \)：\(l\)層のおける重みの行列 要素はユニット\(i\)からユニット\(j\)のリンクの重み</li>
<li>\(\boldsymbol{b}^{(l)}\)：\(l\)層のバイアスのベクトル</li>
<li>\(\boldsymbol{u}^{(l)}_n\)：データ\(\boldsymbol{x}_n\)のときの，\(l\)層の入力ベクトル 1行目はユニットの1つ目の入力に対応</li>
<li>\(\boldsymbol{U}^{(l)} = \left[ \begin{array}{c} \boldsymbol{u}_1^{(l)} ... \boldsymbol{u}_N^{(l)} \end{array} \right]\)：\(l\)層の入力の行列，列がデータ1件，行がユニットに対応</li>
<li>\(\boldsymbol{Z}^{(l)} = \left[ \begin{array}{c} \boldsymbol{z}_1^{(l)} ... \boldsymbol{z}_N^{(l)} \end{array} \right]\)：\(l\)層の出力の行列，列がデータ1件，行がユニットに対応，\(\boldsymbol{U}^{(l)}\)の各要素に活性化を適用しただけ</li>
<li>\(\boldsymbol{Y} = \left[ \begin{array}{c} \boldsymbol{y}_1 ... \boldsymbol{y}_N \end{array} \right]\)：各データに対する最終的な出力を列にもつ行列</li>
<li>\(\boldsymbol{\Delta^{(l)}}\)：列がミニバッチのデータ，行がユニットのデルタ\(\delta^{(l)}_j\)の行列</li>
<li>k：出力層のユニット数</li>
</ul>

<h1 id="順伝播">順伝播</h1>

<p>入力は恒等写像なので， \(\boldsymbol{Z}^{(1)}=\boldsymbol{X}\) ．</p>

<p>\begin{eqnarray}
\boldsymbol{U}^{(l)} &amp;=&amp;\boldsymbol{W^{(l)}} \boldsymbol{Z^{(l-1)}} + \boldsymbol{b}^{(l)} \boldsymbol{1}_N^{T} \tag{1} \\
\boldsymbol{Z}^{(l)} &amp;=&amp; f^{(l)}(\boldsymbol{U}^{(l)}) \tag{2}
\end{eqnarray}</p>

<ul>
<li>\(j\)：\((l)\)層のユニット数（バイアス除く）</li>
<li>\(i\)：\((l-1)\)層のユニット数（バイアス除く）</li>
<li>\(\boldsymbol{W}^{(l)}\)：\(j \times i\)</li>
<li>\(\boldsymbol{U}^{(l)}\)：\(j \times N\)</li>
<li>\(\boldsymbol{Z}^{(l)}\)：\(j \times N\)</li>
<li>\(\boldsymbol{b}^{(l)}\)：\(j \times 1\)</li>
<li>\(\boldsymbol{1}_N\)：\(1 \times N\)</li>
</ul>

<h1 id="逆伝播">逆伝播</h1>

<p>出力層のデルタ\(\boldsymbol{\Delta^{(L)}} = \boldsymbol{D} - \boldsymbol{Y}\)
を\((L-1)\)~\(2\)まで逆伝播させる．</p>

<p>\begin{eqnarray}
\boldsymbol{\Delta}^{(l)} = f&#39;^{(l)}(\boldsymbol{U}^{(l)}) \odot (\boldsymbol{W}^{(l+1)T} \boldsymbol{\Delta}^{(l+1)}) \tag{3}
\end{eqnarray}</p>

<ul>
<li>\(\boldsymbol{\Delta^{(L)}} , \boldsymbol{D} , \boldsymbol{Y}\)：\(k \times N\)</li>
<li>\(\boldsymbol{\Delta^{(l)}} \)：\(j \times N\)</li>
<li>\(\odot\)：<a href="https://ja.wikipedia.org/wiki/%E3%82%A2%E3%83%80%E3%83%9E%E3%83%BC%E3%83%AB%E7%A9%8D" title="アダマール積">アダマール積</a></li>
</ul>

<h1 id="重み更新">重み更新</h1>

<p>逆伝播で計算したデルタを使って微分し，重み\(\boldsymbol{W}\)を更新．
誤差逆伝播が漸化式で求めたので，重みの更新は並列して計算ができる．</p>

<ul>
<li>\(\partial \boldsymbol{W}^{(l)} \)：\((l)\)層の重み\(w_{ji}^{(l)}\)で誤差関数\(E=\sum_{n=1}^N E_n(\boldsymbol{W})\)を微分した値を\((j,i)\)成分にもつ行列</li>
<li>\(\partial \boldsymbol{b}^{(l)} \)：\((l)\)層の重み\(b_{j}^{(l)}\)で誤差関数\(E=\sum_{n=1}^N E_n(\boldsymbol{W})\)を微分した値を\((j)\)成分にもつ列ベクトル</li>
<li>\(\epsilon\)：学習率</li>
</ul>

<p>\begin{eqnarray}
\partial \boldsymbol{W}^{(l)} &amp;=&amp; \frac{1}{N} \boldsymbol{\Delta^{(l)}} \boldsymbol{Z^{(l-1)T}} \tag{4}\\
\partial \boldsymbol{b}^{(l)} &amp;=&amp; \frac{1}{N} \boldsymbol{\Delta^{(l)}} \boldsymbol{1}_N \tag{5}
\end{eqnarray}</p>

<ul>
<li>\(\partial \boldsymbol{W}^{(l)}\)：\(j \times i\)</li>
<li>\(\partial \boldsymbol{b}^{(l)}\)：\(j \times 1\)</li>
<li>\(\boldsymbol{Z^{(l-1)}} \)：\(i \times N\)</li>
<li>\(\boldsymbol{\Delta^{(l)}} \)：\(j \times N\)</li>
<li>\(j\)：\((l)\)層のユニット数（バイアス除く）</li>
<li>\(i\)：\((l-1)\)層のユニット数（バイアス除く）</li>
<li>\(N\)：ミニバッチのデータ数</li>
</ul>

<p>更新式は，</p>

<p>\begin{eqnarray}
\boldsymbol{W}^{(l)} &amp;\leftarrow&amp;
\boldsymbol{W}^{(l)}
- \epsilon \partial \boldsymbol{W}^{(l)} \tag{6} \\ 
\boldsymbol{b}^{(l)} &amp;\leftarrow&amp;
\boldsymbol{b}^{(l)}
- \epsilon \partial \boldsymbol{b}^{(l)} \tag{7} \\ 
\end{eqnarray}</p>

<p>以上です．</p>

        </div>
    </div>
</div>

<div class="article">
    <div class="well">
        <h1><a href="/2015/11/blueDeepLearningChapter44">Nov 9, 2015 - 青深層学習 4.4.1 出力層でのデルタ</a></h1>
        
        <div class="content">
            <h1 id="はじめに">はじめに</h1>

<p>前回の続きです．</p>

<p>高校生で培った微分を駆使してがんばります．</p>

<h2 id="出力層のデルタ">出力層のデルタ</h2>

<p>出力層のデルタを求めます．
出力層のデルタは，誤差関数\(E_n\)を，出力層の入力 \(u_{j}^{(L)}\) で微分(式1)である．</p>

<p>なので，出力層の活性化関数と誤差関数によって計算が異なる（本書の例の場合結果は全て同じになる）</p>

<p>\begin{eqnarray}
\delta_j^{(L)} = 
  \frac{\partial E_n }{\partial u_{j}^{(L)}} \tag1
\end{eqnarray}</p>

<h1 id="回帰">回帰</h1>

<ul>
<li>活性化関数：恒等写像</li>
<li>誤差関数：2乗誤差</li>
</ul>

<p>2乗誤差は以下の式2であり，出力層が恒等写像なので，\(\boldsymbol{y}=\boldsymbol{z}=\boldsymbol{u}\)
より式4に変形できる．</p>

<p>\begin{eqnarray}
E_n &amp;=&amp; \frac{1}{2} ||\boldsymbol{y} - \boldsymbol{d} ||^2 \tag{2}\\
&amp;=&amp; \frac{1}{2}\sum_j (y_j-d_j)^2 \tag{3} \\
&amp;=&amp; \frac{1}{2}\sum_j (u_j^{(L)} - d_j)^2 \tag{4} \\
\end{eqnarray}</p>

<p>式4を\(u_j^{(L)}\)で微分すればいいので\(L\)層のデルタは，
\begin{eqnarray}
\delta_j^{(L)} &amp;=&amp; u_j^{(L)} - d_j \tag{5} \\
&amp;=&amp; y_j - d_j \tag{6}
\end{eqnarray}</p>

<h1 id="2値分類">2値分類</h1>

<ul>
<li>活性化関数：ロジスティック関数</li>
<li>誤差関数：対数尤度</li>
</ul>

<p>2値分類なので出力層のユニットは1つだけ．</p>

<p>対数尤度の総和の内側のデルタ（データ1つに対するデルタ）を求める．</p>

<p>式7で分子を対数尤度で展開し，式8はchain ruleを適用してyで展開する．</p>

<p>\begin{eqnarray}
\delta^{(L)} &amp;=&amp; \frac
{(\partial d \log(y) + (1-d)\log(1-y))}
{\partial u} \tag{7} \\
&amp;=&amp; \frac
{(\partial d \log(y) + (1-d)\log(1-y))}
{\partial y}
\frac{\partial y}
{\partial u} \tag{8} \\
\end{eqnarray}</p>

<p>式8の第1項目は，対数の微分，2項目は，出力層がロジスティック関数なので， \(y=\frac{1}{1+\exp(-u)}\) を\(u\)で微分する必要がある（高校でいうところの商の微分）．</p>

<p>\begin{eqnarray}
\delta^{(L)} &amp;=&amp; (\frac{d}{y} - \frac{1-d}{1-y}) y(1-y) \tag{9}  \\
&amp;=&amp; d(1-y)-(1-d)y \tag{10} \\
&amp;=&amp; d-y \tag{11} \\
\end{eqnarray}</p>

<p>ロジスティック関数をちゃんと微分すると以下のようになる．</p>

<p>\begin{eqnarray}
\frac{\partial y} {\partial u} &amp;=&amp; \frac{\partial}{\partial u} \frac{1}{1+\exp(-u)} \\
&amp;=&amp; \frac{(-1)(1+\exp(-u))(-1)}{(1+\exp(-u))^2}\\
&amp;=&amp; \frac{1+\exp(-u)}{(1+\exp(-u))^2}\\
&amp;=&amp;(\frac{1}{1+\exp(-u)})(1-\frac{1}{1+\exp(-u)})\\
&amp;=&amp;y(1-y)
\end{eqnarray}</p>

<ul>
<li>合成関数の微分</li>
<li>商の微分</li>
</ul>

<p>って名前がついてたきがする．</p>

<h1 id="多値分類">多値分類</h1>

<p>（これが一番時間がかかった）</p>

<ul>
<li>活性化関数：ソフトマックス関数</li>
<li>誤差関数：交差エントロピー</li>
</ul>

<p>誤差関数をソフトマックス関数で展開すると式13になる．</p>

<p>\begin{eqnarray}
E_n &amp;=&amp; - \sum_k d_k \log(y_k) \tag{12} \\
&amp;=&amp; - \sum_k d_k \log (\frac{\exp(u_k^{(L)})}{\sum_i \exp(u_i^{(L)})}) \tag{13}
\end{eqnarray}</p>

<p>\(\delta^{(L)}\)を求めるには，chain ruleを使って\(y\)で展開する．</p>

<hr>

<p>\begin{eqnarray}
\delta^{(L)} &amp;=&amp; \sum_k \frac{\partial E_n}{\partial y_k} \frac{\partial y_k}{u_j^{(L)}} \tag{14} \\
&amp;=&amp; \sum_k (-1) \frac{d_k}{y_k} \frac{\partial }{u_j^{(L)}} \frac{\exp(u_k^{(L)})}{\sum_i \exp(u_i^{(L)})} \tag{15}\\
&amp;=&amp; 
- \frac{d_j}{y_j} \frac{\exp(u_j^{(L)}) \{ \sum_i \exp(u_i^{(L)})\} - \{\exp(u_j^{(L)})\}^2 }{\{ \sum_i \exp(u_i^{(L)})\}^2}
- \sum_{k \neq j} \frac{d_k}{y_k} \frac{-\{\exp(u_k^{(L)})\}\{\exp(u_j^{(L)})\}}{\{\sum_i \exp(u_i^{(L)})\}^2}
 \tag{16}\\
&amp;=&amp;
- d_j \frac{\sum_i \exp(u_i^{(L)}) - 
\exp(u_j^{(L)})
  }{\sum_i \exp(u_i^{(L)})}
+ \sum_{k \neq j} \frac{d_k}{y_k} y_k y_j
 \tag{17}\\
 &amp;=&amp;
- d_j + d_j y_j
+ \sum_{k \neq j} d_k y_j
 \tag{18}\\ 
 &amp;=&amp; - d_j + \sum_{k} d_k y_j \tag{19}\\
 &amp;=&amp; - d_j + y_j \tag{20}\\
\end{eqnarray}</p>

<ul>
<li>式15：第1項目はそのまま誤差関数を \(y_k\)で微分し，2項目はソフトマックスに展開する．</li>
<li>式16：\(k=j\)と\(k \neq j\)の2つで変わるので，2つに分解して商の微分をする．</li>
<li>式17,式18：ソフトマックス関数で約分して整理．</li>
<li>式19：第3項に第2項を合わせる．</li>
<li>式20：\(\sum_k d_k = 1\)</li>
</ul>

<h1 id="まとめ">まとめ</h1>

<p>全部出力と正解の差になる．</p>

        </div>
    </div>
</div>


<div class="pagination">
  
  <span class="page_number ">Page: 1 of 5</span>
  
    <a class="btn btn-default" href="/page2" class="next">Older</a>
  
</div>

        </div>
        
    </div>
</div>

<div class="container-fluid">
    <div class="row-fluid">
        <div class="span12 footer navbar-inverse navbar-fixed-bottom">
            <p class="copyright">&copy;2015 η. Powered by <a href="http://jekyllrb.com">Jekyll</a>, theme by <a href="https://github.com/scotte/jekyll-clean">Scott Emmons</a>
            under
            <a href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution</a></p>
        </div>
    </div>
</div>




  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-56934733-2', 'auto');
    ga('send', 'pageview');
  </script>



</body>
</html>

