---
layout: post
title: "Treasure Data Summer Internship 2017 #td_intern"
comments: false
abstract: 参加記録
---

### はじめに

2017年の8月から2ヶ月間 [Treasure Data Inc.](https://www.treasuredata.com/)の高待遇で有名なサマーインターンシップに参加しました (日給2万)．
例年はインターン生は3名なのですが，今回は私1人でした．

最終発表のスライド:

<script async class="speakerdeck-embed" data-id="0aef2ee4beab45888239295d9bbeede2" data-ratio="1.33333333333333" src="//speakerdeck.com/assets/embed.js"></script>

---

実は2年前にも応募していました．

<blockquote class="twitter-tweet" data-lang="en">
<p lang="ja" dir="ltr">
Treasure Data - ソフトウェアエンジニア (Intern) やばい
<a href="https://t.co/HHjIjyw3fh">https://t.co/HHjIjyw3fh</a>
</p>&mdash; nzw (@nzw0301)
<a href="https://twitter.com/nzw0301/status/599511496545042432">May 16, 2015</a>
</blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

このあとCTOからリプライがきたので応募しました．

当時は，[Treasure Data Summer Intern 2015, myui's memo](http://myui.hateblo.jp/entry/2015/10/06/Treasure_Data_Summer_Intern_2015)にある課題を考えもせず，いきなりコーディングをしてテンパりました．
後から成果発表が英語だったと聞いて，行けなくて当然だという気持ちになりましたが，2年もするとなんとかなるようです．

### 準備

去年までのブログを読みました:

- [Treasure Dataインターンにみる機械学習のリアル #td_intern](https://takuti.me/note/td-intern-2016/)
- [Treasure Data インターンで最高の夏過ごしてきた #td_intern](http://ganmacs.hatenablog.com/entry/2016/10/07/094427)
- [トレジャーデータでインターンしてた話 #td_intern](http://amaya382.hatenablog.jp/entry/2016/10/01/210752)
- [Treasure Data 2015サマーインターンに参加した](https://qiita.com/NeokiStones/items/dde03c52623d4e657f46)
- [Treasure Data Summer Intern 2015](http://myui.hateblo.jp/entry/2015/10/06/Treasure_Data_Summer_Intern_2015)

#### レジュメ

前はJIS規格の履歴書を出したのですが，

1. 当時よりも倍率が高そう（これはそうでもなかったらしい）
2. 成果発表は英語

ということで，D進する予定もあり，英語で書きました．
[こんな感じ](https://www.dropbox.com/s/wve1sk9rdh90yw8/main.pdf?dl=0)です．
上の方に数行でどんなことをしたいか (LDAの高速なアルゴリズムをhivemallで実装したい的なこと) を書きました．

#### 面接

今回のコーディング課題が個人的に馴染みがあったので，運もよかったと思います．
正しい出力はできましたが，ロジックが美しくなかったので受かったらいいなという印象でしたが，
無事通過の連絡をいただきました．

#### インターン前

メンターの[myui](https://twitter.com/myui)さんから実装予定のアルゴリズムとJavaの勉強をしておくとスムーズになる旨を伺ったので，
[パーフェクト Java](https://www.amazon.co.jp/dp/B00V2WMQNE/ref=dp-kindle-redirect?_encoding=UTF8&btkr=1)読んで[skip-gram with negative samplingを実装](https://github.com/nzw0301/word2vec4j)しました．

### やったこと

[myui](https://twitter.com/myui)さんと[takuti](https://twitter.com/takuti)さんの元で[Hivemall](https://hivemall.incubator.apache.org/)の機械学習アルゴリズムを実装しました．
Hivemallは[Hive](https://hive.apache.org/)で使える関数の集合（特に機械学習のアルゴリズム）を提供しています．
HiveというのはHadoopに格納されているデータをHiveQLというSQLライクな問い合わせ言語を使って操作するものです．

今回は，過去ブログにあるような中間報告などは特になく，適宜メンターと相談して進めていました．
最終発表は，↑のスライドを使って英語で発表しました．
過去の自分も英語と聞いて後ずさりしてしましたが，
とりあえずそれらしい風に話せればなんとかなりました．たぶん．
ありがとうDMM英会話．

ちなみになんで英語かというと英語圏のエンジニアがいることが理由なので，場合によっては日本語かもしれません．

#### FMeasure

HiveのUDFやHivemallのお作法に慣れることを含めて，
最初にf1-scoreのを一般化した[fmeasure](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html)のUDAFを実装しました ([PR](https://github.com/apache/incubator-hivemall/pull/107))．

これが2週間くらいです．

#### SLIM

確実に成果が見込めるタスクとして[SLIM](http://glaros.dtc.umn.edu/gkhome/node/774)という行列分解系の推薦アルゴリズムを実装しました．
詳しい解説は[こちら](https://takuti.me/note/slim/)を．
これが一番大変でした．

理由としては

- 行列分解系の実装経験がなかった
- 反復計算処理
- 参考にしていた評価方法が謎

です．

予定だと2週間だったのですが，1か月かかりました．

#### word2vec: Skip-Gram & CBoW with negative sampling

これが挑戦的な課題でした．
予定が1か月でしたが，SLIMが長引いたので2週間くらい．
とりあえず形にはなったのでよかったです ([PR](https://github.com/apache/incubator-hivemall/pull/116))．

[word2vec](github.com/tmikolov/word2vec)の分散学習や高速化は，いくつか実装や論文があります．
オリジナル実装は，シングルノードのスレッド並列で[hogwild!](https://people.eecs.berkeley.edu/~brecht/papers/hogwildTR.pdf)を使っています．
Hivemallはマルチノードで動くので，word2vecをマルチノードで学習するのが最終的な課題です．

- [Spark mllib](https://spark.apache.org/docs/2.1.1/mllib-feature-extraction.html#word2vec)や[deeplearning4j](https://deeplearning4j.org/word2vec.html)ではグローバルに単語ベクトルを持っており，定期的に分散したノードのもつベクトルと同期させます．これだとノードごとにデータとベクトルをもつ必要があるので，スケールしません．
- [Network-Efficient Distributed Word2vec Training System for Large Vocabularies](https://arxiv.org/pdf/1606.08495.pdf)では，N分割した単語ベクトルをノードが担当することで同期をなくしています．例えば200次元のベクトルを10ノードで扱う場合，最初のノードが0--19, 次のノード20--39という感じです．これだと最大で次元数まで分散可能です．word2vecではnegative samplingは乱数依存なので，
異なるノードで同じ負例をサンプルするために，乱数のシード値をノードに送ることで異なるノードでも同じ乱数が使えます．賢い．．．
- [Parallelizing Word2Vec in Shared and Distributed Memory](https://pdfs.semanticscholar.org/cced/c38f68ffaf51cf8c31cd6c6b5c2cf033f91a.pdf)では行列演算にすることで高速化を図っています．オリジナルの実装では，for文使ってベクトルの内積計算をしていますが，blasが使えるなら行列演算のほうが高速です．このため，同じ文脈語（正例）をもつ単語（skip-gramの入力）に対しては同じ負例を使うことで行列積にします．分散学習はspark mllibと似たようなやり方です．
- [Distributed Negative Sampling for Word Embeddings](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14956)が知ってる範囲では最速のアルゴリズムです．この手法では，1単語に対する正例ごとに分散処理します．問題はどのノードに分配するかです．話が前後しますが，オリジナル実装では，語彙数よりもかなり長い配列を用意し，配列の要素の割合がnegative samplingをとってくるnoise distributionと同じ割合になるように要素に負例を格納します．この手法では，まずこの長い配列は使わずにalias methodのテーブルを作ります．このテーブルを分割し，担当するノードに分配しておきます．
1単語に対する正例を1つ決めたら，ノードがもつalias tableの割合と乱数をもとに担当するノードを選択し，単語対をノードに送ります．
ノードの内部がもつalias methodのテーブルの一部を使ってnegative samplingします．最後にパラメータサーバに勾配を送ってベクトルの更新をします．

LDAでalias methodを使ってcollapsed Gibbs samplingを高速にする手法を知っていたので，
word2vecでも使えそうだと思っていたのですが，
最後の手法についてを読んでみるとword2vecにすぐに応用できるということがわかりました（categorical分布からのサンプルになるので当然でした）．

本題です．
今回のインターンでは，データ並列でパラメータを共有せずにやってみました．
結果からいうとデータ数に対して並列数が上がるとベクトル間のcosine similarityが高くなり，
word similarityやanalogyの性能が悪くなります．
このあたりはhivemallのパラメータサーバあたりを使って解決を試みることが今後の課題です．

さて工夫したところですが，並列化のために，単語を予め整数値に変換します．
この変換された整数値を単語ベクトルの重みの初期化に使う乱数生成器のシードに使うことで異なるノードにおいても全く同じ初期化がされます．
これは[[Network-Efficient Distributed Word2vec Training System for Large Vocabularies](https://arxiv.org/pdf/1606.08495.pdf)]から着想を得ました．
これで精度の劣化 (順位相関係数) を多少防ぐことができました．
変換せずに学習できますが，シードが異なると，劣化が激しくなります．

またオリジナルのようにnegative samplingのために語彙数よりも長い配列を使うのは，
hivemallの1ノードあたりのメモリを圧迫するので[[Distributed Negative Sampling for Word Embeddings](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14956)]と同じようにalias methodを使いました．
Alias methodのためのテーブルを作る関数はUDTFとして作ったので，別用途でも使えます．

これ以外にオリジナル実装と違う点は，
1. sub-samplingは予め行っておく (本来は学習中に行う)
2. 反復計算は，文書ごとに行う（本来は文書全体）．これをしても性能が落ちなかったので意外でした．オリジナルどおりにやろうとすると予め冗長なテーブルを作るか，一時ファイルにデータを書き出すため，時間がかかります．

全然関係ないですが．参考にしていたspark mllibで複数イテレーションにするとiterationごとに学習率がリセットされます．
せっかくなのでPR投げました．

### Feature work

心残りなことは，以下の通りです．

- データ並列で学習したベクトルの品質向上
  - e.g. [parameter server](https://github.com/apache/incubator-hivemall/tree/master/mixserv/src/main/java/hivemall/mix)を使う
  - Learing rateはadagradとかadadeltaのような学習率の更新にデータ数が寄与しない最適化を利用
- 今回実装した学習アルゴリズムの比較
  - 利用するときの指針としてHivemallのアルゴリズム同士や他の実装との比較はとても重要

### 感想

これまで分散機械学習をするのは`sklearn`で`fit(..., jobs=-1)` するくらいだったので，貴重な開発経験になりました．

大学の研究室やバイト先と大きく違うと感じたところはメンターがすぐ近くにいて相談できることとメンターも見える範囲でバリバリコーディングをしていることです．これは大変刺激になりました．
（両方とも人によっては違うかもしれませんが）

外資系の会社なので，社内で英語が聞こえてきます．英語頑張ります．

### 最後に

実装の相談やコメントをしてくださった
[myui](https://twitter.com/myui)さんと[takuti](https://twitter/com/takuti)さん，エンジニアやスタッフのみなさま，貴重な機会をどうもありがとうございました．

進学予定にもかかわらずインターンとして受けて入れてくれたことは大変ありがたかったです．
また8月の週で院試があり，1週間ほど休みをいただくことを配慮していただきました．

あと，インターン中にオフィスが丸の内ビルから丸の内北口ビルに引っ越しました．
新しいオフィスは広くて，前のオフィスと同様に地下から出勤できるので快適です．
オフィスの引越しも人生初めてだったので珍しい体験でした．


残りは最終日に食べた[丸の内ビル36Fのフレンチ](http://breezeoftokyo.com/)と打ち上げの火鍋です．

前菜．バラみたいになってる生ハムがめっちゃうまかった．
![alt text](https://www.dropbox.com/s/fmht3yah3bi6ldt/IMG_0450.jpg?raw=1 "Appetizer")

高級になると泡が載ってくる．
![alt text](https://www.dropbox.com/s/77v6wut3bm7s9d6/IMG_0452.jpg?raw=1 "soup")

人生で一番美味しかったカツオ．下に🍆がひいてある．
![alt text](https://www.dropbox.com/s/1a15ohlfdtvc31c/IMG_0454.jpg?raw=1 "fish")

ステーキみたいな鶏肉．黒い粉はコーヒー．
![alt text](https://www.dropbox.com/s/bfi9qgsav00j028/IMG_0456.jpg?raw=1 "meat")

食べ方に戸惑うデザート．美味しかったです．
![alt text](https://www.dropbox.com/s/5ywkq1qbe8ndzee/IMG_0458.jpg?raw=1 "dessert")

打ち上げの火鍋
![alt text](https://www.dropbox.com/s/g0qb6fbfypv43q0/IMG_0466.jpg?raw=1 "pod1")

フカヒレ
![alt text](https://www.dropbox.com/s/dguofx9cke6tgwv/IMG_0462.jpg?raw=1 "pod0")

以上，気分は[丸の内OLだったnzw0301](https://twitter.com/nzw0301)でした．
