---
layout: post
title: "Logical features in distributional word representations"
date: 2016-03-28 22:32:04 +0900
comments: false
---

#### メタデータ
T. Linzen et al. の[論文](http://tallinzen.net/media/papers/linzen_dupoux_spector_under_review.pdf)．

Twitterで著者がリンクを貼っていたので読んだ．
現在査読中の論文．

単語の並びだけじゃなくてもっと文脈のリッチな情報（構造など）を使って学習したほうがいいかもねという結論．

#### 本題

やったこととしては以下の3つ

1. 分散表現の評価に論理的表現を使ったanalogyを提案
  （ _everything_ や _nothing_ などを指す）
1. クラウドソーシングで1.と同じタスクを人に解かせて比較
1. 1.のタスクに関して，データの規模を変化させて性能比較

モデルは _skip-gram_ だけを使い， Levyらの[hyperwords](https://bitbucket.org/omerlevy/hyperwords)を使ってパラメータの調節を行う．

##### 1. analogy

既存研究で言われているパラメータで学習したモデルに対して，全180通りを使ってanalogyのタスクを解かせて性能の評価．
完全一致ではなく上位100単語に含まれているか否かで評価．
また，analogyに現れる項の類義語は除外．

著者によると

$$argmax_{x} cos(a^{*}-a+b, x)$$

の演算結果は，88.9%が $$b$$ に最も近く， 11%が $$a^{*}$$ が最も近い．
というわけでベースラインとして， $$b$$ の近傍の単語を類推結果とする．
（これは 前者が $$a^{*}-a \approx 0$$ で後者が $$-a+b \approx 0$$ ということになってるのかなぁという印象を受けた）

結果としては， _domain_ （ _*where_ とか _*body_ とか）によって高低差が見られた．
3CosAddより3CosMulのほうがよかったが，Mikolovらのanalogyの結果と比べると低め．

意味的なものは _window size_ が小さくても獲得できていると言われているらしいが，このような表現では，ある程度長距離の依存があるので， _window size_ が大きいほうが性能がよかった．
（  _"not ... anywhere"_ のように対になっている場合， _window size_ が小さいと関係性が学習できないということらしい）

##### 2. クラウドソーシング

amazonのを使って同様のanalogyを解かせた．
全体的に _skip-gram_ よりも高い結果となった．
人間の場合，語形が使えるので有利のように見えるが，語形が使えない _domain_ でも高い結果であった．

##### 3. データの規模
データの規模を徐々に増やしていって，性能比較をする．

「子供が1年間に触れる単語数が〜」 という話もあるが本筋ではないので，論文に譲るとして，
実験結果のグラフでは，周囲の単語の位置を考慮するかどうかに注目している．
コーパスのサイズが十分でないときは，単語の位置は考慮しないほうがいいが，ある程度のサイズ（論文では180M words）になると考慮したほうが性能が良くなった．
（ある程度コーパスの規模がないと単語位置を考慮してしまうとスパースになるからではないかと思われる）
