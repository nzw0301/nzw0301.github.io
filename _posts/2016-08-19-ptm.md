---
layout: post
title: "Topic Modeling of Short Texts: A Pseudo-Document View"
date: 2016-08-20 22:25:00 +0900
comments: true
abstract: トピックモデルの論文紹介
---


### メタデータとか

Yuan Zuo, Junjie Wu, Hui Zhang, Hao Lin,  Fei Wang, Ke Xu, Hui Xiongの[論文](http://www.kdd.org/kdd2016/subtopic/view/topic-modeling-of-short-texts-a-pseudo-document-view)．
KDD2016に採択されている．


##### Background

トピックモデルの欠点の1つとして文書に含まれる単語数が少ないと共起情報が不足するため学習が上手くいかない点が挙げられる．
欠点を補う方法として文書以外の情報（以下補助情報）; TwitterであればTweet本文以外の情報 (hashtag, bio, locationなど) ，を使うことで共起情報を増やす手法がある．
この方法の欠点として以下の2つが挙げられる:

- 汎用的でない
  - 例えばLINEのショートメッセージは，Tweetのように発信場所がわからないということだと思われる
- 補助情報を用意する手間がかかる

別の解決策として補助情報を使わずにモデル自体を拡張が挙げられるが，モデルが複雑だとデータ数に対してスケールしないため現実的でない．

以上を踏まえて補助情報を使わず，かつデータ数に対してスケーラブルなモデルの提案を行っている．

##### Methods

提案法は，短い文書が $$D$$ 個にあるような状況を仮定する ( $$D$$ は大きい値)．
これらの複数の文書のくっつけた $$P$$ 個の擬似的な文書 ( _pseudo documents_ ) をつくる ( ただし $$P << D$$ )．
つまり短い文書を $$P$$ 個にハードクラスタリングし，そのクラスタに含まれる文書全体を1文書とみなして学習を行う．
よってGibbs samplingで学習を行う場合，1単語のサンプリング式の計算は$$\mathcal{O}(K)$$ であるのに対してこのモデルでは $$\mathcal{O}(PK)$$ となる．
このモデルを _Pseudo-Document-based Topic Model (PTM)_ として提案している．
実験では，文書が複数の擬似文書に属していいように制約を緩めた _Enhanced Pseudo-Document-based Topic Model (EPTM)_ との比較も行う．
サンプル式の導出は省略されているが，サンプル式は書かれているため，再実装も可能だと思われる．


PTMにおいて $$P$$ の値が小さい (トピック数と近くなる) とトピックと擬似文書が曖昧になるため，PTMの擬似文書のトピック分布に _Spike and Slab prior_ を導入した _Sparsified Pseudo-Document-based Topic Model (SPTM)_ も提案している．
ここでは _Spike and Slab prior_ を使うことでスパースとスムージングを分離することを考える．
PTMにおいて各擬似文書は $$K$$ 個のトピックをもつが，
SPTMでは，各擬似文書に対してそのトピックを使うか否かを決定するBernoulli分布を導入する (これがSpike)．
Bernoulli分布からサンプルしたバイナリ値をDirichlet分布の $$\alpha$$ の係数に使えばよさそうではあるが， $$\alpha$$ は正でないとけないためこれは不可．
なので

- [Wang and Blei [NIPS2009]](https://papers.nips.cc/paper/3835-decoupling-sparsity-and-smoothness-in-the-discrete-hierarchical-dirichlet-process.pdf) では，特殊な項を導入 (ただし計算が複雑)
- [Lin et.al. [WWW2014]](http://www-personal.umich.edu/~qmei/pub/www2014-lin.pdf) でSpikeで選択されなかったトピックについてのパラメータとして $$\alpha$$ とは別途に用意した微小な値を使う

といった対策が取られており，この論文では後者を採用している．
後者をもう少し具体的に説明すると，各擬似文書ごとの各トピックに対してBernoulli分布からサンプルした値が

- 1: Dirichlet分布のパラメータ $$\alpha$$ をつかう
- 0: $$\alpha$$ よりとても小さい値をつかう

という場合分けを使う手法になる．
1回のGibbs samplingでは $$K \times P$$ 回Bernoulli分布からサンプルすればよい．

##### Experiments

PTMの仮定を満たす文書集合として以下の4つを使う．
各文書にはラベルも付与されている．

- 新聞記事
- DBLP（論文データ）
- QAサイト
- Tweets

実験内容としては

1. 既存手法との比較
1. 得られたトピックを眺める
1. 擬似文書数の比較
1. 提案手法間での比較

の4つである．

評価指標は以下の2つ．

- UCI topic Coherence
  - 外部のコーパスが必要
  - UMass topic Coherenceは短文には不向きなので使わない
- F1 score
  - トピックモデルで得たトピックを使ってSVMで文書分類を行わせる
  - 5分割交差検証

LDAといえばPerplexityを見るのだが，今回はトピックのまとまりを見たいので使わない．

実験結果をざっくりとまとめると

- 既存手法との比較
  - 分類
    - 文書数が少ないとSPTM > PTM，多いと逆
    - 既存手法と比べると文書数が少ない場合でも学習がうまくいく
  - Coherence
    - 分類と同じく文書数が少ないとSPTM > PTM，多いと逆
- 擬似文書数 $$P$$
  - 多いほうがよい (比較では2000まで)
  - 少ないとSPTM > PTM，多いと逆
- 提案手法間での比較
  - EPTMが全負け



##### Feature works

ぱっと見ないので自分で思いつくものだけ


- ノンパラの適用
  - トピック数のみ
  - $$P$$ の値
- サンプル近似による高速化
  - 論文では周辺化ギブスサンプリングを使っているため

##### その他

ちょうど[勉強会](https://atnd.org/events/80771)があり，スライド資料も作りました．

<script async class="speakerdeck-embed" data-id="499d9ba0b8f247578bc9e5cf17c74877" data-ratio="1.33333333333333" src="//speakerdeck.com/assets/embed.js"></script>

----
