---
layout: post
title: "LINE: Large-scale Information Network Embedding"
comments: true
abstract: ノードの分散表現
---

### メタデータとか


Jian Tang et al.の[論文](http://dl.acm.org/citation.cfm?id=2741093)でWWW2015に通っている．

関連研究の[deepwalk](https://arxiv.org/abs/1403.6652)の後，[node2vec](http://snap.stanford.edu/node2vec/)の前に出たものになる．

### 本題

word2vec（skip-gram）で単語の分散表現を学習したようにノードについても分散表現を求めたい．

ノードが似ているというのは，以下の2つが考えられる:

1. 隣接ノード対
1. 直接隣接していないが隣接ノードが共通しているノード対

直接つながっているノード同士というのは例えると，仲のいい友だち（頻繁にreplyを送り合ってるユーザとか）で，後者は，直接面識はないが共通の知り合いがたくさんいる人同士となる．

類似研究であるdeepwalkではrandom walkを行っているため，深さ優先探索的であるが，提案手法（LINE）は幅優先探索的といえる．（node2vecではこれらのいいとこどりをしている）

#### 隣接ノード同士
ノード $$u_i$$ と $$u_j$$ のエッジ $$(i, j)$$ の同時確率を

$$p_1(u_i, u_j) = \frac{1}{1+exp(-\mathbf{u}_i^T \cdot \mathbf{u}_j )}$$

とする． $$\mathbf{u}_i$$ はノードの分散表現とする．
この確率分布 ( $$p_1(u_i,u_j)$$ は確率分布にはならないような気がするのだが) とempericalな確率分布

$$\hat{p}_1(u_i, u_j) = \frac{W_{i,j}}{\sum_{(i, j) \in E} W_{i,j}}$$

とのKLダイバージェンスを最小化するようにノードのベクトルを学習する．
ただし $$W_{i,j}$$ は $$u_i, u_j$$ 間の重み（非負）で $$E$$ はエッジの集合．

#### 1hop先

今度は条件付き確率で表現する．
こっちに文脈としてのノードが出てくる．

$$p_2(u_j|u_i) = \frac{exp(\mathbf{u}_j^c \cdot \mathbf{u}_i)} {\sum_k^V exp(\mathbf{u}_k^c \cdot \mathbf{u}_i)}$$

最小化するのはさきほどと同様にKLダイバージェンスになる．

$$\hat{p}_2(u_j| u_i) = \frac{W_{i,j}}{\sum_{k \in N(i)} W_{i,k}}$$

このとき $$N(i)$$ は $$u_i$$ の出リンクと接続したノードの集合．
KLダイバージェンスにノード $$u_i$$ の次数をかける．
PageRankでもよいらしい．
ここでKLダイバージェンスを求める際に，条件付き確率を計算するのはしんどいので，negative-samplingを使う．

2つの目的関数があるのだが，これらを同時に最適化するのはfuture workとなっており，論文では別々に学習したものを別々に使っている．
supervisedなケースだとconcatして重みを少し修正して使っている．

#### 実験

言語，SNSなどのグラフを使って分類性能とword analogyで評価をしている．
word analogyでは，1hop先をみるLINEがskip-gramよりも性能が良かった．

パラメータの感度分析もあるので使う際に参考になるかもしれない．

### その他


- [著者実装](https://github.com/tangjianpku/LINE)
- 個人的にはdeepwalkのほうがいい気がしているのだが，node2vecの実験結果をみるとどっちもどっちなので，パラメータやデータ次第かなという印象
