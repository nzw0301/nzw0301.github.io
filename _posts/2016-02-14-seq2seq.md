---
layout: post
title: "Sequence to Sequence Learning with Neural Networks"
date: 2016-02-14 16:45:00 +0900
comments: false
---

#### はじめに

対話文を生成する
[A Neural Conversational Model](http://arxiv.org/pdf/1506.05869.pdf)の実装がしたくなったんですが，そこで使われるSequence to Sequenceを知らなかったのでその紹介．

Sequence to Sequenceは，NIPS2014の[論文](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)で提案されているアーキテクチャ．

論文ではSequence to Sequenceという入出力が可変長な系列データを扱うためのネットワークの提案を英仏の翻訳のタスクで評価．
なので前回紹介したencoder–decoderとやりたいことは同じ．
例によって翻訳がわかってないので，モデルの紹介だけ．

encoder–decoderと違うこととしては

- LSTMを2つくっつけて使う (LSTMを使うことで長期依存を取ろうとしている)
  - 2つというのは入力系列用のLSTMと出力系列用のLSTM
- 4層のLSTM
  - (nzw: 単層ならわかるが，4層の場合に入力系列の入れ方と中間のベクトルをどうやって作るのかわからない)
- 入力系列の順序を逆順にする
  - (nzw: interactiveにつかえないような)

#### 本題

モデルはencoder–decoderよりもさらにシンプル（にみえる）．


![model]({{ site.url }}/images/seq2seq.svg)

ただし，

- `ABC` : 入力系列 (ex: 英語の1文)
- `<EOS>` : 文末記号
- `WXYZ` : 出力系列 (ex: フランス語の1文)
- 出力層 : `softmax`

とする．

あと，翻訳先ともとでembeddingsは2つ存在する．
(nzw: 会話の場合はembeddinsが1つでいいかもしれない)

encoder部分とdecoder部分はそれぞれ4層のLSTMからできてるのでもう少し複雑になる．
4層のLSTMに系列いれるのがよくわかってません．



#### その他
入力系列を逆順に入れると翻訳の性能が上がるらしい．
つまり，1.ではなくて2.をいれる．

1. 月 が きれい です ね
2. ね です きれい が 月



#### 所感
モデルをもう少し詳しく書いてほしい．．．

encoder–decoder (というか翻訳) の時点で思ってたのは，この枠組が使えると翻訳以外でも会話，言い換え，QAあたりもできるので，汎用性が高い印象を持った． (ただし十分なデータ量がない気がしている) 

4層のLSTMの入力のあたりは関連研究を参考にして調べてみようかと思います．
