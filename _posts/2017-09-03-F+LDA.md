---
layout: post
title: "A Scalable Asynchronous Distributed Algorithm for Topic Modeling"
comments: false
abstract: トピックスケーラブルなCGSのアルゴリズム
---

### メタデータとか

Hsiang-Fu Yu et al.の[論文](http://dl.acm.org/citation.cfm?id=2741682)でWWW2015．

LightLDAと同じ年で同じ会議。

### 本題

collapsed Gibbs samplingのLDA並列計算せずに高速化アルゴリズムとしては、以下が挙げられる。

- SparseLDA, 2009
- AliasLDA, 2014
- LightLDA, 2015
- F+LDA, 2015
- warpLDA, 2016

SparseLDA以外は並列計算についても言及されている。

このうちF+LDAとwarpLDAについては知らなかったので、前者についてアルゴリズムを読んだ。
SparseLDAとF+LDAはLDAのCGSと等価であるのに対して、これら以外は棄却サンプリング（MH法）を使う。

#### 導入

F+LDAでは、トピック数を$$T$$としたときに、

- 初期化: $$O(T)$$
- 更新: $$O(\log T)$$
- サンプル: $$O(\log(T))$$

を満たすデータ構造である F+tree を使う。F+Treeは、全ノードが値をもつ完全2分木。
葉ノードに各トピックの（正規化されてない）確率値をもち、親ノードが子ノード値の和をもつ。
詳しい図は論文のFig. 1を見ていただければと思います。

#### サンプリング

LDAのトピックのサンプル式は

$$p(z_{i, d}=t|w_{i, d} = w) \approx \frac{(n_{t, d} + \alpha_{t})(n_{t, w} + \beta_{w})}{n_{t} + V \beta}$$

から計算できる。
ただし、

- $$z, t$$ トピックID
- $$i$$ 文書内のインデックス
- $$d$$ 文書ID
- $$w$$ 単語
- $$n_{t, d}$$ 文書$$d$$にトピック$$t$$が割り当てられている単語総数
- $$n_{t, w}$$ 単語$$w$$にトピック$$t$$が割り当てられている頻度
- $$n_t$$ トピック$$t$$が割り当てられている頻度
- $$\alpha, \beta$$ Dirichlet分布のハイパーパラメータ
- $$V$$ 語彙数 ($$\beta$$ を対象を仮定しているため)

とする。
以下、略記としてサンプル式の右辺を $$p(t)$$とする。

論文では、2通りのサンプリング方法を示している。

##### document-by-document

サンプル式を以下のように変形する。

\begin{eqnarray}
p(t) &=& \beta(\frac{n_{t,d} + \alpha_t}{n_t + V\beta})+ n_{t,w}(\frac{n_{t,d} + \alpha_t}{n_t + V\beta}) \\\
       &=& \beta q_t + r_t
\end{eqnarray}

ただし、

$$q_t = \frac{n_{t,d} + \alpha_t}{n_t + V \beta}$$

$$r_t = n_{t,w} q_t$$

とする。
$$q$$と$$r$$のどっちの項からサンプルするかを決めてから一方からトピックをサンプルする：
$$U(0, \sum(p_t))$$ からサンプルした$$u$$が$$\sum(r_t)$$よりも小さければ$$r$$からサンプルし、
そうでなければ、$$q$$からサンプルする。

次に2つのそれぞれのサンプルについて見ていく。

Collapsed Gibbs samplingでは1つの単語に対して

1. 割り当てられているトピック$$z_{i d}$$が関係する統計量から1減らす
2. $$p(t)$$からトピック $$z_{i, d}^{new}$$をサンプルする
3. 新しいトピック$$z_{i,d}^{new}$$が関係する統計量に1加える

という操作を行う。

まず $$q_t$$について。
この値は、全$$t$$に対して非ゼロになる。これは$$\alpha_t$$がDirichlet分布のパラメータであるため。
1で更新をする場合、$$q_{z_{i,j}}$$のみが更新対象なので、先程導入したF+Treeを使う。なので更新に関する計算量は$$O(\log(T))$$。
また、これは1つの文書の中では使いまわすことができる。
2ではF+Treeの性質から$$O(\log(T))$$でサンプル。
3では1と同様で更新があったトピックのみを更新。

次に$$r_t$$について。
これは$$n_{t,w}$$があるためにスパースになる。しかし、文書内の単語ごとに全体を計算し直す必要があるのでF+Treeは使えない。
このため、まず$$r_t$$の累積和の配列を計算する。
この計算量は$$O(|T_w|)$$ （単語$$w$$に割り当てられているトピックのユニーク数）。
累積和の配列からbinary searchでサンプルを行う。この計算量は$$O(\log(|T_w|))$$。

というわけでサンプルは常に対数オーダーになる。

##### word-by-word

document-by-documentと同じ手順になるが、分解方法を変える。
まず、サンプル式を以下のように分解する。

\begin{eqnarray}
p(t) &=& \alpha_t(\frac{n_{t,w} + \beta}{n_t + V\beta})+ n_{t,d}(\frac{n_{t,w} + \beta}{n_t + V\beta}) \\\
     &=& \alpha_t q_t + r_t
\end{eqnarray}

ただし、

$$q_t = \frac{n_{t,w} + \beta}{n_t + V \beta}$$

$$r_t = n_{t,d} q_t$$

とする。

まず $$q_t$$については、全$$t$$に対して非ゼロになる。
これは$$\beta$$がDirichlet分布のパラメータであるため。
$$q_t$$は単語$$w_{i,j}$$に依存するので毎回作り直す必要があるが、少し見方を変える。
通常のLDAでは

``` python
for j in 1:D
    for i in 1:Dj
        sample(wij, zij)
```

のように文書ごとにそこに含まれる単語についてサンプルを行う。
ここでは、逆にして単語ごとみて、その単語が含まれる文書について順番にサンプルする。
こうすることでB+Treeを使いまわす。

次に$$r_t$$について。
これは$$n_{t,d}$$があるためにスパースになる。
文書ごとに再計算の必要があるので$$r_t$$の累積和の配列を計算し、binary searchでサンプルを行う。
構築の計算量は $$O(|T_d|)$$で、サンプルの計算量は $$O(\log(|T_d|))$$。


---

この2つについてどちらがサンプルの計算量が優れているかを考える。
前者は$$O(\log(|T_w|))$$で後者は$$O(\log(|T_d|))$$であった。
今回はトピック数が$$T=1024$$のようにトピック数が多い場合について考えている。
この場合、前者のほうが上限値が大きくなることが予想される。
例えば`the`のような高頻度語やストップワードではない高頻度語は、トピック数を超える頻度をもつため、$$O(\log(|T_w|))$$は最悪の場合、$$|T_w|=T$$となる。

一方で後者の上限は、文書長になるため（文書数が長くないデータセットでは）$$|T_d| < T = 1024$$となる。
もちろん、この傾向はデータ依存で、論文の実験では、文書数が大きいデータ（NY times、文書数：298,000）では `word-by-word` のほうが速く、
文書数が小さいデータ（Enron、文書数：37,861）では `document-by-document` が速いという結果が報告されている。
ちなみに、小さいデータに関しては `word-by-word` は `SparseLDA` に負けている。

### その他

warpLDA以外は[C++の実装が公開](https://github.com/dmlc/experimental-lda)されている。
