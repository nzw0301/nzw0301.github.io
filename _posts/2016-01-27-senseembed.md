---
layout: post
title: "SENSEMBED：Learning Sense Embeddings for Word and Relational Similarity"
date: 2016-01-27 18:15:04 +0900
comments: false
---

#### 概要
ACL2015の[論文](http://wwwusers.di.uniroma1.it/~navigli/pubs/ACL_2015_Iacobaccietal.pdf)

nzwは，手法の部分が気になったので，その部分をかいつまんで説明します．（といっても数行）


word2vecやGloVeといったツールで獲得できる分散表現は，多義語でも1つのベクトルとして表現する．
例えば *bank* は，土手の意味と銀行の意味をもっているがこれを1つのベクトルで表現している．
この論文は，語義ごとにベクトルを学習できれば，より分散表現として優れるのではという研究．

モデル自体は単純で，まずWSDのSOTAな手法を使ってコーパスの単語ごとに語義を付与する．
次にCBoWで分散表現を学習する．
このときに同じ単語でも別の語義は付与されていれば別の単語とみなす．
この部分だけ気になっていたのでここらへんで読むのをやめた．

（コーパスだけを使って語義を取れるものだと勘違いしていた）


おそらくメインは残りで，類似度計算をCosine similarityではなく，語義を考慮したような式をいくつか提案している．
それをもとに既存手法と順位相関係数で評価．

#### 気になったこと

- analogyの評価指標にaccuracyがでてこないので，Mikolov論文やglove論文などの評価と違う
- 分散表現の良さは，ベクトルの演算と単語の演算（？）の対応がうまくできていて，扱いやすく計算が速いこと，という認識を持っていたのでだけど，この手法だと計算が重そう．
  - （出してないということは普通にベクトルとして計算してもあんまり性能が上がらなかった？）
