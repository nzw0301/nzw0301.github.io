---
layout: post
title: "青深層学習 4章 まとめ"
date: 2015-11-13 21:00:00 +0900
comments: false
---

4章誤差逆伝播法のまとめです．

3章で扱った勾配降下法は，誤差関数の勾配を求めて重みを更新し，ネットワークの性能を高めるために使いました．

誤差逆伝播法は，誤差関数の重みによる微分を効率良く計算するアルゴリズムです．

# 4.1 勾配計算の難しさ
![nn]({{ site.url }}/images/nn1.svg)

順伝播によって得られた出力を使うため，重みが出力層から遠いほど（入力層に近いほど）chain ruleで展開する項の数が増えていきます．

この場合では`hidden layer`と`output layer`の間の重みによる微分は比較的楽にできますが，`input layer`と`hidden layer`の間の重みによる微分は`hidden layer`の活性化関数を介しているので複雑になると言えます．

`4.2`以降で導出する誤差逆伝播法を用いることで効率的に計算を行えるようになります．


# 4.2 2層ネットワークでの計算
隠れ層が1層のニューラルネットワークにおいて，重みは`input layer`と`hidden layer`の間と`hidden layer`と`output layer`の間の2つに分けることができます．

[詳しい式の導出](http://nzw0301.github.io/2015/11/blueDeepLearningChapter42/)はこちらを参照．

冒頭の`4.1`で書いたように出力層から遠いほど式が複雑になっていることが確認できます．

# 4.3 多層ネットワークへの一般化
`4.2`は2層でしたが，層の数を一般化したときの微分を求めてます．
なので，ここで誤差逆伝播法の本筋の話になります．

[詳しい式の導出](http://nzw0301.github.io/2015/11/blueDeepLearningChapter43/)はこちらをみていただくとして，
肝心なのは， 各層ごとに\\(\delta\\)を\\(L\\)層（出力層）から逆順に求めることで，各重みによる微分を効率的に計算できることです．
\\(\delta\\)が計算できれば，重みの微分は別々に求めることができます．

# 4.4 勾配降下法の完全アルゴリズム

前半部分では，出力層の\\(\delta\\)の導出を行います．
[テキストより詳しい式変形をした](http://nzw0301.github.io/2015/11/blueDeepLearningChapter44/)ので，これからわかる通り，出力層の\\(\delta\\)は出力と正解の差です．

後半部分では，行列表記による

- 順伝播法
- 誤差逆伝播法
- 重み\\(w\\)の更新

の式です．
詳細については，コーディングしやすいように行列の次元を意識して[かきました](http://nzw0301.github.io/2015/11/blueDeepLearningChapter442/)．

`4.4.3 勾配の差分近似計算`  では，多層になった場合に誤差関数の勾配計算の計算が正しいかを確かめるために，近似計算を扱います．
偏微分の定義から
十分小さい\\(\epsilon\\)を用いた式1で検証できます．

\begin{eqnarray}
\frac{\partial E}{\partial w_{ji}^{(l)}} = \frac{E(...,w_{ji}^{(l)}+\epsilon,...)-E(...,w_{ji}^{(l)},...)}{\epsilon} \tag{1}
\end{eqnarray}

微分なので\\(\epsilon\\)は十分小さな値を選択する必要があります．
注意点として計算機の特性上，打ち切り誤差や丸めの誤差などで誤差が大きくなるため以下のように
`計算機イプシロン`(計算機の浮動小数点数で表現できる1より大きい最小の数と1との差のこと)\\(\epsilon_c\\)を使い，以下のように近似式の\\(\epsilon\\)を決定します．

\begin{eqnarray}
\epsilon = \sqrt{\epsilon_c} |w_{ji}|
\end{eqnarray}

`python`の`numpy`でいえば`np.finfo(float).eps`が計算機イプシロンです．
(手元では`2.2204460492503131e-16`でした)

? 実際につかうんでしょうか？

# 4.5 勾配消失問題

順伝播は活性化関数を経由するので活性化関数が非線形ならば，この層の出力も非線形であり，発散しません．（ロジスティック関数が活性化関数ならば0から1の間に収まる）

対して逆伝播法は定義(4.13)のように線形な計算なので，層が深くなるほど発散しやすい（あるいは一度0になるとずっと0）になってしまうため学習がうまくいかない`勾配消失問題`に直面します．


\begin{eqnarray}
\delta_j^{(l)} &=& \sum_k \delta_k^{(l+1)} (w_{kj}^{( l+1)} f'(u_j^{(l)})) \tag{4.12}\\\
\end{eqnarray}

# 補足

十分多い隠れ層が1層あれば任意の入出力の関係を実現できるが，タスクよっては層を増やしたほうが必要なノード数は少なく済む．
ただし，層を増やしすぎると勾配消失が起こる．[深層学習, 人工知能学会]


# 参考
- 深層学習. 人工知能学会, 神嶌 敏弘, 麻生 英樹, 安田 宗樹, 前田 新一, 岡野原 大輔, 岡谷 貴之, 久保 陽太郎, Bollegala Danushka. 近代科学社, 2015.
- 深層学習 = Deep learning. 岡谷 貴之. 講談社, MLP機械学習プロフェッショナルシリーズ, 2015.
